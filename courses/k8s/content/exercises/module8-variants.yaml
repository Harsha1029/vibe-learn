conceptLinks:
  Service DNS: "#lesson-service-dns-records"
  Headless Services: "#lesson-headless-services-clusterip-none"
  DNS Debugging: "#lesson-debugging-dns-issues"
  Pod DNS: "#lesson-pod-dns-records"
  DNS Policies: "#lesson-dns-policies"
  Cross-Namespace DNS: "#lesson-cross-namespace-communication"
  CoreDNS: "#lesson-how-dns-works-in-kubernetes"
  ndots: "#lesson-the-ndots5-gotcha"
sharedContent: {}
variants:
  warmups:
    - id: warmup_1
      concept: Service DNS
      variants:
        - id: v1
          title: Basic Service FQDN
          description: >-
            A Service named <code>api</code> exists in the <code>default</code> namespace. Write its fully qualified
            domain name (FQDN).
          hints:
            - "The FQDN pattern is: <code>&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
            - Replace the placeholders with <code>api</code> and <code>default</code>.
          solution: |-
            api.default.svc.cluster.local
        - id: v2
          title: Service FQDN in Production
          description: >-
            A Service named <code>payments</code> exists in the <code>production</code> namespace. Write its FQDN.
          hints:
            - "Pattern: <code>&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
          solution: |-
            payments.production.svc.cluster.local
        - id: v3
          title: Same-Namespace Short Name
          description: >-
            Your Pod is in the <code>default</code> namespace. A Service named <code>redis</code> is also in
            <code>default</code>. What is the shortest DNS name you can use to reach it?
          hints:
            - Services in the same namespace can be reached by just their name.
            - The search domain in <code>/etc/resolv.conf</code> includes your Pod's namespace.
          solution: |-
            redis
        - id: v4
          title: Cross-Namespace Short Name
          description: >-
            Your Pod is in the <code>frontend</code> namespace. You need to reach a Service named <code>api</code> in
            the <code>backend</code> namespace. What is the shortest DNS name that works?
          hints:
            - Cross-namespace requires at least <code>&lt;service&gt;.&lt;namespace&gt;</code>.
            - The search domains will expand it to the full FQDN automatically.
          solution: |-
            api.backend
        - id: v5
          title: FQDN for kube-dns
          description: >-
            What is the FQDN for the DNS Service itself? It is named <code>kube-dns</code> in the
            <code>kube-system</code> namespace.
          hints:
            - Apply the same FQDN pattern to the DNS Service itself.
          solution: |-
            kube-dns.kube-system.svc.cluster.local
        - id: v6
          title: Service in Custom Namespace
          description: >-
            A Service named <code>elasticsearch</code> exists in namespace <code>logging</code>. Write its FQDN.
          hints:
            - "Pattern: <code>&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
          solution: |-
            elasticsearch.logging.svc.cluster.local
        - id: v7
          title: Multiple Services Same Name
          description: >-
            There is a Service named <code>web</code> in both <code>staging</code> and <code>production</code>
            namespaces. Your Pod is in <code>staging</code>. What does <code>curl http://web</code> resolve to -- the
            staging or production Service?
          hints:
            - Short names resolve using the search domain, which starts with the Pod's own namespace.
            - The first search domain tried is <code>&lt;pod-namespace&gt;.svc.cluster.local</code>.
          solution: |-
            The staging Service. Short name "web" resolves to
            web.staging.svc.cluster.local because the Pod's
            own namespace (staging) is the first search domain.
        - id: v8
          title: Trailing Dot FQDN
          description: >-
            Write the FQDN for a Service named <code>api</code> in namespace <code>production</code> with a trailing
            dot to bypass search domain expansion.
          hints:
            - A trailing dot tells the resolver the name is already fully qualified.
            - This avoids the extra DNS lookups caused by <code>ndots:5</code>.
          solution: |-
            api.production.svc.cluster.local.
        - id: v9
          title: SRV Record Format
          description: >-
            A Service named <code>web</code> in namespace <code>default</code> has a port named <code>http</code>
            using TCP. What is the SRV record DNS name?
          hints:
            - "SRV record format: <code>_&lt;port-name&gt;._&lt;protocol&gt;.&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
          solution: |-
            _http._tcp.web.default.svc.cluster.local
        - id: v10
          title: Service FQDN in team-b
          description: >-
            A Service named <code>cache</code> is in namespace <code>team-b</code>. Your Pod is in
            <code>team-a</code>. Write both the short name and the FQDN to reach it.
          hints:
            - "Short cross-namespace name: <code>&lt;service&gt;.&lt;namespace&gt;</code>."
            - FQDN adds <code>.svc.cluster.local</code> to that.
          solution: |-
            # Short name (cross-namespace)
            cache.team-b

            # Full FQDN
            cache.team-b.svc.cluster.local
        - id: v11
          title: Database Service FQDN
          description: >-
            A Service named <code>postgres</code> exists in the <code>data</code> namespace. A Pod in the
            <code>app</code> namespace needs to connect to it on port 5432. Write the connection string using the
            shortest DNS name.
          hints:
            - Cross-namespace requires <code>&lt;service&gt;.&lt;namespace&gt;</code>.
            - Append the port with a colon.
          solution: |-
            postgres.data:5432
        - id: v12
          title: Resolv.conf Search Domain
          description: >-
            A Pod runs in the <code>monitoring</code> namespace. What search domains appear in its
            <code>/etc/resolv.conf</code>?
          hints:
            - The first search domain uses the Pod's own namespace.
            - "Three search domains are added: <code>&lt;namespace&gt;.svc.cluster.local</code>, <code>svc.cluster.local</code>, <code>cluster.local</code>."
          solution: |-
            search monitoring.svc.cluster.local svc.cluster.local cluster.local
    - id: warmup_2
      concept: Headless Services
      variants:
        - id: v1
          title: Basic Headless Service YAML
          description: >-
            Write a Headless Service YAML named <code>db</code> in the <code>default</code> namespace that selects
            Pods with label <code>app: db</code> on port 5432.
          hints:
            - A Headless Service sets <code>clusterIP: None</code>.
            - Everything else looks like a normal Service.
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: db
            spec:
              clusterIP: None
              selector:
                app: db
              ports:
              - port: 5432
        - id: v2
          title: StatefulSet Pod DNS Name
          description: >-
            A StatefulSet named <code>redis</code> with <code>serviceName: redis</code> runs in the
            <code>cache</code> namespace with 3 replicas. What is the FQDN for the second Pod (index 1)?
          hints:
            - "StatefulSet Pod DNS: <code>&lt;pod-name&gt;.&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
            - StatefulSet Pods are named <code>&lt;statefulset-name&gt;-&lt;ordinal&gt;</code>.
          solution: |-
            redis-1.redis.cache.svc.cluster.local
        - id: v3
          title: Headless Service for MySQL StatefulSet
          description: >-
            Write the Headless Service YAML that would be paired with a StatefulSet named <code>mysql</code>. The
            Service should select <code>app: mysql</code> on port 3306.
          hints:
            - The Service name must match the StatefulSet's <code>serviceName</code> field.
            - Set <code>clusterIP: None</code>.
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: mysql
            spec:
              clusterIP: None
              selector:
                app: mysql
              ports:
              - port: 3306
        - id: v4
          title: Predict All Pod DNS Names
          description: >-
            A StatefulSet named <code>kafka</code> with <code>serviceName: kafka-headless</code> has 3 replicas in
            namespace <code>streaming</code>. List the FQDN for all three Pods.
          hints:
            - "Pod names follow: <code>&lt;statefulset&gt;-0</code>, <code>&lt;statefulset&gt;-1</code>, <code>&lt;statefulset&gt;-2</code>."
            - "FQDN: <code>&lt;pod&gt;.&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
          solution: |-
            kafka-0.kafka-headless.streaming.svc.cluster.local
            kafka-1.kafka-headless.streaming.svc.cluster.local
            kafka-2.kafka-headless.streaming.svc.cluster.local
        - id: v5
          title: Headless vs Regular Service
          description: >-
            What does <code>nslookup db</code> return for a Headless Service with 3 backend Pods, versus a regular
            ClusterIP Service with 3 backend Pods?
          hints:
            - A regular Service returns one IP -- the ClusterIP (virtual IP).
            - A Headless Service returns the actual Pod IPs directly.
          solution: |-
            # Regular ClusterIP Service:
            # Returns a single ClusterIP (e.g., 10.96.50.100)

            # Headless Service (clusterIP: None):
            # Returns all Pod IPs directly (e.g., 10.1.0.15, 10.1.0.16, 10.1.0.17)
        - id: v6
          title: Headless Service with Named Pod
          description: >-
            A Pod has <code>hostname: my-pod</code> and <code>subdomain: app-svc</code>. A Headless Service named
            <code>app-svc</code> exists in the <code>default</code> namespace. What is the Pod's DNS name?
          hints:
            - "When a Pod sets <code>hostname</code> and <code>subdomain</code>, its DNS is: <code>&lt;hostname&gt;.&lt;subdomain&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
            - The <code>subdomain</code> must match a Headless Service name.
          solution: |-
            my-pod.app-svc.default.svc.cluster.local
        - id: v7
          title: Elasticsearch StatefulSet DNS
          description: >-
            A StatefulSet named <code>es</code> with <code>serviceName: es-headless</code> runs 5 replicas in
            namespace <code>logging</code>. Write the FQDN for the master node (index 0).
          hints:
            - "StatefulSet Pod FQDN: <code>&lt;pod-name&gt;.&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
            - The first Pod is <code>es-0</code>.
          solution: |-
            es-0.es-headless.logging.svc.cluster.local
        - id: v8
          title: Complete Headless + StatefulSet Pair
          description: >-
            Write a minimal Headless Service and StatefulSet for a ZooKeeper cluster with 3 replicas in the
            <code>default</code> namespace. Use image <code>zookeeper:3.8</code>, port 2181, and label
            <code>app: zk</code>.
          hints:
            - The Service needs <code>clusterIP: None</code> and must match the StatefulSet's <code>serviceName</code>.
            - The StatefulSet needs <code>serviceName</code>, <code>replicas</code>, <code>selector</code>, and a template.
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: zk-headless
            spec:
              clusterIP: None
              selector:
                app: zk
              ports:
              - port: 2181
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: zk
            spec:
              serviceName: zk-headless
              replicas: 3
              selector:
                matchLabels:
                  app: zk
              template:
                metadata:
                  labels:
                    app: zk
                spec:
                  containers:
                  - name: zookeeper
                    image: zookeeper:3.8
                    ports:
                    - containerPort: 2181
        - id: v9
          title: Headless Service for Cassandra
          description: >-
            Write a Headless Service YAML named <code>cassandra</code> selecting <code>app: cassandra</code> on port
            9042 in namespace <code>data</code>.
          hints:
            - Same pattern as any Headless Service -- <code>clusterIP: None</code>.
            - Add <code>namespace: data</code> in metadata.
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: cassandra
              namespace: data
            spec:
              clusterIP: None
              selector:
                app: cassandra
              ports:
              - port: 9042
        - id: v10
          title: StatefulSet Pod DNS After Restart
          description: >-
            Pod <code>mysql-0</code> in StatefulSet <code>mysql</code> (serviceName <code>mysql</code>, namespace
            <code>db</code>) is deleted and rescheduled to a different node with a new IP. Does its DNS name change?
          hints:
            - StatefulSet Pods maintain their identity (name and ordinal) across restarts.
            - The DNS name is based on the Pod name, not the IP.
          solution: |-
            No. The DNS name mysql-0.mysql.db.svc.cluster.local
            stays the same. Only the IP address it resolves to
            changes. StatefulSet Pod DNS names are stable
            across rescheduling.
        - id: v11
          title: Peer Discovery with Headless Service
          description: >-
            You have a Headless Service named <code>etcd</code> in namespace <code>infra</code> with 3 StatefulSet
            Pods. Write the DNS names an application would use to discover all peers.
          hints:
            - Query the Headless Service name to get all Pod IPs.
            - Or address each peer individually by Pod DNS name.
          solution: |-
            # Query all peers at once (returns all Pod IPs):
            etcd.infra.svc.cluster.local

            # Address individual peers:
            etcd-0.etcd.infra.svc.cluster.local
            etcd-1.etcd.infra.svc.cluster.local
            etcd-2.etcd.infra.svc.cluster.local
    - id: warmup_3
      concept: DNS Debugging
      variants:
        - id: v1
          title: Run a DNS Debug Pod
          description: >-
            Write the <code>kubectl</code> command to run a temporary busybox Pod and execute
            <code>nslookup kubernetes</code> to test DNS resolution.
          hints:
            - Use <code>kubectl run</code> with <code>--rm -it --restart=Never</code> for a temporary Pod.
            - The image <code>busybox</code> includes <code>nslookup</code>.
          solution: |-
            kubectl run dns-debug --image=busybox --rm -it --restart=Never -- nslookup kubernetes
        - id: v2
          title: Check CoreDNS Pods
          description: >-
            Write the <code>kubectl</code> command to check if CoreDNS Pods are running in the cluster.
          hints:
            - CoreDNS runs in the <code>kube-system</code> namespace.
            - CoreDNS Pods have the label <code>k8s-app=kube-dns</code>.
          solution: |-
            kubectl get pods -n kube-system -l k8s-app=kube-dns
        - id: v3
          title: View CoreDNS Logs
          description: >-
            Write the <code>kubectl</code> command to view the last 50 lines of CoreDNS logs to troubleshoot DNS
            failures.
          hints:
            - Use <code>kubectl logs</code> with <code>-l</code> to select by label.
            - Use <code>--tail=50</code> to limit output.
          solution: |-
            kubectl logs -n kube-system -l k8s-app=kube-dns --tail=50
        - id: v4
          title: Check Pod resolv.conf
          description: >-
            Write the <code>kubectl</code> command to inspect <code>/etc/resolv.conf</code> inside a running Pod
            named <code>web-abc12</code>.
          hints:
            - Use <code>kubectl exec</code> to run a command inside an existing Pod.
            - Run <code>cat /etc/resolv.conf</code> inside the Pod.
          solution: |-
            kubectl exec web-abc12 -- cat /etc/resolv.conf
        - id: v5
          title: Test Cross-Namespace Resolution
          description: >-
            Write the <code>kubectl</code> command to test DNS resolution for a Service named <code>api</code> in
            namespace <code>backend</code> from a temporary Pod.
          hints:
            - Use <code>kubectl run</code> with busybox and <code>nslookup</code>.
            - Use the cross-namespace name <code>api.backend</code>.
          solution: |-
            kubectl run dns-debug --image=busybox --rm -it --restart=Never -- nslookup api.backend
        - id: v6
          title: Check kube-dns Service IP
          description: >-
            Write the <code>kubectl</code> command to get the ClusterIP of the <code>kube-dns</code> Service, which
            should match the <code>nameserver</code> in Pod resolv.conf files.
          hints:
            - Use <code>kubectl get svc</code> in the <code>kube-system</code> namespace.
            - Use <code>-o jsonpath</code> to extract just the ClusterIP.
          solution: |-
            kubectl get svc -n kube-system kube-dns -o jsonpath='{.spec.clusterIP}'
        - id: v7
          title: Test External DNS Resolution
          description: >-
            Write the <code>kubectl</code> command to test that external DNS resolution works from inside the cluster
            by looking up <code>google.com</code>.
          hints:
            - Same pattern as testing cluster DNS, but with an external domain.
          solution: |-
            kubectl run dns-debug --image=busybox --rm -it --restart=Never -- nslookup google.com
        - id: v8
          title: DNS Debug with dnsutils Image
          description: >-
            Write the <code>kubectl</code> command to start an interactive debug Pod with the
            <code>tutum/dnsutils</code> image (which includes <code>dig</code>) and launch a bash shell.
          hints:
            - Use <code>tutum/dnsutils</code> for more DNS tools like <code>dig</code>.
            - End the command with <code>-- bash</code> for an interactive shell.
          solution: |-
            kubectl run dns-debug --image=tutum/dnsutils --rm -it --restart=Never -- bash
        - id: v9
          title: Dig SRV Record
          description: >-
            Write the command to query the SRV record for a Service named <code>web</code> in namespace
            <code>default</code> using <code>dig</code> from a debug Pod.
          hints:
            - Use <code>tutum/dnsutils</code> which has <code>dig</code>.
            - "SRV query: <code>dig SRV &lt;fqdn&gt;</code>."
          solution: |-
            kubectl run dns-debug --image=tutum/dnsutils --rm -it --restart=Never -- \
              dig SRV web.default.svc.cluster.local
        - id: v10
          title: Check CoreDNS ConfigMap
          description: >-
            Write the <code>kubectl</code> command to view the CoreDNS configuration (stored as a ConfigMap) to check
            upstream DNS settings.
          hints:
            - The CoreDNS config is a ConfigMap named <code>coredns</code> in <code>kube-system</code>.
            - Use <code>-o yaml</code> to see the full configuration.
          solution: |-
            kubectl get configmap coredns -n kube-system -o yaml
        - id: v11
          title: Restart CoreDNS
          description: >-
            CoreDNS seems stuck. Write the <code>kubectl</code> command to restart the CoreDNS Deployment without
            deleting it.
          hints:
            - Use <code>kubectl rollout restart</code> to trigger a rolling restart.
            - CoreDNS is a Deployment in <code>kube-system</code>.
          solution: |-
            kubectl rollout restart deploy/coredns -n kube-system
        - id: v12
          title: Verify DNS from Specific Namespace
          description: >-
            Write the <code>kubectl</code> command to run a temporary debug Pod <em>in the
            <code>production</code> namespace</em> and test DNS resolution for the Service <code>db</code> in the
            same namespace.
          hints:
            - Use <code>-n production</code> to run the debug Pod in that namespace.
            - Since the debug Pod is in the same namespace, just use the short name <code>db</code>.
          solution: |-
            kubectl run dns-debug -n production --image=busybox --rm -it --restart=Never -- nslookup db
  challenges:
    - id: challenge_1
      block: 1
      difficulty: 1
      concept: Cross-Namespace DNS
      variants:
        - id: v1
          title: Frontend to Backend DNS
          description: >-
            Your Pod runs in the <code>frontend</code> namespace. Write the DNS name to reach a Service named
            <code>api</code> in the <code>backend</code> namespace. Provide both the short name and the FQDN.
          functionSignature: "DNS name for api in backend from frontend namespace"
          testCases:
            - input: "Pod in frontend, Service api in backend"
              output: "api.backend or api.backend.svc.cluster.local"
            - input: "Pod in frontend, Service api in frontend"
              output: "api (short name works, same namespace)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                When a Pod and Service are in different namespaces, the short name alone resolves in the Pod's own
                namespace. You need to qualify it.
            - title: "\U0001F4A1 Hint"
              content: >-
                Cross-namespace DNS uses <code>&lt;service&gt;.&lt;namespace&gt;</code> as the shortest form.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Same namespace: just the service name
                Cross-namespace: service.namespace
                Always works: service.namespace.svc.cluster.local</pre>
          solution: |-
            # Short cross-namespace name:
            api.backend

            # Full FQDN:
            api.backend.svc.cluster.local
          difficulty: 1
        - id: v2
          title: Three-Tier Application DNS
          description: >-
            You have three namespaces: <code>web</code>, <code>api</code>, and <code>data</code>. Each has a Service
            with the same name as the namespace. Write the DNS name a Pod in <code>web</code> would use to reach the
            Service in <code>api</code>, and the DNS name a Pod in <code>api</code> would use to reach the Service in
            <code>data</code>.
          functionSignature: "DNS names for three-tier app communication"
          testCases:
            - input: "Pod in web namespace, target Service api in api namespace"
              output: "api.api"
            - input: "Pod in api namespace, target Service data in data namespace"
              output: "data.data"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                When the Service name and namespace name are the same, the DNS name looks like
                <code>name.name</code>. This is valid.
            - title: "\U0001F4A1 Hint"
              content: >-
                Apply the pattern <code>&lt;service&gt;.&lt;namespace&gt;</code> for each cross-namespace call.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>web Pod → api Service: api.api
                api Pod → data Service: data.data
                Full FQDNs: api.api.svc.cluster.local, data.data.svc.cluster.local</pre>
          solution: |-
            # Pod in web → Service in api namespace:
            api.api
            # FQDN: api.api.svc.cluster.local

            # Pod in api → Service in data namespace:
            data.data
            # FQDN: data.data.svc.cluster.local
          difficulty: 1
        - id: v3
          title: Shared Database Access
          description: >-
            Teams A (<code>team-a</code>) and B (<code>team-b</code>) both need to access a PostgreSQL Service named
            <code>postgres</code> in the <code>shared-db</code> namespace on port 5432. Write the connection string
            each team's Pods would use.
          functionSignature: "Connection string for postgres in shared-db namespace"
          testCases:
            - input: "Pod in team-a, Service postgres in shared-db, port 5432"
              output: "postgres.shared-db:5432"
            - input: "Pod in team-b, Service postgres in shared-db, port 5432"
              output: "postgres.shared-db:5432"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Both teams are in different namespaces from the database. They both use the same cross-namespace DNS
                name.
            - title: "\U0001F4A1 Hint"
              content: >-
                The DNS name is the same regardless of which namespace the calling Pod is in -- only the target
                namespace matters.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>service.namespace:port
                postgres.shared-db:5432</pre>
          solution: |-
            # Both teams use the same DNS name:
            postgres.shared-db:5432

            # Or with full FQDN:
            postgres.shared-db.svc.cluster.local:5432
          difficulty: 1
        - id: v4
          title: Ambiguous Short Name
          description: >-
            A Service named <code>cache</code> exists in both <code>staging</code> and <code>production</code>
            namespaces. A Pod in <code>staging</code> calls <code>curl http://cache</code>. Which Service does it
            reach? How would it reach the production one instead?
          functionSignature: "Resolve ambiguous DNS name and fix cross-namespace"
          testCases:
            - input: "Pod in staging calls 'cache' -- cache exists in both staging and production"
              output: "Reaches cache in staging (own namespace). Use cache.production for the other."
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Short names resolve using search domains, and the Pod's own namespace is always first in the search
                list.
            - title: "\U0001F4A1 Hint"
              content: >-
                To reach a Service in a different namespace, you must explicitly include the namespace in the DNS name.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Short name "cache" → resolves in Pod's namespace (staging)
                Explicit: cache.production → resolves in production namespace</pre>
          solution: |-
            # "cache" resolves to cache.staging.svc.cluster.local
            # (Pod's own namespace comes first in search domains)

            # To reach production:
            cache.production
          difficulty: 2
        - id: v5
          title: FQDN with Trailing Dot Optimization
          description: >-
            Your application in namespace <code>app</code> frequently calls <code>api.production.svc.cluster.local</code>.
            This name has 4 dots, and <code>ndots</code> is set to 5. How many DNS queries are made? How do you fix
            this with a trailing dot?
          functionSignature: "Optimize FQDN to avoid extra DNS lookups"
          testCases:
            - input: "api.production.svc.cluster.local with ndots:5"
              output: "4 queries (3 NXDOMAIN + 1 success). Fix: api.production.svc.cluster.local."
            - input: "api.production.svc.cluster.local. (trailing dot) with ndots:5"
              output: "1 query (direct resolution, no search domain expansion)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Count the dots in the name. If fewer than <code>ndots</code> (5), the resolver tries appending each
                search domain first.
            - title: "\U0001F4A1 Hint"
              content: >-
                <code>api.production.svc.cluster.local</code> has 4 dots. Since 4 < 5, the resolver appends search
                domains first, generating 3 failed queries before trying the name as-is.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Without trailing dot: 4 queries total
                  1. api.production.svc.cluster.local.app.svc.cluster.local → NXDOMAIN
                  2. api.production.svc.cluster.local.svc.cluster.local → NXDOMAIN
                  3. api.production.svc.cluster.local.cluster.local → NXDOMAIN
                  4. api.production.svc.cluster.local → SUCCESS

                With trailing dot: 1 query
                  api.production.svc.cluster.local. → SUCCESS</pre>
          solution: |-
            # Without trailing dot: 4 DNS queries
            api.production.svc.cluster.local
            # Tries 3 search domain expansions first (all NXDOMAIN),
            # then the name as-is (success)

            # With trailing dot: 1 DNS query
            api.production.svc.cluster.local.
            # Resolver knows it's already fully qualified
          difficulty: 2
        - id: v6
          title: External Domain ndots Impact
          description: >-
            Your Pod has default <code>ndots:5</code>. It calls <code>api.stripe.com</code> (2 dots). How many DNS
            queries are made? Write the <code>dnsConfig</code> to reduce wasted lookups.
          functionSignature: "dnsConfig to optimize external DNS resolution"
          testCases:
            - input: "api.stripe.com with ndots:5"
              output: "4 queries (3 wasted). Fix: add trailing dot or lower ndots."
            - input: "api.stripe.com. (trailing dot)"
              output: "1 query (direct resolution)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                External domains usually have 1-2 dots, which is far below <code>ndots:5</code>. This means every
                external DNS call generates extra wasted queries.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use <code>dnsConfig</code> to override <code>ndots</code> to a lower value like 2.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>spec:
                  dnsConfig:
                    options:
                    - name: ndots
                      value: "2"</pre>
          solution: |-
            # Add to Pod spec to reduce wasted DNS lookups:
            spec:
              dnsConfig:
                options:
                - name: ndots
                  value: "2"

            # Or append trailing dot to external domains:
            # api.stripe.com.
          difficulty: 3
        - id: v7
          title: Multi-Cluster Service Naming
          description: >-
            You manage two clusters. In Cluster A, namespace <code>payments</code> has Service <code>gateway</code>.
            In Cluster B, namespace <code>orders</code>, a Pod needs to call that Service. Assuming the clusters use
            the same domain suffix <code>cluster.local</code>, can the Pod in Cluster B use
            <code>gateway.payments.svc.cluster.local</code>? What is the fundamental limitation?
          functionSignature: "Explain cross-cluster DNS limitations"
          testCases:
            - input: "Pod in Cluster B calls gateway.payments.svc.cluster.local"
              output: "Fails. cluster.local DNS is local to each cluster. Needs external DNS or service mesh."
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                CoreDNS only knows about Services in its own cluster. It has no knowledge of Services in other
                clusters.
            - title: "\U0001F4A1 Hint"
              content: >-
                Cross-cluster communication requires external mechanisms like ExternalDNS, a service mesh (Istio),
                or multi-cluster DNS solutions.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>cluster.local is scoped to a single cluster.
                Cross-cluster options:
                - ExternalDNS with external domain names
                - Service mesh (Istio multi-cluster)
                - DNS forwarding between clusters</pre>
          solution: |-
            # No -- cluster.local is scoped per-cluster.
            # CoreDNS in Cluster B does not know about
            # Services in Cluster A.

            # Solutions:
            # 1. Use ExternalDNS with real domain names
            # 2. Use a service mesh for cross-cluster routing
            # 3. Configure DNS forwarding between clusters
          difficulty: 3
    - id: challenge_2
      block: 1
      difficulty: 2
      concept: Headless Services
      variants:
        - id: v1
          title: Design a Redis Cluster
          description: >-
            Design a Headless Service and StatefulSet for a 3-node Redis cluster in namespace <code>cache</code>. Use
            image <code>redis:7</code>, port 6379, label <code>app: redis</code>. Each Pod should be individually
            addressable by DNS name.
          functionSignature: "Headless Service + StatefulSet YAML for Redis cluster"
          testCases:
            - input: "redis-0 DNS name in cache namespace"
              output: "redis-0.redis.cache.svc.cluster.local"
            - input: "redis-2 DNS name in cache namespace"
              output: "redis-2.redis.cache.svc.cluster.local"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                A Redis cluster needs each node to have a stable, predictable DNS name so nodes can find each other.
                What Kubernetes constructs give you stable Pod identities?
            - title: "\U0001F4A1 Hint"
              content: >-
                Combine a Headless Service (<code>clusterIP: None</code>) with a StatefulSet. The StatefulSet's
                <code>serviceName</code> must match the Headless Service name.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Headless Service: clusterIP: None, selector matches StatefulSet Pods
                2. StatefulSet: serviceName matches the Service name
                3. Pod DNS: pod-name.service-name.namespace.svc.cluster.local</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: redis
              namespace: cache
            spec:
              clusterIP: None
              selector:
                app: redis
              ports:
              - port: 6379
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: redis
              namespace: cache
            spec:
              serviceName: redis
              replicas: 3
              selector:
                matchLabels:
                  app: redis
              template:
                metadata:
                  labels:
                    app: redis
                spec:
                  containers:
                  - name: redis
                    image: redis:7
                    ports:
                    - containerPort: 6379
          difficulty: 2
        - id: v2
          title: Cassandra Peer Discovery
          description: >-
            Design a Headless Service for Cassandra peer discovery. Cassandra nodes (4 replicas) need to discover each
            other by querying DNS. Namespace <code>data</code>, image <code>cassandra:4</code>, port 9042, label
            <code>app: cassandra</code>. Write the YAML and explain how a new Cassandra node discovers existing
            peers.
          functionSignature: "Headless Service + StatefulSet for Cassandra with peer discovery explanation"
          testCases:
            - input: "nslookup cassandra.data.svc.cluster.local"
              output: "Returns all 4 Pod IPs (no ClusterIP, direct Pod IPs)"
            - input: "cassandra-3 DNS name"
              output: "cassandra-3.cassandra.data.svc.cluster.local"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Cassandra uses seed nodes for cluster formation. New nodes need to discover existing nodes. A Headless
                Service returns all Pod IPs, which is exactly what Cassandra needs.
            - title: "\U0001F4A1 Hint"
              content: >-
                A DNS lookup on the Headless Service name returns all Pod IPs. Cassandra can use this as its seed
                provider. Individual Pods are also addressable by name.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Headless Service returns all Pod IPs on DNS lookup
                2. Cassandra seed_provider queries the Service DNS
                3. New node: nslookup cassandra → gets all peer IPs
                4. Individual: cassandra-N.cassandra.data.svc.cluster.local</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: cassandra
              namespace: data
            spec:
              clusterIP: None
              selector:
                app: cassandra
              ports:
              - port: 9042
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: cassandra
              namespace: data
            spec:
              serviceName: cassandra
              replicas: 4
              selector:
                matchLabels:
                  app: cassandra
              template:
                metadata:
                  labels:
                    app: cassandra
                spec:
                  containers:
                  - name: cassandra
                    image: cassandra:4
                    ports:
                    - containerPort: 9042

            # Peer discovery: a new Cassandra node queries
            # "cassandra.data.svc.cluster.local" via DNS.
            # The Headless Service returns all Pod IPs directly,
            # allowing the new node to find and join existing peers.
          difficulty: 2
        - id: v3
          title: MySQL Primary-Replica with Headless
          description: >-
            Design a MySQL StatefulSet where the primary is always <code>mysql-0</code> and replicas are
            <code>mysql-1</code> and <code>mysql-2</code>. Write the Headless Service and StatefulSet YAML.
            Applications should write to the primary using its predictable DNS name. Namespace
            <code>database</code>, image <code>mysql:8.0</code>, port 3306.
          functionSignature: "Headless + StatefulSet for MySQL primary-replica topology"
          testCases:
            - input: "Primary (write) DNS name"
              output: "mysql-0.mysql.database.svc.cluster.local"
            - input: "Read from any replica"
              output: "mysql.database.svc.cluster.local (returns all Pod IPs)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                StatefulSet Pods are created in order (0, 1, 2). By convention, the first Pod (index 0) is the
                primary. Its DNS name is always predictable.
            - title: "\U0001F4A1 Hint"
              content: >-
                Writes go to <code>mysql-0.mysql.database.svc.cluster.local</code>. Reads can go to the Headless
                Service name (all Pods) or specific replicas.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Writes → mysql-0.mysql.database.svc.cluster.local (primary)
                Reads → mysql.database.svc.cluster.local (all Pods)
                Specific replica → mysql-1.mysql.database.svc.cluster.local</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: mysql
              namespace: database
            spec:
              clusterIP: None
              selector:
                app: mysql
              ports:
              - port: 3306
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: mysql
              namespace: database
            spec:
              serviceName: mysql
              replicas: 3
              selector:
                matchLabels:
                  app: mysql
              template:
                metadata:
                  labels:
                    app: mysql
                spec:
                  containers:
                  - name: mysql
                    image: mysql:8.0
                    ports:
                    - containerPort: 3306

            # Write to primary:
            #   mysql-0.mysql.database.svc.cluster.local:3306
            # Read from any replica:
            #   mysql.database.svc.cluster.local:3306
            # Read from specific replica:
            #   mysql-1.mysql.database.svc.cluster.local:3306
          difficulty: 2
        - id: v4
          title: Headless Service with Port Discovery
          description: >-
            Create a Headless Service named <code>grpc-svc</code> for a gRPC StatefulSet with named port
            <code>grpc</code> (port 50051). Write the Service YAML and the SRV record DNS name that clients can use
            to discover the port.
          functionSignature: "Headless Service with SRV record for port discovery"
          testCases:
            - input: "SRV record for grpc port"
              output: "_grpc._tcp.grpc-svc.default.svc.cluster.local"
            - input: "Pod-0 FQDN"
              output: "grpc-app-0.grpc-svc.default.svc.cluster.local"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                SRV records require named ports. The SRV DNS name follows the pattern
                <code>_port-name._protocol.service.namespace.svc.cluster.local</code>.
            - title: "\U0001F4A1 Hint"
              content: >-
                Name the port in the Service spec. Then SRV record is
                <code>_grpc._tcp.grpc-svc.default.svc.cluster.local</code>.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Service port:
                  ports:
                  - name: grpc       ← required for SRV records
                    port: 50051
                    protocol: TCP

                SRV record: _grpc._tcp.grpc-svc.default.svc.cluster.local</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: grpc-svc
            spec:
              clusterIP: None
              selector:
                app: grpc-app
              ports:
              - name: grpc
                port: 50051
                protocol: TCP

            # SRV record DNS name:
            # _grpc._tcp.grpc-svc.default.svc.cluster.local
            #
            # Dig query to discover port:
            # dig SRV _grpc._tcp.grpc-svc.default.svc.cluster.local
          difficulty: 3
        - id: v5
          title: Headless vs ClusterIP Decision
          description: >-
            You are designing a Service for each scenario. For each, decide whether to use a Headless Service
            (<code>clusterIP: None</code>) or a regular ClusterIP Service, and explain why.<br><br>
            1. A 3-node Elasticsearch cluster where nodes need to discover each other<br>
            2. A stateless REST API with 5 replicas behind a load balancer<br>
            3. A Kafka cluster where producers need to connect to specific brokers<br>
            4. A web frontend serving static files
          functionSignature: "Headless vs ClusterIP for each scenario"
          testCases:
            - input: "Elasticsearch cluster with peer discovery"
              output: "Headless -- nodes need to discover all peers by DNS"
            - input: "Stateless REST API"
              output: "ClusterIP -- standard load balancing, no need for individual Pod addressing"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Headless Services are for when clients need to know individual Pod IPs or when Pods need to discover
                peers. ClusterIP is for standard load-balanced access.
            - title: "\U0001F4A1 Hint"
              content: >-
                Ask: "Does the client need to talk to a <em>specific</em> Pod, or just <em>any</em> Pod?" If specific,
                use Headless.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Headless: stateful apps, peer discovery, client-side LB
                ClusterIP: stateless apps, server-side LB, simple routing</pre>
          solution: |-
            # 1. Elasticsearch: HEADLESS
            #    Nodes need peer discovery to form a cluster.
            #    DNS returns all Pod IPs for discovery.

            # 2. REST API: CLUSTERIP
            #    Stateless, any Pod can handle any request.
            #    ClusterIP provides built-in load balancing.

            # 3. Kafka: HEADLESS
            #    Producers connect to specific brokers (partitions
            #    are assigned to specific brokers). Each broker
            #    needs a stable, addressable DNS name.

            # 4. Web Frontend: CLUSTERIP
            #    Stateless static file serving. Any Pod can serve
            #    any request. Standard load balancing is ideal.
          difficulty: 2
        - id: v6
          title: Etcd Cluster with Headless Service
          description: >-
            Design a 3-node etcd cluster using a Headless Service and StatefulSet. Namespace <code>kube-system</code>,
            image <code>etcd:3.5</code>, ports 2379 (client) and 2380 (peer). Etcd nodes need to know each other's
            DNS names for cluster formation. Write the YAML and the <code>--initial-cluster</code> flag value using
            DNS names.
          functionSignature: "Headless Service + StatefulSet + etcd initial-cluster config"
          testCases:
            - input: "etcd-0 peer DNS"
              output: "etcd-0.etcd.kube-system.svc.cluster.local"
            - input: "initial-cluster value"
              output: "etcd-0=https://etcd-0.etcd.kube-system.svc.cluster.local:2380,etcd-1=...,etcd-2=..."
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Etcd requires each member to know the addresses of all other members at startup. StatefulSet Pod DNS
                names are predictable, so you can pre-compute them.
            - title: "\U0001F4A1 Hint"
              content: >-
                The <code>--initial-cluster</code> flag takes
                <code>name=url,name=url</code> format. Use the StatefulSet Pod DNS names for the URLs.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>--initial-cluster \
                  etcd-0=https://etcd-0.etcd.kube-system.svc.cluster.local:2380,\
                  etcd-1=https://etcd-1.etcd.kube-system.svc.cluster.local:2380,\
                  etcd-2=https://etcd-2.etcd.kube-system.svc.cluster.local:2380</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: etcd
              namespace: kube-system
            spec:
              clusterIP: None
              selector:
                app: etcd
              ports:
              - name: client
                port: 2379
              - name: peer
                port: 2380
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: etcd
              namespace: kube-system
            spec:
              serviceName: etcd
              replicas: 3
              selector:
                matchLabels:
                  app: etcd
              template:
                metadata:
                  labels:
                    app: etcd
                spec:
                  containers:
                  - name: etcd
                    image: etcd:3.5
                    ports:
                    - containerPort: 2379
                      name: client
                    - containerPort: 2380
                      name: peer

            # --initial-cluster value:
            # etcd-0=https://etcd-0.etcd.kube-system.svc.cluster.local:2380,
            # etcd-1=https://etcd-1.etcd.kube-system.svc.cluster.local:2380,
            # etcd-2=https://etcd-2.etcd.kube-system.svc.cluster.local:2380
          difficulty: 3
    - id: challenge_3
      block: 2
      difficulty: 2
      concept: DNS Debugging
      variants:
        - id: v1
          title: Service Not Resolving
          description: >-
            A developer reports that <code>curl http://api</code> fails with "Could not resolve host" from a Pod in
            the <code>frontend</code> namespace. The <code>api</code> Service exists in the <code>backend</code>
            namespace. Diagnose the issue and provide the fix.
          functionSignature: "Diagnose and fix DNS resolution failure"
          testCases:
            - input: "curl http://api from frontend namespace, api Service is in backend namespace"
              output: "Short name 'api' resolves in frontend (not found). Fix: use api.backend"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                When you use a short name like <code>api</code>, the search domain prepends the Pod's own namespace.
                If the Service is in a different namespace, the short name will not find it.
            - title: "\U0001F4A1 Hint"
              content: >-
                The Pod is in <code>frontend</code>, so <code>api</code> resolves to
                <code>api.frontend.svc.cluster.local</code>, which does not exist.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Problem: short name resolves in wrong namespace
                Diagnosis:
                  1. kubectl run dns-debug --image=busybox --rm -it --restart=Never -- nslookup api
                     (run from frontend namespace -- will fail)
                  2. kubectl run dns-debug --image=busybox --rm -it --restart=Never -- nslookup api.backend
                     (will succeed)
                Fix: use api.backend or api.backend.svc.cluster.local</pre>
          solution: |-
            # Diagnosis: "api" resolves as api.frontend.svc.cluster.local
            # (Pod's own namespace), but the Service is in backend.

            # Verify:
            kubectl run dns-debug -n frontend --image=busybox --rm -it --restart=Never -- nslookup api
            # NXDOMAIN -- no "api" Service in frontend

            kubectl run dns-debug -n frontend --image=busybox --rm -it --restart=Never -- nslookup api.backend
            # Success -- resolves to api.backend.svc.cluster.local

            # Fix: use the namespace-qualified name
            curl http://api.backend
          difficulty: 2
        - id: v2
          title: CoreDNS CrashLoopBackOff
          description: >-
            All DNS queries in the cluster are failing. <code>nslookup kubernetes</code> times out from every Pod.
            Write the commands to diagnose and fix the issue, starting with checking CoreDNS status.
          functionSignature: "kubectl commands to diagnose cluster-wide DNS failure"
          testCases:
            - input: "All DNS queries fail from all Pods"
              output: "Check CoreDNS Pods → found CrashLoopBackOff → check logs → restart"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                If DNS fails everywhere, the problem is likely CoreDNS itself. Start by checking if the CoreDNS Pods
                are healthy.
            - title: "\U0001F4A1 Hint"
              content: >-
                Check CoreDNS Pod status, then logs. Common causes: bad ConfigMap, resource limits, or crashloop from
                a forwarding loop.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. kubectl get pods -n kube-system -l k8s-app=kube-dns
                2. kubectl logs -n kube-system -l k8s-app=kube-dns --tail=50
                3. kubectl get configmap coredns -n kube-system -o yaml
                4. kubectl rollout restart deploy/coredns -n kube-system</pre>
          solution: |-
            # Step 1: Check CoreDNS Pod status
            kubectl get pods -n kube-system -l k8s-app=kube-dns
            # If CrashLoopBackOff or not Running:

            # Step 2: Check CoreDNS logs for errors
            kubectl logs -n kube-system -l k8s-app=kube-dns --tail=50

            # Step 3: Check CoreDNS ConfigMap for misconfig
            kubectl get configmap coredns -n kube-system -o yaml

            # Step 4: Restart CoreDNS
            kubectl rollout restart deploy/coredns -n kube-system

            # Step 5: Verify DNS works again
            kubectl run dns-debug --image=busybox --rm -it --restart=Never -- nslookup kubernetes
          difficulty: 2
        - id: v3
          title: NetworkPolicy Blocking DNS
          description: >-
            After applying a restrictive NetworkPolicy, Pods in the <code>secure</code> namespace can no longer
            resolve any DNS names. Other namespaces work fine. The NetworkPolicy allows ingress from the same
            namespace but has no egress rules. Diagnose and fix.
          functionSignature: "Fix NetworkPolicy that blocks DNS egress"
          testCases:
            - input: "Pods in secure namespace, restrictive NetworkPolicy, DNS fails"
              output: "NetworkPolicy blocks egress to CoreDNS. Fix: allow egress to kube-system on port 53."
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                DNS resolution requires the Pod to send a UDP query to CoreDNS (port 53) in <code>kube-system</code>.
                If egress is restricted, this traffic is blocked.
            - title: "\U0001F4A1 Hint"
              content: >-
                Add an egress rule allowing traffic to port 53 (UDP and TCP) in the <code>kube-system</code> namespace
                where CoreDNS runs.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>egress:
                - to:
                  - namespaceSelector:
                      matchLabels:
                        kubernetes.io/metadata.name: kube-system
                  ports:
                  - protocol: UDP
                    port: 53
                  - protocol: TCP
                    port: 53</pre>
          solution: |-
            # Problem: NetworkPolicy restricts egress, blocking DNS
            # queries to CoreDNS in kube-system.

            # Fix: add egress rule for DNS
            apiVersion: networking.k8s.io/v1
            kind: NetworkPolicy
            metadata:
              name: allow-dns-egress
              namespace: secure
            spec:
              podSelector: {}
              policyTypes:
              - Egress
              egress:
              - to:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: kube-system
                ports:
                - protocol: UDP
                  port: 53
                - protocol: TCP
                  port: 53
          difficulty: 3
        - id: v4
          title: Wrong dnsPolicy
          description: >-
            A Pod with <code>hostNetwork: true</code> cannot resolve cluster Service names like
            <code>kubernetes</code> or <code>api.default</code>, but it can resolve external names like
            <code>google.com</code>. Diagnose the issue and write the fix.
          functionSignature: "Fix dnsPolicy for hostNetwork Pod"
          testCases:
            - input: "hostNetwork Pod, cluster DNS fails, external DNS works"
              output: "dnsPolicy defaults to node DNS with hostNetwork. Fix: set dnsPolicy: ClusterFirstWithHostNet"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Pods with <code>hostNetwork: true</code> share the node's network namespace, including its DNS
                configuration. The node's DNS does not know about cluster Services.
            - title: "\U0001F4A1 Hint"
              content: >-
                Set <code>dnsPolicy: ClusterFirstWithHostNet</code> to force the Pod to use CoreDNS even though it
                uses the host network.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>spec:
                  hostNetwork: true
                  dnsPolicy: ClusterFirstWithHostNet</pre>
          solution: |-
            # Problem: hostNetwork Pod uses node DNS, not CoreDNS.
            # Node DNS doesn't know about cluster Services.

            # Fix: add dnsPolicy
            spec:
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              containers:
              - name: app
                image: myapp:v1
          difficulty: 2
        - id: v5
          title: Intermittent DNS Failures
          description: >-
            DNS resolution works sometimes but fails intermittently with timeouts. The cluster has 2 CoreDNS replicas.
            Write the commands to diagnose this issue. Consider: are both replicas healthy? Is there a resource
            constraint? Is there a DNS query loop?
          functionSignature: "kubectl commands to diagnose intermittent DNS failures"
          testCases:
            - input: "DNS works sometimes, fails with timeouts intermittently"
              output: "Check both CoreDNS replicas, check resource usage, check for loop detection"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Intermittent failures suggest one of the CoreDNS replicas might be unhealthy, or CoreDNS is under
                resource pressure (CPU/memory limits too low).
            - title: "\U0001F4A1 Hint"
              content: >-
                Check each CoreDNS Pod individually. Look at resource usage with <code>kubectl top</code>. Check for
                restarts. Also check the CoreDNS ConfigMap for forwarding loops.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
                2. kubectl logs -n kube-system <pod-1> --tail=50
                3. kubectl logs -n kube-system <pod-2> --tail=50
                4. kubectl top pods -n kube-system -l k8s-app=kube-dns
                5. kubectl describe pods -n kube-system -l k8s-app=kube-dns
                6. kubectl get configmap coredns -n kube-system -o yaml</pre>
          solution: |-
            # Step 1: Check all CoreDNS Pods
            kubectl get pods -n kube-system -l k8s-app=kube-dns -o wide
            # Look for restarts, not-Ready, or Pods on same node

            # Step 2: Check logs of each replica
            kubectl logs -n kube-system -l k8s-app=kube-dns --tail=50
            # Look for errors, timeouts, or loop detection messages

            # Step 3: Check resource usage
            kubectl top pods -n kube-system -l k8s-app=kube-dns
            # If near CPU/memory limits, CoreDNS may be throttled

            # Step 4: Check for restarts and events
            kubectl describe pods -n kube-system -l k8s-app=kube-dns
            # Look for OOMKilled, liveness probe failures

            # Step 5: Check CoreDNS config for loop
            kubectl get configmap coredns -n kube-system -o yaml
            # Ensure the "loop" plugin is present to detect forwarding loops

            # Step 6: Consider scaling up CoreDNS
            kubectl scale deploy/coredns -n kube-system --replicas=3
          difficulty: 3
        - id: v6
          title: NXDOMAIN for Existing Service
          description: >-
            A Service named <code>auth</code> exists in namespace <code>platform</code> (confirmed by
            <code>kubectl get svc -n platform</code>), but DNS resolution returns NXDOMAIN. Write the systematic
            debugging steps and identify possible causes.
          functionSignature: "Debug NXDOMAIN for a verified existing Service"
          testCases:
            - input: "kubectl get svc auth -n platform returns the Service, nslookup auth.platform returns NXDOMAIN"
              output: "Check CoreDNS, check endpoints, check Service has selector, check for typos in DNS name"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The Service exists, but DNS does not resolve it. Possible causes: CoreDNS is not syncing, the Service
                was just created (DNS cache), or there is a CoreDNS configuration issue.
            - title: "\U0001F4A1 Hint"
              content: >-
                Check CoreDNS logs for errors. Try the full FQDN. Check if the Service has endpoints. CoreDNS watches
                the Kubernetes API -- if the watch is broken, new Services will not resolve.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Verify FQDN: nslookup auth.platform.svc.cluster.local
                2. Check CoreDNS: kubectl logs -n kube-system -l k8s-app=kube-dns
                3. Check endpoints: kubectl get endpoints auth -n platform
                4. Restart CoreDNS: kubectl rollout restart deploy/coredns -n kube-system
                5. Check Kubernetes API connectivity from CoreDNS</pre>
          solution: |-
            # Step 1: Try full FQDN to rule out search domain issues
            kubectl run dns-debug --image=busybox --rm -it --restart=Never -- \
              nslookup auth.platform.svc.cluster.local

            # Step 2: Check CoreDNS logs
            kubectl logs -n kube-system -l k8s-app=kube-dns --tail=100
            # Look for API connection errors or watch failures

            # Step 3: Verify Service and Endpoints
            kubectl get svc auth -n platform
            kubectl get endpoints auth -n platform

            # Step 4: Check CoreDNS can reach the API server
            kubectl get configmap coredns -n kube-system -o yaml
            # Verify the kubernetes plugin is configured

            # Step 5: Restart CoreDNS to re-sync
            kubectl rollout restart deploy/coredns -n kube-system

            # Step 6: Wait and re-test
            kubectl run dns-debug --image=busybox --rm -it --restart=Never -- \
              nslookup auth.platform.svc.cluster.local
          difficulty: 2
        - id: v7
          title: Slow External DNS Resolution
          description: >-
            An application Pod reports that external API calls to <code>api.stripe.com</code> are slow (3-4 seconds
            initial connection). Internal Service calls are fast. Diagnose the DNS performance issue and write the
            fix.
          functionSignature: "Diagnose and fix slow external DNS resolution"
          testCases:
            - input: "api.stripe.com slow (3-4s), internal services fast"
              output: "ndots:5 causes 3 extra DNS queries for external domains. Fix: lower ndots or use trailing dot."
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                External domains have few dots. With <code>ndots:5</code>, the resolver tries appending search domains
                first, generating multiple failed lookups before resolving the external name.
            - title: "\U0001F4A1 Hint"
              content: >-
                <code>api.stripe.com</code> has 2 dots. With <code>ndots:5</code>, the resolver tries 3 search domain
                expansions (all NXDOMAIN) before trying the name directly. Each failed lookup adds latency.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Diagnosis:
                  1. Check ndots: kubectl exec pod -- cat /etc/resolv.conf
                  2. Count dots in external domain (api.stripe.com = 2)
                  3. 2 < 5 (ndots), so search domains are tried first

                Fix options:
                  1. Use trailing dot: api.stripe.com.
                  2. Lower ndots in Pod spec:
                     dnsConfig:
                       options:
                       - name: ndots
                         value: "2"</pre>
          solution: |-
            # Diagnosis: ndots:5 causes extra lookups
            kubectl exec <pod> -- cat /etc/resolv.conf
            # options ndots:5
            # api.stripe.com has 2 dots < 5
            # Resolver tries: api.stripe.com.ns.svc.cluster.local (fail)
            #                 api.stripe.com.svc.cluster.local (fail)
            #                 api.stripe.com.cluster.local (fail)
            #                 api.stripe.com (success)
            # = 3 wasted DNS queries adding ~3 seconds

            # Fix: lower ndots in Pod spec
            spec:
              dnsConfig:
                options:
                - name: ndots
                  value: "2"
              containers:
              - name: app
                image: myapp:v1

            # Or: use trailing dot in application config
            # api.stripe.com.
          difficulty: 3
        - id: v8
          title: DNS Policy None Misconfiguration
          description: >-
            A Pod was configured with <code>dnsPolicy: None</code> but the developer forgot to add
            <code>dnsConfig</code>. The Pod cannot resolve any DNS names at all -- not cluster Services, not external
            domains. Write the diagnosis commands and the fix.
          functionSignature: "Diagnose and fix dnsPolicy None without dnsConfig"
          testCases:
            - input: "dnsPolicy: None, no dnsConfig, all DNS fails"
              output: "Empty resolv.conf. Fix: add dnsConfig with nameservers, or change dnsPolicy to ClusterFirst."
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                <code>dnsPolicy: None</code> tells Kubernetes to not configure DNS at all. Without
                <code>dnsConfig</code>, the Pod has an empty <code>/etc/resolv.conf</code>.
            - title: "\U0001F4A1 Hint"
              content: >-
                Check <code>/etc/resolv.conf</code> in the Pod. It will be empty. Either add <code>dnsConfig</code>
                with the CoreDNS IP, or change <code>dnsPolicy</code> back to <code>ClusterFirst</code>.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Diagnosis:
                  kubectl exec pod -- cat /etc/resolv.conf
                  # (empty or missing nameserver)

                Fix option 1: Add dnsConfig
                  dnsPolicy: None
                  dnsConfig:
                    nameservers: ["10.96.0.10"]
                    searches: ["default.svc.cluster.local"]

                Fix option 2: Change policy
                  dnsPolicy: ClusterFirst</pre>
          solution: |-
            # Diagnosis:
            kubectl exec <pod> -- cat /etc/resolv.conf
            # Empty or no nameserver -- nothing to resolve against

            # Fix option 1: Add dnsConfig (if None is intentional)
            spec:
              dnsPolicy: None
              dnsConfig:
                nameservers:
                - 10.96.0.10
                searches:
                - default.svc.cluster.local
                - svc.cluster.local
                - cluster.local
                options:
                - name: ndots
                  value: "5"

            # Fix option 2: Change to ClusterFirst (simpler)
            spec:
              dnsPolicy: ClusterFirst
          difficulty: 2
