conceptLinks:
  Pod Spec: "#lesson-the-pod-spec"
  Environment Variables: "#lesson-environment-variables"
  Multi-Container Patterns: "#lesson-multi-container-patterns"
  Init Containers: "#lesson-init-containers"
  Sidecar Containers: "#lesson-sidecar-containers"
  Ambassador Pattern: "#lesson-ambassador-pattern"
  Resource Requests and Limits: "#lesson-resource-requests-and-limits"
  QoS Classes: "#lesson-quality-of-service-qos-classes"
  Pod Lifecycle: "#lesson-pod-lifecycle"
  Debugging Pods: "#lesson-debugging-pods"
sharedContent: {}
variants:
  warmups:
    - id: warmup_1
      concept: Pod Spec
      variants:
        - id: v1
          title: Nginx Pod
          description: >-
            Write a Pod manifest that runs an <code>nginx:1.25</code> container named <code>web</code> on port 80.
            The Pod should be named <code>web-server</code> with a label <code>app: web</code>.
          hints:
            - "Use <code>apiVersion: v1</code> and <code>kind: Pod</code>"
            - "The containers array goes under <code>spec</code>"
            - "<code>containerPort</code> goes inside the <code>ports</code> array on the container"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: web-server
              labels:
                app: web
            spec:
              containers:
              - name: web
                image: nginx:1.25
                ports:
                - containerPort: 80
        - id: v2
          title: Redis Pod
          description: >-
            Write a Pod manifest that runs a <code>redis:7.2</code> container named <code>cache</code> on port 6379.
            The Pod should be named <code>redis-cache</code> with labels <code>app: redis</code> and <code>tier: cache</code>.
          hints:
            - "Redis listens on port 6379 by default"
            - "Labels are key-value pairs under <code>metadata.labels</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: redis-cache
              labels:
                app: redis
                tier: cache
            spec:
              containers:
              - name: cache
                image: redis:7.2
                ports:
                - containerPort: 6379
        - id: v3
          title: Custom Command Pod
          description: >-
            Write a Pod named <code>hello-pod</code> that runs a <code>busybox:1.36</code> container named
            <code>greeter</code>. Override the command to run <code>echo "Hello from Kubernetes!"</code> and
            set <code>restartPolicy: Never</code>.
          hints:
            - "Use the <code>command</code> field to override the entrypoint"
            - "<code>restartPolicy</code> goes under <code>spec</code>, not under the container"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: hello-pod
            spec:
              restartPolicy: Never
              containers:
              - name: greeter
                image: busybox:1.36
                command: ["echo", "Hello from Kubernetes!"]
        - id: v4
          title: Pod with Args
          description: >-
            Write a Pod named <code>sleeper</code> running <code>busybox:1.36</code>. Use <code>command</code> to set
            the entrypoint to <code>["/bin/sh", "-c"]</code> and <code>args</code> to
            <code>["echo Starting && sleep 3600"]</code>. Set <code>restartPolicy: OnFailure</code>.
          hints:
            - "<code>command</code> overrides the image ENTRYPOINT, <code>args</code> overrides CMD"
            - "The shell (<code>/bin/sh -c</code>) lets you chain commands with <code>&&</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: sleeper
            spec:
              restartPolicy: OnFailure
              containers:
              - name: sleeper
                image: busybox:1.36
                command: ["/bin/sh", "-c"]
                args: ["echo Starting && sleep 3600"]
        - id: v5
          title: Named Port Pod
          description: >-
            Write a Pod named <code>api-server</code> running <code>myapp/api:v2.0</code> with a container named
            <code>api</code>. Expose port 8080 with the name <code>http-api</code> and port 9090 with the name
            <code>metrics</code>. Add labels <code>app: api</code> and <code>version: v2</code>.
          hints:
            - "Each port entry can have a <code>name</code> field"
            - "Port names are used by Services to reference specific ports"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: api-server
              labels:
                app: api
                version: v2
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                ports:
                - containerPort: 8080
                  name: http-api
                - containerPort: 9090
                  name: metrics
        - id: v6
          title: Pod with Volume Mount
          description: >-
            Write a Pod named <code>data-pod</code> running <code>nginx:1.25</code>. Mount an <code>emptyDir</code>
            volume named <code>html</code> at <code>/usr/share/nginx/html</code>. Add the label <code>app: web</code>.
          hints:
            - "Define the volume under <code>spec.volumes</code>"
            - "Reference it by name in <code>volumeMounts</code> on the container"
            - "<code>emptyDir: {}</code> creates an ephemeral volume that lives as long as the Pod"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: data-pod
              labels:
                app: web
            spec:
              containers:
              - name: web
                image: nginx:1.25
                volumeMounts:
                - name: html
                  mountPath: /usr/share/nginx/html
              volumes:
              - name: html
                emptyDir: {}
        - id: v7
          title: Pod with Annotations
          description: >-
            Write a Pod named <code>monitored-app</code> running <code>myapp/web:v1.0</code>. Add labels
            <code>app: web</code> and <code>team: platform</code>. Add annotations
            <code>prometheus.io/scrape: "true"</code> and <code>prometheus.io/port: "9090"</code>.
          hints:
            - "Annotations go under <code>metadata.annotations</code>"
            - "Annotation values must be strings -- wrap numbers in quotes"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: monitored-app
              labels:
                app: web
                team: platform
              annotations:
                prometheus.io/scrape: "true"
                prometheus.io/port: "9090"
            spec:
              containers:
              - name: web
                image: myapp/web:v1.0
        - id: v8
          title: PostgreSQL Pod
          description: >-
            Write a Pod named <code>postgres</code> running <code>postgres:16</code> with a container named
            <code>db</code> on port 5432. Set the environment variable <code>POSTGRES_PASSWORD</code> to
            <code>"mysecretpassword"</code>. Add the label <code>app: postgres</code>.
          hints:
            - "PostgreSQL requires the <code>POSTGRES_PASSWORD</code> env var to be set"
            - "Use the <code>env</code> array with <code>name</code> and <code>value</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: postgres
              labels:
                app: postgres
            spec:
              containers:
              - name: db
                image: postgres:16
                ports:
                - containerPort: 5432
                env:
                - name: POSTGRES_PASSWORD
                  value: "mysecretpassword"
        - id: v9
          title: Multi-Label Selector Pod
          description: >-
            Write a Pod named <code>frontend</code> running <code>nginx:1.25</code>. Add four labels:
            <code>app: frontend</code>, <code>tier: web</code>, <code>environment: production</code>,
            and <code>version: v3.1</code>.
          hints:
            - "Labels are key-value pairs -- you can add as many as you need"
            - "All labels go under <code>metadata.labels</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: frontend
              labels:
                app: frontend
                tier: web
                environment: production
                version: v3.1
            spec:
              containers:
              - name: web
                image: nginx:1.25
        - id: v10
          title: Pod with Read-Only Volume
          description: >-
            Write a Pod named <code>config-reader</code> running <code>busybox:1.36</code> with
            <code>command: ["sleep", "3600"]</code>. Mount an <code>emptyDir</code> volume named
            <code>config</code> at <code>/etc/app-config</code> with <code>readOnly: true</code>.
          hints:
            - "Set <code>readOnly: true</code> on the <code>volumeMounts</code> entry"
            - "This prevents the container from writing to the volume"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: config-reader
            spec:
              containers:
              - name: reader
                image: busybox:1.36
                command: ["sleep", "3600"]
                volumeMounts:
                - name: config
                  mountPath: /etc/app-config
                  readOnly: true
              volumes:
              - name: config
                emptyDir: {}
        - id: v11
          title: Pod with Two Containers
          description: >-
            Write a Pod named <code>dual-app</code> with two containers: <code>web</code> running
            <code>nginx:1.25</code> on port 80, and <code>api</code> running <code>myapp/api:v1.0</code>
            on port 8080. Add label <code>app: dual-app</code>.
          hints:
            - "List both containers in the <code>spec.containers</code> array"
            - "Each container must have a unique <code>name</code>"
            - "Both containers share the same network namespace (localhost)"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: dual-app
              labels:
                app: dual-app
            spec:
              containers:
              - name: web
                image: nginx:1.25
                ports:
                - containerPort: 80
              - name: api
                image: myapp/api:v1.0
                ports:
                - containerPort: 8080
        - id: v12
          title: Pod in a Namespace
          description: >-
            Write a Pod named <code>backend-api</code> in the namespace <code>staging</code>. Run
            <code>myapp/backend:v2.5</code> on port 3000. Add labels <code>app: backend</code>
            and <code>environment: staging</code>.
          hints:
            - "Set <code>namespace</code> under <code>metadata</code>"
            - "The namespace must already exist in the cluster"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: backend-api
              namespace: staging
              labels:
                app: backend
                environment: staging
            spec:
              containers:
              - name: backend
                image: myapp/backend:v2.5
                ports:
                - containerPort: 3000
    - id: warmup_2
      concept: Environment Variables
      variants:
        - id: v1
          title: Direct Env Vars
          description: >-
            Write a Pod named <code>app-pod</code> running <code>myapp/web:v1.0</code>. Set two environment variables
            directly: <code>APP_ENV=production</code> and <code>LOG_LEVEL=info</code>.
          hints:
            - "Use the <code>env</code> array with <code>name</code> and <code>value</code> for each variable"
            - "Environment variable values must be strings"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: app-pod
            spec:
              containers:
              - name: app
                image: myapp/web:v1.0
                env:
                - name: APP_ENV
                  value: "production"
                - name: LOG_LEVEL
                  value: "info"
        - id: v2
          title: Env from ConfigMap Key
          description: >-
            Write a Pod named <code>config-app</code> running <code>myapp/web:v1.0</code>. Set the environment
            variable <code>DATABASE_URL</code> from a ConfigMap named <code>app-config</code>, key
            <code>database_url</code>.
          hints:
            - "Use <code>valueFrom.configMapKeyRef</code> instead of <code>value</code>"
            - "Specify the ConfigMap <code>name</code> and the <code>key</code> within it"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: config-app
            spec:
              containers:
              - name: app
                image: myapp/web:v1.0
                env:
                - name: DATABASE_URL
                  valueFrom:
                    configMapKeyRef:
                      name: app-config
                      key: database_url
        - id: v3
          title: Env from Secret Key
          description: >-
            Write a Pod named <code>secure-app</code> running <code>myapp/api:v2.0</code>. Set the environment
            variable <code>DB_PASSWORD</code> from a Secret named <code>db-credentials</code>, key
            <code>password</code>.
          hints:
            - "Use <code>valueFrom.secretKeyRef</code> to reference a Secret"
            - "The structure is the same as <code>configMapKeyRef</code> but with <code>secretKeyRef</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: secure-app
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                env:
                - name: DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-credentials
                      key: password
        - id: v4
          title: Downward API - Pod Name and Namespace
          description: >-
            Write a Pod named <code>self-aware</code> running <code>busybox:1.36</code> with
            <code>command: ["sleep", "3600"]</code>. Use the Downward API to inject the Pod name as
            <code>POD_NAME</code> and namespace as <code>POD_NAMESPACE</code>.
          hints:
            - "Use <code>valueFrom.fieldRef.fieldPath</code> to reference Pod metadata"
            - "<code>metadata.name</code> for the Pod name, <code>metadata.namespace</code> for the namespace"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: self-aware
            spec:
              containers:
              - name: app
                image: busybox:1.36
                command: ["sleep", "3600"]
                env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.name
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
        - id: v5
          title: Downward API - Pod IP and Node Name
          description: >-
            Write a Pod named <code>network-info</code> running <code>myapp/web:v1.0</code>. Use the Downward API
            to inject the Pod IP as <code>POD_IP</code> and the node name as <code>NODE_NAME</code>.
          hints:
            - "Use <code>fieldRef</code> with <code>fieldPath: status.podIP</code> for the Pod IP"
            - "Use <code>fieldRef</code> with <code>fieldPath: spec.nodeName</code> for the node name"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: network-info
            spec:
              containers:
              - name: web
                image: myapp/web:v1.0
                env:
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
        - id: v6
          title: Mixed Env Sources
          description: >-
            Write a Pod named <code>mixed-env</code> running <code>myapp/api:v2.0</code>. Set three env vars:
            <code>APP_ENV=production</code> (direct), <code>DB_HOST</code> from ConfigMap <code>app-config</code>
            key <code>db_host</code>, and <code>DB_PASSWORD</code> from Secret <code>db-creds</code> key
            <code>password</code>.
          hints:
            - "You can mix <code>value</code>, <code>configMapKeyRef</code>, and <code>secretKeyRef</code> in the same <code>env</code> array"
            - "Each entry uses either <code>value</code> or <code>valueFrom</code>, never both"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: mixed-env
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                env:
                - name: APP_ENV
                  value: "production"
                - name: DB_HOST
                  valueFrom:
                    configMapKeyRef:
                      name: app-config
                      key: db_host
                - name: DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-creds
                      key: password
        - id: v7
          title: Downward API - Resource Limits
          description: >-
            Write a Pod named <code>resource-aware</code> running <code>myapp/api:v1.0</code> with container name
            <code>api</code>. Set CPU request <code>250m</code> and memory limit <code>512Mi</code>. Use the
            Downward API to inject the CPU limit as <code>CPU_LIMIT</code> via <code>resourceFieldRef</code>.
          hints:
            - "Use <code>valueFrom.resourceFieldRef</code> with <code>containerName</code> and <code>resource</code>"
            - "The resource field is <code>limits.cpu</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: resource-aware
            spec:
              containers:
              - name: api
                image: myapp/api:v1.0
                resources:
                  requests:
                    cpu: "250m"
                  limits:
                    memory: "512Mi"
                env:
                - name: CPU_LIMIT
                  valueFrom:
                    resourceFieldRef:
                      containerName: api
                      resource: limits.cpu
        - id: v8
          title: Multiple ConfigMap Keys
          description: >-
            Write a Pod named <code>full-config</code> running <code>myapp/web:v1.0</code>. Set three env vars
            from a ConfigMap named <code>web-config</code>: <code>DB_HOST</code> from key <code>db_host</code>,
            <code>DB_PORT</code> from key <code>db_port</code>, and <code>CACHE_TTL</code> from key
            <code>cache_ttl</code>.
          hints:
            - "Each env var gets its own <code>configMapKeyRef</code> entry"
            - "All three reference the same ConfigMap name but different keys"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: full-config
            spec:
              containers:
              - name: web
                image: myapp/web:v1.0
                env:
                - name: DB_HOST
                  valueFrom:
                    configMapKeyRef:
                      name: web-config
                      key: db_host
                - name: DB_PORT
                  valueFrom:
                    configMapKeyRef:
                      name: web-config
                      key: db_port
                - name: CACHE_TTL
                  valueFrom:
                    configMapKeyRef:
                      name: web-config
                      key: cache_ttl
        - id: v9
          title: Env with Downward API Labels
          description: >-
            Write a Pod named <code>labeled-app</code> with label <code>app: web</code> running
            <code>busybox:1.36</code> with <code>command: ["sleep", "3600"]</code>. Inject the Pod's
            labels as <code>POD_LABELS</code> using <code>fieldRef</code> with
            <code>fieldPath: metadata.labels</code>.
          hints:
            - "Use <code>fieldRef</code> with <code>fieldPath: metadata.labels</code>"
            - "This injects all labels as a formatted string"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: labeled-app
              labels:
                app: web
            spec:
              containers:
              - name: app
                image: busybox:1.36
                command: ["sleep", "3600"]
                env:
                - name: POD_LABELS
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.labels
        - id: v10
          title: Env from Secret and ConfigMap Combined
          description: >-
            Write a Pod named <code>db-client</code> running <code>myapp/client:v1.0</code>. Set
            <code>DB_HOST</code> from ConfigMap <code>db-config</code> key <code>host</code>,
            <code>DB_PORT</code> from ConfigMap <code>db-config</code> key <code>port</code>,
            <code>DB_USER</code> from Secret <code>db-auth</code> key <code>username</code>,
            and <code>DB_PASSWORD</code> from Secret <code>db-auth</code> key <code>password</code>.
          hints:
            - "Non-sensitive config goes in ConfigMaps, sensitive credentials go in Secrets"
            - "The pattern is the same -- just switch between <code>configMapKeyRef</code> and <code>secretKeyRef</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: db-client
            spec:
              containers:
              - name: client
                image: myapp/client:v1.0
                env:
                - name: DB_HOST
                  valueFrom:
                    configMapKeyRef:
                      name: db-config
                      key: host
                - name: DB_PORT
                  valueFrom:
                    configMapKeyRef:
                      name: db-config
                      key: port
                - name: DB_USER
                  valueFrom:
                    secretKeyRef:
                      name: db-auth
                      key: username
                - name: DB_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-auth
                      key: password
    - id: warmup_3
      concept: Resource Requests and Limits
      variants:
        - id: v1
          title: Basic Requests and Limits
          description: >-
            Write a Pod named <code>resource-pod</code> running <code>nginx:1.25</code>. Set CPU request
            <code>100m</code>, CPU limit <code>500m</code>, memory request <code>128Mi</code>, and memory limit
            <code>256Mi</code>.
          hints:
            - "Resources go under <code>containers[].resources</code>"
            - "<code>requests</code> for scheduling guarantees, <code>limits</code> for hard ceilings"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: resource-pod
            spec:
              containers:
              - name: web
                image: nginx:1.25
                resources:
                  requests:
                    cpu: "100m"
                    memory: "128Mi"
                  limits:
                    cpu: "500m"
                    memory: "256Mi"
        - id: v2
          title: Guaranteed QoS Pod
          description: >-
            Write a Pod named <code>guaranteed-pod</code> running <code>myapp/api:v2.0</code> that will receive
            the <strong>Guaranteed</strong> QoS class. Use CPU <code>500m</code> and memory <code>256Mi</code>.
          hints:
            - "Guaranteed QoS requires requests == limits for both CPU and memory"
            - "Set identical values for requests and limits"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: guaranteed-pod
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                resources:
                  requests:
                    cpu: "500m"
                    memory: "256Mi"
                  limits:
                    cpu: "500m"
                    memory: "256Mi"
        - id: v3
          title: BestEffort QoS Pod
          description: >-
            Write a Pod named <code>besteffort-pod</code> running <code>busybox:1.36</code> with
            <code>command: ["sleep", "3600"]</code>. It should receive the <strong>BestEffort</strong> QoS class.
          hints:
            - "BestEffort means no resource requests or limits are set at all"
            - "Simply omit the entire <code>resources</code> block"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: besteffort-pod
            spec:
              containers:
              - name: app
                image: busybox:1.36
                command: ["sleep", "3600"]
        - id: v4
          title: Identify the QoS Class
          description: >-
            A Pod has one container with <code>requests: {cpu: "250m", memory: "128Mi"}</code> and
            <code>limits: {cpu: "1", memory: "512Mi"}</code>. What QoS class will it get? Write the QoS class name.
          hints:
            - "Compare requests to limits -- are they equal?"
            - "If requests != limits, but at least one is set, it's Burstable"
          solution: |-
            Burstable
        - id: v5
          title: High-Memory Pod
          description: >-
            Write a Pod named <code>ml-worker</code> running <code>myapp/ml:v1.0</code>. Set CPU request
            <code>2</code> (2 full cores), CPU limit <code>4</code>, memory request <code>4Gi</code>, and memory
            limit <code>8Gi</code>.
          hints:
            - "You can use whole numbers for CPU: <code>\"2\"</code> means 2 full cores (= 2000m)"
            - "Use <code>Gi</code> for gibibytes"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: ml-worker
            spec:
              containers:
              - name: ml
                image: myapp/ml:v1.0
                resources:
                  requests:
                    cpu: "2"
                    memory: "4Gi"
                  limits:
                    cpu: "4"
                    memory: "8Gi"
        - id: v6
          title: Requests Only (No Limits)
          description: >-
            Write a Pod named <code>flexible-app</code> running <code>myapp/web:v1.0</code>. Set only requests:
            CPU <code>100m</code> and memory <code>64Mi</code>. Do not set any limits. What QoS class will
            this Pod get?
          hints:
            - "You can set <code>requests</code> without setting <code>limits</code>"
            - "Since at least one resource field is set but requests != limits, it's Burstable"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: flexible-app
            spec:
              containers:
              - name: web
                image: myapp/web:v1.0
                resources:
                  requests:
                    cpu: "100m"
                    memory: "64Mi"
            # QoS Class: Burstable
        - id: v7
          title: Multi-Container Guaranteed
          description: >-
            Write a Pod named <code>guaranteed-multi</code> with two containers: <code>web</code>
            (<code>nginx:1.25</code>, CPU 200m, memory 128Mi) and <code>sidecar</code>
            (<code>busybox:1.36</code> with <code>command: ["sleep", "3600"]</code>, CPU 50m, memory 32Mi).
            Both should have Guaranteed QoS. What must be true about requests and limits?
          hints:
            - "For Guaranteed QoS, EVERY container must have requests == limits for both CPU and memory"
            - "Set the same values for requests and limits on both containers"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: guaranteed-multi
            spec:
              containers:
              - name: web
                image: nginx:1.25
                resources:
                  requests:
                    cpu: "200m"
                    memory: "128Mi"
                  limits:
                    cpu: "200m"
                    memory: "128Mi"
              - name: sidecar
                image: busybox:1.36
                command: ["sleep", "3600"]
                resources:
                  requests:
                    cpu: "50m"
                    memory: "32Mi"
                  limits:
                    cpu: "50m"
                    memory: "32Mi"
        - id: v8
          title: Identify QoS - Limits Only
          description: >-
            A Pod has one container with only <code>limits: {cpu: "500m", memory: "256Mi"}</code> and no explicit
            requests. What QoS class does it get? (Hint: when you set limits without requests, Kubernetes
            automatically sets requests equal to limits.)
          hints:
            - "When you only set limits, K8s auto-sets requests = limits"
            - "Since requests == limits, the QoS class is Guaranteed"
          solution: |-
            Guaranteed
        - id: v9
          title: Burstable with Partial Resources
          description: >-
            Write a Pod named <code>partial-pod</code> with two containers. Container <code>app</code>
            (<code>myapp/web:v1.0</code>) has <code>requests: {cpu: "250m", memory: "128Mi"}</code> and
            <code>limits: {cpu: "500m", memory: "256Mi"}</code>. Container <code>helper</code>
            (<code>busybox:1.36</code> with <code>command: ["sleep", "3600"]</code>) has no resources set.
            What QoS class?
          hints:
            - "If any container has resources and any container does not, it's Burstable"
            - "BestEffort requires NO container to have any resources set"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: partial-pod
            spec:
              containers:
              - name: app
                image: myapp/web:v1.0
                resources:
                  requests:
                    cpu: "250m"
                    memory: "128Mi"
                  limits:
                    cpu: "500m"
                    memory: "256Mi"
              - name: helper
                image: busybox:1.36
                command: ["sleep", "3600"]
            # QoS Class: Burstable
        - id: v10
          title: Tiny Sidecar Resources
          description: >-
            Write a Pod named <code>efficient-pod</code> running <code>myapp/api:v1.0</code> with container
            <code>api</code>. Set request <code>cpu: "50m"</code>, <code>memory: "32Mi"</code> and limits
            <code>cpu: "200m"</code>, <code>memory: "128Mi"</code>. This is a typical lightweight sidecar
            resource profile.
          hints:
            - "Sidecars should use minimal resources so they don't compete with the main container"
            - "<code>50m</code> CPU is 5% of a core -- very lightweight"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: efficient-pod
            spec:
              containers:
              - name: api
                image: myapp/api:v1.0
                resources:
                  requests:
                    cpu: "50m"
                    memory: "32Mi"
                  limits:
                    cpu: "200m"
                    memory: "128Mi"
    - id: warmup_4
      concept: Debugging Pods
      variants:
        - id: v1
          title: Describe a Pod
          description: >-
            Write the <code>kubectl</code> command to view full details and events for a Pod named
            <code>broken-app</code> in the <code>default</code> namespace.
          hints:
            - "<code>kubectl describe pod</code> shows full spec, status, conditions, and events"
            - "Events at the bottom are the most useful for debugging"
          solution: |-
            kubectl describe pod broken-app
        - id: v2
          title: View Pod Logs
          description: >-
            Write the <code>kubectl</code> command to view the logs of a Pod named <code>web-server</code>.
          hints:
            - "<code>kubectl logs</code> shows the stdout/stderr output of the container"
          solution: |-
            kubectl logs web-server
        - id: v3
          title: Logs from Previous Container
          description: >-
            A Pod named <code>crash-pod</code> is in CrashLoopBackOff. Write the <code>kubectl</code> command
            to see the logs from the previous (crashed) container.
          hints:
            - "The current container has no logs because it hasn't started yet"
            - "Use <code>--previous</code> to see logs from the last crashed run"
          solution: |-
            kubectl logs crash-pod --previous
        - id: v4
          title: Logs from Specific Container
          description: >-
            Write the <code>kubectl</code> command to view logs from the container named <code>sidecar</code>
            in a multi-container Pod named <code>multi-pod</code>.
          hints:
            - "Use <code>-c</code> to specify which container's logs to view"
          solution: |-
            kubectl logs multi-pod -c sidecar
        - id: v5
          title: Exec into a Pod
          description: >-
            Write the <code>kubectl</code> command to open an interactive shell (<code>/bin/sh</code>) in a Pod
            named <code>debug-pod</code>.
          hints:
            - "Use <code>kubectl exec</code> with <code>-it</code> for interactive terminal"
            - "The <code>--</code> separates kubectl args from the command to run"
          solution: |-
            kubectl exec -it debug-pod -- /bin/sh
        - id: v6
          title: Exec into Specific Container
          description: >-
            Write the <code>kubectl</code> command to open a bash shell in the container named
            <code>web</code> in a Pod named <code>fullstack-pod</code>.
          hints:
            - "Use <code>-c</code> to target a specific container, <code>-it</code> for interactive mode"
          solution: |-
            kubectl exec -it fullstack-pod -c web -- /bin/bash
        - id: v7
          title: Check Environment Variables
          description: >-
            Write the <code>kubectl</code> command to print all environment variables from a Pod named
            <code>app-pod</code> (run the <code>env</code> command inside the container).
          hints:
            - "Use <code>kubectl exec</code> to run a command inside the Pod"
            - "The <code>env</code> command prints all environment variables"
          solution: |-
            kubectl exec app-pod -- env
        - id: v8
          title: Follow Logs in Real Time
          description: >-
            Write the <code>kubectl</code> command to follow (stream) the logs in real time from a Pod named
            <code>web-app</code>, showing only the last 100 lines.
          hints:
            - "Use <code>-f</code> to follow (like <code>tail -f</code>)"
            - "Use <code>--tail=N</code> to start from the last N lines"
          solution: |-
            kubectl logs web-app -f --tail=100
        - id: v9
          title: Check Pod QoS Class
          description: >-
            Write the <code>kubectl</code> command to print only the QoS class of a Pod named
            <code>my-pod</code> using <code>jsonpath</code>.
          hints:
            - "Use <code>-o jsonpath</code> to extract specific fields from the Pod"
            - "The QoS class is at <code>.status.qosClass</code>"
          solution: |-
            kubectl get pod my-pod -o jsonpath='{.status.qosClass}'
        - id: v10
          title: Get Pod Events
          description: >-
            Write the <code>kubectl</code> command to list all events in the <code>default</code> namespace
            sorted by timestamp, which is useful for debugging Pod scheduling and lifecycle issues.
          hints:
            - "Use <code>kubectl get events</code>"
            - "Sort by <code>.metadata.creationTimestamp</code> or <code>.lastTimestamp</code>"
          solution: |-
            kubectl get events --sort-by=.metadata.creationTimestamp
        - id: v11
          title: Check OOMKilled Status
          description: >-
            Write the <code>kubectl</code> command to check if a Pod named <code>memory-hog</code> was
            OOMKilled. Use jsonpath to extract the last termination reason from the first container.
          hints:
            - "The termination reason is at <code>.status.containerStatuses[0].lastState.terminated.reason</code>"
            - "Use <code>-o jsonpath</code> to extract it"
          solution: |-
            kubectl get pod memory-hog -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'
        - id: v12
          title: Logs Since a Time Window
          description: >-
            Write the <code>kubectl</code> command to view logs from the last 30 minutes for a Pod named
            <code>api-server</code>.
          hints:
            - "Use <code>--since</code> to filter logs by time"
            - "The format is a duration like <code>30m</code>, <code>1h</code>, <code>2h30m</code>"
          solution: |-
            kubectl logs api-server --since=30m
  challenges:
    - id: challenge_1
      block: 1
      difficulty: 3
      concept: Multi-Container Patterns
      variants:
        - id: v1
          title: Init Container - Wait for Database
          description: >-
            Write a Pod named <code>web-app</code> with an init container that waits for PostgreSQL to be available
            at <code>postgres.default.svc.cluster.local:5432</code> using <code>busybox:1.36</code> and
            <code>nc -z</code>. Once ready, the main container <code>web</code> runs <code>myapp/web:v1.0</code>
            on port 8080.
          functionSignature: "Kind: Pod with initContainers"
          testCases:
            - input: "Init container checks postgres availability"
              output: "Main container starts only after postgres is reachable"
            - input: "kubectl get pod web-app (during init)"
              output: "STATUS: Init:0/1"
          hints:
            - title: "Think about it"
              content: >-
                Init containers run before the main containers. Use a shell loop with <code>nc -z</code>
                to check TCP connectivity until the database is ready.
            - title: "Hint"
              content: >-
                Use <code>command: ["sh", "-c"]</code> with an <code>until nc -z HOST PORT; do sleep 2; done</code>
                loop.
            - title: "Pattern"
              content: |-
                <pre>initContainers:
                - name: wait-for-db
                  image: busybox:1.36
                  command: ["sh", "-c", "until nc -z HOST PORT; do sleep 2; done"]</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: web-app
            spec:
              initContainers:
              - name: wait-for-db
                image: busybox:1.36
                command:
                - sh
                - -c
                - |
                  until nc -z postgres.default.svc.cluster.local 5432; do
                    echo "Waiting for postgres..."
                    sleep 2
                  done
              containers:
              - name: web
                image: myapp/web:v1.0
                ports:
                - containerPort: 8080
          difficulty: 2
        - id: v2
          title: Sidecar - Log Collector
          description: >-
            Write a Pod named <code>web-with-logging</code> with two containers. The main container
            <code>web</code> runs <code>nginx:1.25</code> on port 80 and writes logs to
            <code>/var/log/nginx</code>. A sidecar container <code>log-shipper</code> runs
            <code>fluent/fluent-bit:2.2</code> and reads from <code>/var/log/nginx</code> (read-only).
            Use a shared <code>emptyDir</code> volume named <code>logs</code>.
          functionSignature: "Kind: Pod with sidecar"
          testCases:
            - input: "nginx writes access logs to /var/log/nginx"
              output: "fluent-bit reads logs from the same path via shared volume"
            - input: "kubectl get pod web-with-logging"
              output: "READY: 2/2"
          hints:
            - title: "Think about it"
              content: >-
                Both containers mount the same <code>emptyDir</code> volume. The web container writes to it,
                the sidecar reads from it. Use <code>readOnly: true</code> on the sidecar's mount.
            - title: "Hint"
              content: >-
                Define one volume under <code>spec.volumes</code> and reference it by name in both containers'
                <code>volumeMounts</code>.
            - title: "Pattern"
              content: |-
                <pre>volumes:
                - name: logs
                  emptyDir: {}
                # Both containers mount "logs" at the same or different paths</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: web-with-logging
            spec:
              containers:
              - name: web
                image: nginx:1.25
                ports:
                - containerPort: 80
                volumeMounts:
                - name: logs
                  mountPath: /var/log/nginx
              - name: log-shipper
                image: fluent/fluent-bit:2.2
                volumeMounts:
                - name: logs
                  mountPath: /var/log/nginx
                  readOnly: true
              volumes:
              - name: logs
                emptyDir: {}
          difficulty: 2
        - id: v3
          title: Ambassador - Redis Proxy
          description: >-
            Write a Pod named <code>app-with-proxy</code>. The main container <code>app</code> runs
            <code>myapp/api:v2.0</code> and connects to Redis via <code>localhost:6379</code> (set as env var
            <code>REDIS_HOST=localhost</code>). An ambassador container <code>redis-proxy</code> runs
            <code>myapp/redis-proxy:v1.0</code> on port 6379, with env var <code>REDIS_CLUSTER_ADDR</code> set
            to <code>redis-cluster.prod.svc.cluster.local:6379</code>.
          functionSignature: "Kind: Pod with ambassador"
          testCases:
            - input: "App connects to localhost:6379"
              output: "Ambassador proxies to redis-cluster.prod.svc.cluster.local:6379"
            - input: "kubectl get pod app-with-proxy"
              output: "READY: 2/2"
          hints:
            - title: "Think about it"
              content: >-
                The ambassador pattern lets the main container use <code>localhost</code> while the proxy
                handles the complexity of connecting to the external service.
            - title: "Hint"
              content: >-
                Both containers share the Pod's network namespace. The app talks to <code>localhost:6379</code>,
                and the ambassador listens on port 6379 and forwards to the real Redis cluster.
            - title: "Pattern"
              content: |-
                <pre>containers:
                - name: app
                  env:
                  - name: REDIS_HOST
                    value: "localhost"
                - name: redis-proxy
                  ports:
                  - containerPort: 6379
                  env:
                  - name: REDIS_CLUSTER_ADDR
                    value: "redis-cluster.prod.svc.cluster.local:6379"</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: app-with-proxy
            spec:
              containers:
              - name: app
                image: myapp/api:v2.0
                env:
                - name: REDIS_HOST
                  value: "localhost"
                - name: REDIS_PORT
                  value: "6379"
              - name: redis-proxy
                image: myapp/redis-proxy:v1.0
                ports:
                - containerPort: 6379
                env:
                - name: REDIS_CLUSTER_ADDR
                  value: "redis-cluster.prod.svc.cluster.local:6379"
          difficulty: 3
        - id: v4
          title: Init + Sidecar Combined
          description: >-
            Write a Pod named <code>fullstack-pod</code> that combines init and sidecar patterns. An init container
            <code>setup</code> (<code>busybox:1.36</code>) writes an HTML file to a shared volume
            <code>content</code> at <code>/work-dir/index.html</code>. The main container <code>web</code>
            (<code>nginx:1.25</code>, port 80) serves it from <code>/usr/share/nginx/html</code> and writes
            logs to a second volume <code>logs</code> at <code>/var/log/nginx</code>. A sidecar
            <code>log-tailer</code> (<code>busybox:1.36</code>) tails the logs with
            <code>command: ["tail", "-f", "/var/log/nginx/access.log"]</code>.
          functionSignature: "Kind: Pod with initContainers and sidecar"
          testCases:
            - input: "Init container creates index.html"
              output: "nginx serves the file, log-tailer streams access logs"
            - input: "kubectl get pod fullstack-pod (during init)"
              output: "STATUS: Init:0/1, then READY: 2/2"
          hints:
            - title: "Think about it"
              content: >-
                You need two volumes: <code>content</code> (shared between init and web) and <code>logs</code>
                (shared between web and log-tailer). The init container only uses the content volume.
            - title: "Hint"
              content: >-
                The init container writes to <code>/work-dir</code>, and the web container reads from
                <code>/usr/share/nginx/html</code>. Both mount the same <code>content</code> volume.
            - title: "Pattern"
              content: |-
                <pre>initContainers:
                - name: setup
                  volumeMounts:
                  - name: content
                    mountPath: /work-dir
                containers:
                - name: web
                  volumeMounts:
                  - name: content
                    mountPath: /usr/share/nginx/html
                  - name: logs
                    mountPath: /var/log/nginx
                - name: log-tailer
                  volumeMounts:
                  - name: logs
                    mountPath: /var/log/nginx</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: fullstack-pod
            spec:
              initContainers:
              - name: setup
                image: busybox:1.36
                command:
                - sh
                - -c
                - |
                  echo "<html><body><h1>Hello from init container</h1></body></html>" > /work-dir/index.html
                volumeMounts:
                - name: content
                  mountPath: /work-dir
              containers:
              - name: web
                image: nginx:1.25
                ports:
                - containerPort: 80
                volumeMounts:
                - name: content
                  mountPath: /usr/share/nginx/html
                  readOnly: true
                - name: logs
                  mountPath: /var/log/nginx
              - name: log-tailer
                image: busybox:1.36
                command: ["tail", "-f", "/var/log/nginx/access.log"]
                volumeMounts:
                - name: logs
                  mountPath: /var/log/nginx
                  readOnly: true
              volumes:
              - name: content
                emptyDir: {}
              - name: logs
                emptyDir: {}
          difficulty: 3
        - id: v5
          title: Multiple Init Containers in Sequence
          description: >-
            Write a Pod named <code>ordered-init</code> with two init containers and one main container.
            Init container 1 (<code>check-config</code>, <code>busybox:1.36</code>) verifies a config
            service is up at <code>config-svc.default.svc.cluster.local:8080</code> using <code>wget</code>.
            Init container 2 (<code>fetch-config</code>, <code>busybox:1.36</code>) downloads config to
            <code>/config/app.conf</code> on a shared volume. The main container <code>app</code>
            (<code>myapp/web:v1.0</code>) mounts the config at <code>/etc/app</code>.
          functionSignature: "Kind: Pod with multiple initContainers"
          testCases:
            - input: "kubectl get pod ordered-init (init phase)"
              output: "STATUS: Init:0/2, then Init:1/2, then Running"
            - input: "Init containers run sequentially"
              output: "check-config must succeed before fetch-config runs"
          hints:
            - title: "Think about it"
              content: >-
                Init containers run in order. The first checks connectivity, the second downloads config.
                Only the second init container and the main container need the shared volume.
            - title: "Hint"
              content: >-
                Use <code>wget --spider</code> for the health check and <code>wget -O</code> to download
                the config file. Both init containers are listed in the <code>initContainers</code> array
                in order.
            - title: "Pattern"
              content: |-
                <pre>initContainers:
                - name: check-config   # runs first
                  ...
                - name: fetch-config   # runs second
                  ...
                containers:
                - name: app            # runs after all inits succeed
                  ...</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: ordered-init
            spec:
              initContainers:
              - name: check-config
                image: busybox:1.36
                command:
                - sh
                - -c
                - |
                  until wget --spider -q http://config-svc.default.svc.cluster.local:8080/health; do
                    echo "Config service not ready, retrying..."
                    sleep 2
                  done
              - name: fetch-config
                image: busybox:1.36
                command:
                - sh
                - -c
                - wget -O /config/app.conf http://config-svc.default.svc.cluster.local:8080/config
                volumeMounts:
                - name: config-vol
                  mountPath: /config
              containers:
              - name: app
                image: myapp/web:v1.0
                volumeMounts:
                - name: config-vol
                  mountPath: /etc/app
                  readOnly: true
              volumes:
              - name: config-vol
                emptyDir: {}
          difficulty: 3
        - id: v6
          title: Sidecar - Prometheus Exporter
          description: >-
            Write a Pod named <code>monitored-app</code>. The main container <code>api</code> runs
            <code>myapp/api:v2.0</code> on port 8080 and writes metrics to a file at
            <code>/tmp/metrics/app.prom</code>. A sidecar container <code>exporter</code> runs
            <code>prom/node-exporter:v1.7</code> on port 9100 and reads from <code>/metrics-data</code>
            (read-only). Use a shared <code>emptyDir</code> volume named <code>metrics</code>.
            Add annotation <code>prometheus.io/scrape: "true"</code>.
          functionSignature: "Kind: Pod with Prometheus sidecar"
          testCases:
            - input: "App writes metrics, exporter reads them"
              output: "Prometheus scrapes the exporter on port 9100"
            - input: "kubectl get pod monitored-app"
              output: "READY: 2/2"
          hints:
            - title: "Think about it"
              content: >-
                The app writes metrics to a shared volume, and the Prometheus exporter reads and exposes
                them on its own HTTP port for scraping.
            - title: "Hint"
              content: >-
                The app mounts the volume at <code>/tmp/metrics</code>, the exporter at
                <code>/metrics-data</code>. Same volume, different mount paths.
            - title: "Pattern"
              content: |-
                <pre>metadata:
                  annotations:
                    prometheus.io/scrape: "true"
                spec:
                  containers:
                  - name: api
                    volumeMounts:
                    - name: metrics
                      mountPath: /tmp/metrics
                  - name: exporter
                    volumeMounts:
                    - name: metrics
                      mountPath: /metrics-data
                      readOnly: true</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: monitored-app
              annotations:
                prometheus.io/scrape: "true"
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                ports:
                - containerPort: 8080
                volumeMounts:
                - name: metrics
                  mountPath: /tmp/metrics
              - name: exporter
                image: prom/node-exporter:v1.7
                ports:
                - containerPort: 9100
                volumeMounts:
                - name: metrics
                  mountPath: /metrics-data
                  readOnly: true
              volumes:
              - name: metrics
                emptyDir: {}
          difficulty: 2
        - id: v7
          title: Init Container - Git Clone
          description: >-
            Write a Pod named <code>static-site</code>. An init container <code>git-clone</code>
            (<code>alpine/git:2.40</code>) clones a repository from <code>https://github.com/example/site.git</code>
            into <code>/repo</code> on a shared volume <code>site-content</code>. The main container
            <code>web</code> (<code>nginx:1.25</code>, port 80) serves the content from
            <code>/usr/share/nginx/html</code> mounted from the same volume.
          functionSignature: "Kind: Pod with git-clone init container"
          testCases:
            - input: "Init container clones repo to shared volume"
              output: "nginx serves the cloned content"
            - input: "kubectl logs static-site -c git-clone"
              output: "Cloning into '/repo'..."
          hints:
            - title: "Think about it"
              content: >-
                The init container clones a repo into a shared volume. The main container serves the files
                from that same volume. The init container completes before nginx starts.
            - title: "Hint"
              content: >-
                Use <code>command: ["git", "clone", "URL", "/repo"]</code> for the init container.
                Mount the volume at <code>/repo</code> in the init container and at the nginx html directory
                in the web container.
            - title: "Pattern"
              content: |-
                <pre>initContainers:
                - name: git-clone
                  image: alpine/git:2.40
                  command: ["git", "clone", "REPO_URL", "/repo"]
                  volumeMounts:
                  - name: site-content
                    mountPath: /repo</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: static-site
            spec:
              initContainers:
              - name: git-clone
                image: alpine/git:2.40
                command: ["git", "clone", "https://github.com/example/site.git", "/repo"]
                volumeMounts:
                - name: site-content
                  mountPath: /repo
              containers:
              - name: web
                image: nginx:1.25
                ports:
                - containerPort: 80
                volumeMounts:
                - name: site-content
                  mountPath: /usr/share/nginx/html
                  readOnly: true
              volumes:
              - name: site-content
                emptyDir: {}
          difficulty: 2
        - id: v8
          title: Three-Container Pod
          description: >-
            Write a Pod named <code>three-tier</code> with three containers sharing a volume named
            <code>shared-data</code>. Container <code>writer</code> (<code>busybox:1.36</code>) writes
            timestamps to <code>/data/log.txt</code> every 5 seconds. Container <code>processor</code>
            (<code>busybox:1.36</code>) reads <code>/data/log.txt</code> and writes processed output to
            <code>/data/processed.txt</code>. Container <code>reader</code> (<code>busybox:1.36</code>)
            tails <code>/data/processed.txt</code> (read-only mount).
          functionSignature: "Kind: Pod with three containers and shared volume"
          testCases:
            - input: "Writer produces data, processor transforms it, reader consumes it"
              output: "Pipeline via shared emptyDir volume"
            - input: "kubectl get pod three-tier"
              output: "READY: 3/3"
          hints:
            - title: "Think about it"
              content: >-
                All three containers mount the same <code>emptyDir</code> volume. The writer and processor
                need read-write access. The reader only needs read access.
            - title: "Hint"
              content: >-
                Use shell loops for the writer and processor. The reader uses <code>tail -f</code>.
                All mount the same volume at <code>/data</code>.
            - title: "Pattern"
              content: |-
                <pre>containers:
                - name: writer
                  command: ["sh", "-c", "while true; do date >> /data/log.txt; sleep 5; done"]
                - name: processor
                  command: ["sh", "-c", "tail -f /data/log.txt | sed 's/^/PROCESSED: /' >> /data/processed.txt"]
                - name: reader
                  command: ["tail", "-f", "/data/processed.txt"]</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: three-tier
            spec:
              containers:
              - name: writer
                image: busybox:1.36
                command: ["sh", "-c", "while true; do date >> /data/log.txt; sleep 5; done"]
                volumeMounts:
                - name: shared-data
                  mountPath: /data
              - name: processor
                image: busybox:1.36
                command: ["sh", "-c", "tail -f /data/log.txt | sed 's/^/PROCESSED: /' >> /data/processed.txt"]
                volumeMounts:
                - name: shared-data
                  mountPath: /data
              - name: reader
                image: busybox:1.36
                command: ["tail", "-f", "/data/processed.txt"]
                volumeMounts:
                - name: shared-data
                  mountPath: /data
                  readOnly: true
              volumes:
              - name: shared-data
                emptyDir: {}
          difficulty: 4
    - id: challenge_2
      block: 2
      difficulty: 3
      concept: Debugging Pods
      variants:
        - id: v1
          title: Diagnose ImagePullBackOff
          description: >-
            A Pod named <code>broken-app</code> shows the following events:<br>
            <pre>Warning  Failed     kubelet  Failed to pull image "myapp:latset": not found
            Warning  Failed     kubelet  Error: ErrImagePull
            Normal   BackOff    kubelet  Back-off pulling image "myapp:latset"
            Warning  Failed     kubelet  Error: ImagePullBackOff</pre>
            What is wrong and how do you fix it? Write the kubectl command to identify the issue, then write
            the corrected image name.
          functionSignature: "kubectl describe + corrected image"
          testCases:
            - input: "kubectl describe pod broken-app (events)"
              output: "Image tag 'latset' is a typo -- should be 'latest'"
            - input: "Corrected image"
              output: "myapp:latest"
          hints:
            - title: "Think about it"
              content: >-
                The events show <code>Failed to pull image "myapp:latset"</code>. Look carefully at the
                image tag -- is it spelled correctly?
            - title: "Hint"
              content: >-
                The tag <code>latset</code> is a typo for <code>latest</code>. Fix the image name in the
                Pod spec and re-apply.
            - title: "Pattern"
              content: |-
                <pre># Diagnose:
                kubectl describe pod broken-app
                # Fix: correct the image tag from "latset" to "latest"
                # In the Pod YAML: image: myapp:latest</pre>
          solution: |-
            # Diagnose:
            kubectl describe pod broken-app

            # Problem: image tag "latset" is a typo for "latest"
            # Fix: correct the image in the Pod spec
            # image: myapp:latest

            apiVersion: v1
            kind: Pod
            metadata:
              name: broken-app
            spec:
              containers:
              - name: app
                image: myapp:latest
          difficulty: 2
        - id: v2
          title: Diagnose CrashLoopBackOff - Missing Env Var
          description: >-
            A Pod named <code>api-server</code> is in CrashLoopBackOff. The logs from the previous container show:
            <pre>Error: required environment variable DATABASE_URL is not set
            Exiting with code 1</pre>
            Write the kubectl command to view the crash logs, then write the fix: add the missing env var
            <code>DATABASE_URL</code> with value <code>postgres://db.default.svc.cluster.local:5432/mydb</code>.
            The Pod runs <code>myapp/api:v2.0</code>.
          functionSignature: "kubectl logs --previous + fixed Pod YAML"
          testCases:
            - input: "kubectl logs api-server --previous"
              output: "Error: required environment variable DATABASE_URL is not set"
            - input: "Fixed Pod spec"
              output: "Pod with DATABASE_URL env var added"
          hints:
            - title: "Think about it"
              content: >-
                The container crashes because it expects <code>DATABASE_URL</code> but it's not set.
                Use <code>kubectl logs --previous</code> to see crash output.
            - title: "Hint"
              content: >-
                Add an <code>env</code> entry for <code>DATABASE_URL</code> in the container spec,
                then delete and re-create the Pod.
            - title: "Pattern"
              content: |-
                <pre>kubectl logs api-server --previous
                # Then fix the Pod YAML by adding:
                env:
                - name: DATABASE_URL
                  value: "postgres://..."</pre>
          solution: |-
            # Diagnose:
            kubectl logs api-server --previous

            # Fix: add the missing environment variable
            apiVersion: v1
            kind: Pod
            metadata:
              name: api-server
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                env:
                - name: DATABASE_URL
                  value: "postgres://db.default.svc.cluster.local:5432/mydb"
          difficulty: 2
        - id: v3
          title: Diagnose Pending - Insufficient Resources
          description: >-
            A Pod named <code>big-pod</code> is stuck in Pending. Events show:
            <pre>Warning  FailedScheduling  default-scheduler
              0/3 nodes are available: 3 Insufficient memory.
              preemption: 0/3 nodes are available: 3 No preemption victims found.</pre>
            The Pod requests <code>memory: "32Gi"</code> but no node has that much available. Write the kubectl
            command to diagnose this, then write a corrected Pod spec with <code>memory: "2Gi"</code> request
            and <code>4Gi</code> limit. The Pod runs <code>myapp/worker:v1.0</code>.
          functionSignature: "kubectl describe + fixed Pod YAML"
          testCases:
            - input: "kubectl describe pod big-pod"
              output: "FailedScheduling: Insufficient memory"
            - input: "Fixed Pod spec"
              output: "Reduced memory request to 2Gi"
          hints:
            - title: "Think about it"
              content: >-
                The scheduler can't find a node with 32Gi of free memory. Either reduce the request or
                add more capacity to the cluster.
            - title: "Hint"
              content: >-
                Look at the events in <code>kubectl describe pod</code>. The fix is to reduce the memory
                request to something the cluster can accommodate.
            - title: "Pattern"
              content: |-
                <pre>kubectl describe pod big-pod
                # Events show: 0/3 nodes available, Insufficient memory
                # Fix: reduce memory requests in the Pod spec</pre>
          solution: |-
            # Diagnose:
            kubectl describe pod big-pod

            # Problem: memory request 32Gi exceeds available node capacity
            # Fix: reduce memory request
            apiVersion: v1
            kind: Pod
            metadata:
              name: big-pod
            spec:
              containers:
              - name: worker
                image: myapp/worker:v1.0
                resources:
                  requests:
                    memory: "2Gi"
                  limits:
                    memory: "4Gi"
          difficulty: 2
        - id: v4
          title: Diagnose OOMKilled
          description: >-
            A Pod named <code>memory-hog</code> keeps restarting. The describe output shows:
            <pre>Last State:     Terminated
              Reason:       OOMKilled
              Exit Code:    137</pre>
            The container has <code>limits: {memory: "128Mi"}</code>. The app actually needs about 400Mi.
            Write the kubectl commands to diagnose this, then write a corrected Pod spec with appropriate
            memory limits. The Pod runs <code>myapp/processor:v1.0</code>.
          functionSignature: "kubectl describe + kubectl get -o jsonpath + fixed Pod YAML"
          testCases:
            - input: "kubectl describe pod memory-hog"
              output: "Last State: Terminated, Reason: OOMKilled, Exit Code: 137"
            - input: "Fixed Pod spec"
              output: "Memory limit increased to 512Mi"
          hints:
            - title: "Think about it"
              content: >-
                Exit code 137 means the process was killed by SIGKILL (128 + 9). OOMKilled means the
                container exceeded its memory limit. Increase the limit.
            - title: "Hint"
              content: >-
                Use <code>kubectl describe pod</code> to find the OOMKilled reason.
                Then increase the memory limit to accommodate the app's needs (give some headroom above 400Mi).
            - title: "Pattern"
              content: |-
                <pre># Check termination reason:
                kubectl get pod memory-hog -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'
                # OOMKilled
                # Fix: increase memory limits in the Pod spec</pre>
          solution: |-
            # Diagnose:
            kubectl describe pod memory-hog
            kubectl get pod memory-hog -o jsonpath='{.status.containerStatuses[0].lastState.terminated.reason}'

            # Problem: memory limit 128Mi is too low, app needs ~400Mi
            # Fix: increase memory request and limit
            apiVersion: v1
            kind: Pod
            metadata:
              name: memory-hog
            spec:
              containers:
              - name: processor
                image: myapp/processor:v1.0
                resources:
                  requests:
                    memory: "256Mi"
                  limits:
                    memory: "512Mi"
          difficulty: 3
        - id: v5
          title: Diagnose CrashLoopBackOff - Wrong Command
          description: >-
            A Pod named <code>worker</code> is in CrashLoopBackOff. The logs show:
            <pre>/bin/sh: ./start.sh: not found</pre>
            The describe output shows the command is <code>["./start.sh"]</code> but the correct entrypoint
            in the image is <code>/app/start.sh</code>. Write the kubectl commands to diagnose and the corrected
            Pod spec. The Pod runs <code>myapp/worker:v1.0</code>.
          functionSignature: "kubectl logs --previous + fixed Pod YAML"
          testCases:
            - input: "kubectl logs worker --previous"
              output: "/bin/sh: ./start.sh: not found"
            - input: "Fixed Pod spec"
              output: "command corrected to /app/start.sh"
          hints:
            - title: "Think about it"
              content: >-
                The error means the container can't find the script at the relative path <code>./start.sh</code>.
                The correct path is absolute: <code>/app/start.sh</code>.
            - title: "Hint"
              content: >-
                Use <code>kubectl logs --previous</code> to see the crash output. Then fix the
                <code>command</code> to use the correct absolute path.
            - title: "Pattern"
              content: |-
                <pre>kubectl logs worker --previous
                # /bin/sh: ./start.sh: not found
                # Fix: correct the command path
                command: ["/app/start.sh"]</pre>
          solution: |-
            # Diagnose:
            kubectl logs worker --previous

            # Problem: command uses relative path ./start.sh instead of /app/start.sh
            # Fix: correct the command path
            apiVersion: v1
            kind: Pod
            metadata:
              name: worker
            spec:
              containers:
              - name: worker
                image: myapp/worker:v1.0
                command: ["/app/start.sh"]
          difficulty: 2
        - id: v6
          title: Diagnose Init Container Failure
          description: >-
            A Pod named <code>web-init</code> is stuck in <code>Init:CrashLoopBackOff</code>. The init container
            <code>wait-for-db</code> logs show:
            <pre>Waiting for postgres...
            nc: bad address 'postgress.default.svc.cluster.local'</pre>
            The hostname has a typo. Write the kubectl commands to diagnose and the corrected Pod spec.
            The init container uses <code>busybox:1.36</code> and the main container runs
            <code>myapp/web:v1.0</code> on port 8080.
          functionSignature: "kubectl logs -c + fixed Pod YAML"
          testCases:
            - input: "kubectl logs web-init -c wait-for-db"
              output: "nc: bad address 'postgress.default.svc.cluster.local'"
            - input: "Fixed Pod spec"
              output: "Hostname corrected to postgres (single 's')"
          hints:
            - title: "Think about it"
              content: >-
                Look at the hostname carefully: <code>postgress</code> has a double 's'. The correct
                service name is <code>postgres</code>.
            - title: "Hint"
              content: >-
                Use <code>kubectl logs web-init -c wait-for-db</code> to see the init container's output.
                The DNS name has a typo -- fix it in the init container's command.
            - title: "Pattern"
              content: |-
                <pre>kubectl logs web-init -c wait-for-db
                # Fix the hostname typo: postgress -> postgres</pre>
          solution: |-
            # Diagnose:
            kubectl logs web-init -c wait-for-db

            # Problem: hostname typo "postgress" should be "postgres"
            # Fix: correct the hostname in the init container command
            apiVersion: v1
            kind: Pod
            metadata:
              name: web-init
            spec:
              initContainers:
              - name: wait-for-db
                image: busybox:1.36
                command:
                - sh
                - -c
                - |
                  until nc -z postgres.default.svc.cluster.local 5432; do
                    echo "Waiting for postgres..."
                    sleep 2
                  done
              containers:
              - name: web
                image: myapp/web:v1.0
                ports:
                - containerPort: 8080
          difficulty: 3
        - id: v7
          title: Diagnose Wrong containerPort
          description: >-
            A Pod named <code>web-pod</code> is Running (1/1 Ready) but a Service cannot reach it. The Pod
            runs <code>nginx:1.25</code> which listens on port 80, but the Pod spec has
            <code>containerPort: 8080</code>. A teammate asks: "Is the containerPort wrong? Is that why
            the Service can't connect?" Explain why <code>containerPort</code> is not the cause, identify
            the real issue (the Service selector or targetPort), and write a corrected Pod spec with the
            proper <code>containerPort</code>.
          functionSignature: "Explanation + fixed Pod YAML"
          testCases:
            - input: "containerPort: 8080 but nginx listens on 80"
              output: "containerPort is documentation only -- the real issue is the Service's targetPort"
            - input: "Fixed Pod spec"
              output: "containerPort corrected to 80 for documentation accuracy"
          hints:
            - title: "Think about it"
              content: >-
                <code>containerPort</code> is purely informational in the Pod spec. It does NOT control
                which port the container listens on. The real issue is usually the Service's
                <code>targetPort</code>.
            - title: "Hint"
              content: >-
                Fix the <code>containerPort</code> to 80 for accuracy. But the real fix is ensuring the
                Service's <code>targetPort</code> matches the port nginx actually listens on (80).
            - title: "Pattern"
              content: |-
                <pre># containerPort is documentation only.
                # nginx binds to port 80 regardless of what containerPort says.
                # The Service's targetPort must match the actual listening port (80).
                ports:
                - containerPort: 80  # corrected for documentation accuracy</pre>
          solution: |-
            # containerPort is documentation only -- it does NOT expose or restrict ports.
            # nginx listens on port 80 regardless of what containerPort says.
            # The real fix: ensure the Service's targetPort is 80 (not 8080).

            # Corrected Pod spec (containerPort fixed for documentation accuracy):
            apiVersion: v1
            kind: Pod
            metadata:
              name: web-pod
              labels:
                app: web
            spec:
              containers:
              - name: web
                image: nginx:1.25
                ports:
                - containerPort: 80
          difficulty: 3
    - id: challenge_3
      block: 1
      difficulty: 2
      concept: QoS Classes
      variants:
        - id: v1
          title: Design a Guaranteed Pod
          description: >-
            Write a Pod named <code>critical-db</code> running <code>postgres:16</code> on port 5432 that
            will receive the <strong>Guaranteed</strong> QoS class. Allocate 1 CPU core and 1Gi of memory.
            Explain why this configuration gets the Guaranteed class.
          functionSignature: "Kind: Pod with Guaranteed QoS"
          testCases:
            - input: "kubectl get pod critical-db -o jsonpath='{.status.qosClass}'"
              output: "Guaranteed"
            - input: "All container requests == limits"
              output: "QoS class is Guaranteed"
          hints:
            - title: "Think about it"
              content: >-
                Guaranteed QoS requires every container to have identical requests and limits for both
                CPU and memory.
            - title: "Hint"
              content: >-
                Set <code>requests.cpu == limits.cpu</code> and <code>requests.memory == limits.memory</code>.
            - title: "Pattern"
              content: |-
                <pre>resources:
                  requests:
                    cpu: "1"
                    memory: "1Gi"
                  limits:
                    cpu: "1"        # same as request
                    memory: "1Gi"   # same as request</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: critical-db
            spec:
              containers:
              - name: db
                image: postgres:16
                ports:
                - containerPort: 5432
                resources:
                  requests:
                    cpu: "1"
                    memory: "1Gi"
                  limits:
                    cpu: "1"
                    memory: "1Gi"
            # QoS: Guaranteed because requests == limits for all
            # containers, for both CPU and memory
          difficulty: 1
        - id: v2
          title: Design a Burstable Pod
          description: >-
            Write a Pod named <code>web-burst</code> running <code>nginx:1.25</code> on port 80 that will
            receive the <strong>Burstable</strong> QoS class. Set CPU request <code>100m</code> with limit
            <code>500m</code>, memory request <code>128Mi</code> with limit <code>512Mi</code>.
          functionSignature: "Kind: Pod with Burstable QoS"
          testCases:
            - input: "kubectl get pod web-burst -o jsonpath='{.status.qosClass}'"
              output: "Burstable"
            - input: "requests != limits"
              output: "QoS class is Burstable"
          hints:
            - title: "Think about it"
              content: >-
                Burstable means at least one container has requests or limits set, but they are not
                all equal. The Pod can burst beyond its request up to the limit.
            - title: "Hint"
              content: >-
                Set different values for requests and limits. The scheduler reserves the request
                amount, but the container can use up to the limit.
            - title: "Pattern"
              content: |-
                <pre>resources:
                  requests:
                    cpu: "100m"       # lower bound
                    memory: "128Mi"
                  limits:
                    cpu: "500m"       # different from request = Burstable
                    memory: "512Mi"</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: web-burst
            spec:
              containers:
              - name: web
                image: nginx:1.25
                ports:
                - containerPort: 80
                resources:
                  requests:
                    cpu: "100m"
                    memory: "128Mi"
                  limits:
                    cpu: "500m"
                    memory: "512Mi"
            # QoS: Burstable because requests != limits
          difficulty: 1
        - id: v3
          title: BestEffort Pod
          description: >-
            Write a Pod named <code>dev-debug</code> running <code>ubuntu:22.04</code> with
            <code>command: ["sleep", "infinity"]</code> that receives the <strong>BestEffort</strong> QoS
            class. This is a throwaway debug Pod. Explain when BestEffort is acceptable and when it's dangerous.
          functionSignature: "Kind: Pod with BestEffort QoS"
          testCases:
            - input: "kubectl get pod dev-debug -o jsonpath='{.status.qosClass}'"
              output: "BestEffort"
            - input: "No resources block"
              output: "QoS class is BestEffort"
          hints:
            - title: "Think about it"
              content: >-
                BestEffort is assigned when no container has any resource requests or limits.
                These Pods are evicted first under memory pressure.
            - title: "Hint"
              content: >-
                Simply omit the <code>resources</code> block entirely. No requests, no limits, no guarantees.
            - title: "Pattern"
              content: |-
                <pre>containers:
                - name: debug
                  image: ubuntu:22.04
                  command: ["sleep", "infinity"]
                  # no resources block = BestEffort</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: dev-debug
            spec:
              containers:
              - name: debug
                image: ubuntu:22.04
                command: ["sleep", "infinity"]
            # QoS: BestEffort because no resources are set
            # Acceptable for: dev/debug Pods, throwaway tools
            # Dangerous for: production workloads (evicted first under memory pressure)
          difficulty: 1
        - id: v4
          title: Multi-Container Guaranteed Design
          description: >-
            Write a Pod named <code>ha-app</code> with two containers that gets the <strong>Guaranteed</strong>
            QoS class. Container <code>api</code> runs <code>myapp/api:v2.0</code> on port 8080 (500m CPU,
            256Mi memory). Container <code>proxy</code> runs <code>envoyproxy/envoy:v1.28</code> on port 9901
            (100m CPU, 64Mi memory). Both must have requests == limits.
          functionSignature: "Kind: Pod with two Guaranteed containers"
          testCases:
            - input: "kubectl describe pod ha-app | grep 'QoS Class'"
              output: "QoS Class: Guaranteed"
            - input: "Both containers: requests == limits"
              output: "Guaranteed QoS for the entire Pod"
          hints:
            - title: "Think about it"
              content: >-
                For Guaranteed QoS with multiple containers, EVERY container must have requests == limits
                for both CPU and memory. If even one container is missing resources, the Pod drops to
                Burstable or BestEffort.
            - title: "Hint"
              content: >-
                Set resources on both containers. Each must have identical request and limit values.
            - title: "Pattern"
              content: |-
                <pre>containers:
                - name: api
                  resources:
                    requests: {cpu: "500m", memory: "256Mi"}
                    limits: {cpu: "500m", memory: "256Mi"}
                - name: proxy
                  resources:
                    requests: {cpu: "100m", memory: "64Mi"}
                    limits: {cpu: "100m", memory: "64Mi"}</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: ha-app
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                ports:
                - containerPort: 8080
                resources:
                  requests:
                    cpu: "500m"
                    memory: "256Mi"
                  limits:
                    cpu: "500m"
                    memory: "256Mi"
              - name: proxy
                image: envoyproxy/envoy:v1.28
                ports:
                - containerPort: 9901
                resources:
                  requests:
                    cpu: "100m"
                    memory: "64Mi"
                  limits:
                    cpu: "100m"
                    memory: "64Mi"
            # QoS: Guaranteed -- both containers have requests == limits
          difficulty: 2
        - id: v5
          title: Identify QoS from Spec
          description: >-
            Three Pods have the following resource configurations. Identify the QoS class for each:
            <br><br>
            <strong>Pod A</strong>: one container with <code>requests: {cpu: "500m"}</code> and no limits
            <br><strong>Pod B</strong>: two containers, both with <code>requests == limits</code> for CPU and memory
            <br><strong>Pod C</strong>: one container with no resources block at all
            <br><br>
            Write the QoS class for each Pod.
          functionSignature: "QoS classification"
          testCases:
            - input: "Pod A: requests only, no limits"
              output: "Burstable"
            - input: "Pod B: all containers requests == limits"
              output: "Guaranteed"
            - input: "Pod C: no resources at all"
              output: "BestEffort"
          hints:
            - title: "Think about it"
              content: >-
                Apply the three QoS rules: Guaranteed = all requests == limits, BestEffort = no resources
                at all, Burstable = everything else.
            - title: "Hint"
              content: >-
                Pod A has partial resources (requests but no limits), so it's not Guaranteed or BestEffort.
            - title: "Pattern"
              content: |-
                <pre>Guaranteed: requests == limits for ALL containers, both CPU and memory
                BestEffort: NO resources set on ANY container
                Burstable: everything else (at least one resource set, but not all equal)</pre>
          solution: |-
            Pod A: Burstable
              - Has requests but no limits, so requests != limits

            Pod B: Guaranteed
              - All containers have requests == limits for both CPU and memory

            Pod C: BestEffort
              - No resources set at all, evicted first under memory pressure
          difficulty: 2
        - id: v6
          title: Convert Burstable to Guaranteed
          description: >-
            The following Pod is Burstable. Modify it to be <strong>Guaranteed</strong> while keeping the
            limits unchanged:
            <pre>apiVersion: v1
            kind: Pod
            metadata:
              name: api-pod
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                resources:
                  requests:
                    cpu: "250m"
                    memory: "128Mi"
                  limits:
                    cpu: "500m"
                    memory: "256Mi"</pre>
          functionSignature: "Kind: Pod (modified to Guaranteed QoS)"
          testCases:
            - input: "Original: requests != limits (Burstable)"
              output: "Modified: requests == limits (Guaranteed)"
            - input: "kubectl describe pod api-pod | grep 'QoS Class'"
              output: "QoS Class: Guaranteed"
          hints:
            - title: "Think about it"
              content: >-
                To convert from Burstable to Guaranteed, make requests equal to limits. You can either
                raise requests to match limits or lower limits to match requests.
            - title: "Hint"
              content: >-
                The instruction says keep limits unchanged. So set requests equal to the limit values:
                <code>cpu: "500m"</code> and <code>memory: "256Mi"</code>.
            - title: "Pattern"
              content: |-
                <pre>resources:
                  requests:
                    cpu: "500m"      # raised to match limit
                    memory: "256Mi"  # raised to match limit
                  limits:
                    cpu: "500m"
                    memory: "256Mi"</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: api-pod
            spec:
              containers:
              - name: api
                image: myapp/api:v2.0
                resources:
                  requests:
                    cpu: "500m"
                    memory: "256Mi"
                  limits:
                    cpu: "500m"
                    memory: "256Mi"
            # QoS: Guaranteed -- requests now equal limits
          difficulty: 2
        - id: v7
          title: Production Pod with Proper Resources
          description: >-
            Write a production-ready Pod named <code>prod-api</code> running <code>myapp/api:v3.0</code> on
            port 8080. It needs Guaranteed QoS with 2 CPU cores and 2Gi memory. Add labels
            <code>app: api</code> and <code>environment: production</code>. Include env vars
            <code>APP_ENV=production</code> and <code>LOG_LEVEL=warn</code>.
          functionSignature: "Kind: Pod (production-ready)"
          testCases:
            - input: "Guaranteed QoS with production labels and config"
              output: "Pod with requests == limits, proper labels, and env vars"
            - input: "kubectl describe pod prod-api | grep 'QoS Class'"
              output: "QoS Class: Guaranteed"
          hints:
            - title: "Think about it"
              content: >-
                A production Pod should have Guaranteed QoS (requests == limits), meaningful labels for
                Service selection, and proper environment configuration.
            - title: "Hint"
              content: >-
                Combine everything: metadata with labels, container with ports, env vars, and resources
                where requests == limits.
            - title: "Pattern"
              content: |-
                <pre>metadata:
                  labels:
                    app: api
                    environment: production
                spec:
                  containers:
                  - name: api
                    resources:
                      requests: {cpu: "2", memory: "2Gi"}
                      limits: {cpu: "2", memory: "2Gi"}</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: prod-api
              labels:
                app: api
                environment: production
            spec:
              containers:
              - name: api
                image: myapp/api:v3.0
                ports:
                - containerPort: 8080
                env:
                - name: APP_ENV
                  value: "production"
                - name: LOG_LEVEL
                  value: "warn"
                resources:
                  requests:
                    cpu: "2"
                    memory: "2Gi"
                  limits:
                    cpu: "2"
                    memory: "2Gi"
            # QoS: Guaranteed
          difficulty: 3
