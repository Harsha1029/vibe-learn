conceptLinks:
  HPA Basics: "#lesson-horizontal-pod-autoscaler-hpa"
  HPA Algorithm: "#lesson-how-the-hpa-algorithm-works"
  Scaling Behavior: "#lesson-scaling-behavior-preventing-flapping"
  KEDA: "#lesson-keda-kubernetes-event-driven-autoscaler"
  VPA: "#lesson-vertical-pod-autoscaler-vpa"
  Cluster Autoscaler: "#lesson-cluster-autoscaler"
  Resource Requests: "#lesson-resource-requests-the-foundation"
  Custom Metrics: "#lesson-custom-metrics-brief"
sharedContent: {}
variants:
  warmups:
    - id: warmup_1
      concept: HPA Basics
      variants:
        - id: v1
          title: Imperative HPA for CPU
          description: >-
            Create an HPA for the Deployment <code>web</code> that targets 50% average CPU utilization
            with a minimum of 2 and maximum of 10 replicas. Use the imperative <code>kubectl autoscale</code> command.
          hints:
            - "The verb is <code>kubectl autoscale deployment</code>"
            - "Use <code>--cpu-percent</code>, <code>--min</code>, and <code>--max</code> flags"
          solution: |-
            kubectl autoscale deployment web --cpu-percent=50 --min=2 --max=10
        - id: v2
          title: Imperative HPA with Different Targets
          description: >-
            Create an HPA for the Deployment <code>api-server</code> targeting 70% CPU with min 3 and max 20 replicas.
          hints:
            - "Use <code>kubectl autoscale deployment api-server</code>"
            - "Set <code>--cpu-percent=70 --min=3 --max=20</code>"
          solution: |-
            kubectl autoscale deployment api-server --cpu-percent=70 --min=3 --max=20
        - id: v3
          title: Declarative HPA with CPU Target
          description: >-
            Write an <code>autoscaling/v2</code> HPA YAML that targets the Deployment <code>web</code>,
            scales between 2 and 10 replicas, and targets 50% average CPU utilization.
          hints:
            - "Use <code>apiVersion: autoscaling/v2</code> and <code>kind: HorizontalPodAutoscaler</code>"
            - "The metric type is <code>Resource</code> with <code>target.type: Utilization</code>"
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: web
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: web
              minReplicas: 2
              maxReplicas: 10
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 50
        - id: v4
          title: HPA with Memory Target
          description: >-
            Write an HPA YAML (<code>autoscaling/v2</code>) for the Deployment <code>cache</code> that
            targets 70% average memory utilization, with min 1 and max 8 replicas.
          hints:
            - "Same structure as a CPU HPA but change <code>resource.name</code> to <code>memory</code>"
            - "Memory-based scaling is trickier because many apps don't release memory after usage drops"
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: cache
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: cache
              minReplicas: 1
              maxReplicas: 8
              metrics:
              - type: Resource
                resource:
                  name: memory
                  target:
                    type: Utilization
                    averageUtilization: 70
        - id: v5
          title: HPA with Both CPU and Memory
          description: >-
            Write an HPA YAML for the Deployment <code>web</code> that targets both 50% CPU and 70% memory
            utilization, with min 2 and max 15 replicas. The HPA will use whichever metric produces the highest replica count.
          hints:
            - "List both metrics under the <code>metrics</code> array"
            - "The HPA evaluates each metric independently and picks the highest desired replica count"
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: web
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: web
              minReplicas: 2
              maxReplicas: 15
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 50
              - type: Resource
                resource:
                  name: memory
                  target:
                    type: Utilization
                    averageUtilization: 70
        - id: v6
          title: HPA for a StatefulSet
          description: >-
            Write an HPA YAML for a <strong>StatefulSet</strong> named <code>postgres</code> targeting
            60% CPU, with min 3 and max 6 replicas. Note the <code>scaleTargetRef.kind</code> must change.
          hints:
            - "Change <code>scaleTargetRef.kind</code> to <code>StatefulSet</code>"
            - "Everything else is identical to a Deployment HPA"
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: postgres
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: StatefulSet
                name: postgres
              minReplicas: 3
              maxReplicas: 6
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 60
        - id: v7
          title: Inspect an HPA
          description: >-
            Write the kubectl command to see the detailed status of an HPA named <code>web</code>,
            including its conditions and scaling events.
          hints:
            - "Use <code>describe</code> to see conditions and events"
            - "<code>kubectl get hpa</code> shows a summary; <code>describe</code> shows details"
          solution: |-
            kubectl describe hpa web
        - id: v8
          title: HPA with Custom Metric (Pods Type)
          description: >-
            Write an HPA metric entry (just the <code>metrics</code> array item) that scales on a custom
            Pod metric called <code>http_requests_per_second</code> with an average value target of 1000.
          hints:
            - "Use <code>type: Pods</code> instead of <code>type: Resource</code>"
            - "For Pods metrics, the target type is <code>AverageValue</code>"
          solution: |-
            - type: Pods
              pods:
                metric:
                  name: http_requests_per_second
                target:
                  type: AverageValue
                  averageValue: 1000
        - id: v9
          title: HPA Targeting Aggressive Min/Max
          description: >-
            Create an imperative HPA for Deployment <code>worker</code> targeting 40% CPU with min 5 and max 50 replicas.
            This is a high-throughput worker that needs aggressive scaling.
          hints:
            - "Same <code>kubectl autoscale</code> syntax, just different numbers"
          solution: |-
            kubectl autoscale deployment worker --cpu-percent=40 --min=5 --max=50
        - id: v10
          title: View HPA as YAML
          description: >-
            Write the kubectl command to output the full YAML representation of the HPA named <code>web</code>,
            which shows the complete spec including current metric values and conditions.
          hints:
            - "Use <code>-o yaml</code> output format on a get command"
          solution: |-
            kubectl get hpa web -o yaml
        - id: v11
          title: HPA with averageValue CPU Target
          description: >-
            Write an HPA metric entry that targets an absolute average CPU value of <code>200m</code> (200 millicores)
            per Pod instead of a percentage. Use the <code>AverageValue</code> target type.
          hints:
            - "Instead of <code>type: Utilization</code>, use <code>type: AverageValue</code>"
            - "Specify <code>averageValue: 200m</code> instead of <code>averageUtilization</code>"
          solution: |-
            - type: Resource
              resource:
                name: cpu
                target:
                  type: AverageValue
                  averageValue: 200m
        - id: v12
          title: Delete an HPA
          description: >-
            Write the kubectl command to delete the HPA named <code>web</code> and stop autoscaling the Deployment.
            The Deployment will keep its current replica count.
          hints:
            - "Use <code>kubectl delete hpa</code>"
          solution: |-
            kubectl delete hpa web
        - id: v13
          title: List All HPAs
          description: >-
            Write the kubectl command to list all HPAs across all namespaces, showing their targets, min/max, and current
            replica counts.
          hints:
            - "Use <code>-A</code> for all namespaces"
          solution: |-
            kubectl get hpa -A

    - id: warmup_2
      concept: HPA Algorithm
      variants:
        - id: v1
          title: Basic Scale-Up Calculation
          description: >-
            An HPA targets 50% CPU. Currently there are <strong>3 replicas</strong> each using <strong>80% CPU</strong>.
            Apply the HPA formula to calculate the desired replica count.<br><br>
            Formula: <code>desiredReplicas = ceil(currentReplicas * (currentMetric / targetMetric))</code>
          hints:
            - "Plug in: ceil(3 * (80 / 50))"
            - "80 / 50 = 1.6, then 3 * 1.6 = 4.8, ceil(4.8) = 5"
          solution: |-
            desiredReplicas = ceil(3 * (80 / 50))
                            = ceil(3 * 1.6)
                            = ceil(4.8)
                            = 5
        - id: v2
          title: Basic Scale-Down Calculation
          description: >-
            An HPA targets 50% CPU. Currently there are <strong>5 replicas</strong> each using <strong>20% CPU</strong>.
            Calculate the desired replica count.
          hints:
            - "Plug in: ceil(5 * (20 / 50))"
            - "20 / 50 = 0.4, then 5 * 0.4 = 2.0, ceil(2.0) = 2"
          solution: |-
            desiredReplicas = ceil(5 * (20 / 50))
                            = ceil(5 * 0.4)
                            = ceil(2.0)
                            = 2
        - id: v3
          title: Scale-Up with High Load
          description: >-
            An HPA targets 60% CPU with min 2, max 20. Currently there are <strong>4 replicas</strong> each at
            <strong>150% CPU</strong> (burst above request). What is the desired replica count?
          hints:
            - "ceil(4 * (150 / 60)) = ceil(4 * 2.5) = ceil(10) = 10"
            - "Check: 10 is within the 2-20 range, so no clamping needed"
          solution: |-
            desiredReplicas = ceil(4 * (150 / 60))
                            = ceil(4 * 2.5)
                            = ceil(10.0)
                            = 10
        - id: v4
          title: Clamped by maxReplicas
          description: >-
            An HPA targets 50% CPU with min 2, <strong>max 8</strong>. Currently there are <strong>6 replicas</strong>
            each using <strong>90% CPU</strong>. Calculate the desired replica count. Will it be clamped?
          hints:
            - "ceil(6 * (90 / 50)) = ceil(6 * 1.8) = ceil(10.8) = 11"
            - "11 exceeds maxReplicas of 8, so the HPA clamps to 8"
          solution: |-
            desiredReplicas = ceil(6 * (90 / 50))
                            = ceil(6 * 1.8)
                            = ceil(10.8)
                            = 11

            Clamped to maxReplicas: 8
        - id: v5
          title: Clamped by minReplicas
          description: >-
            An HPA targets 70% CPU with <strong>min 3</strong>, max 15. Currently there are <strong>8 replicas</strong>
            each using <strong>10% CPU</strong>. What is the desired count after clamping?
          hints:
            - "ceil(8 * (10 / 70)) = ceil(8 * 0.1428) = ceil(1.14) = 2"
            - "2 is below minReplicas of 3, so clamp to 3"
          solution: |-
            desiredReplicas = ceil(8 * (10 / 70))
                            = ceil(8 * 0.1428)
                            = ceil(1.14)
                            = 2

            Clamped to minReplicas: 3
        - id: v6
          title: Memory-Based Calculation
          description: >-
            An HPA targets 70% memory. Currently there are <strong>4 replicas</strong> each using <strong>90% memory</strong>.
            Calculate the desired count.
          hints:
            - "Same formula applies to memory: ceil(4 * (90 / 70))"
            - "90 / 70 = 1.2857, then 4 * 1.2857 = 5.14, ceil = 6"
          solution: |-
            desiredReplicas = ceil(4 * (90 / 70))
                            = ceil(4 * 1.2857)
                            = ceil(5.14)
                            = 6
        - id: v7
          title: No Scaling Needed
          description: >-
            An HPA targets 50% CPU. Currently there are <strong>4 replicas</strong> each using <strong>48% CPU</strong>.
            What does the formula produce? Will the HPA scale?
          hints:
            - "ceil(4 * (48 / 50)) = ceil(4 * 0.96) = ceil(3.84) = 4"
            - "Result equals current replicas — no scaling action"
          solution: |-
            desiredReplicas = ceil(4 * (48 / 50))
                            = ceil(4 * 0.96)
                            = ceil(3.84)
                            = 4

            No scaling needed (desired == current).
        - id: v8
          title: Scale from 1 Replica
          description: >-
            An HPA targets 50% CPU with min 1, max 10. There is <strong>1 replica</strong> using
            <strong>250% CPU</strong> (extreme burst). What is the desired count?
          hints:
            - "ceil(1 * (250 / 50)) = ceil(5.0) = 5"
          solution: |-
            desiredReplicas = ceil(1 * (250 / 50))
                            = ceil(1 * 5.0)
                            = ceil(5.0)
                            = 5
        - id: v9
          title: Uneven Load Average
          description: >-
            An HPA targets 50% CPU. There are <strong>3 replicas</strong>. Pod A uses 90%, Pod B uses 10%, Pod C uses 50%.
            The HPA averages across all Pods. What is the average and the desired count?
          hints:
            - "Average CPU = (90 + 10 + 50) / 3 = 50%"
            - "ceil(3 * (50 / 50)) = ceil(3) = 3 — no scaling despite Pod A being overloaded"
          solution: |-
            Average CPU = (90 + 10 + 50) / 3 = 50%

            desiredReplicas = ceil(3 * (50 / 50))
                            = ceil(3 * 1.0)
                            = ceil(3.0)
                            = 3

            No scaling. Uneven distribution tricks the HPA.
        - id: v10
          title: Large Scale-Up
          description: >-
            An HPA targets 40% CPU with min 2, max 100. Currently there are <strong>10 replicas</strong>
            each using <strong>95% CPU</strong>. Calculate the desired replica count.
          hints:
            - "ceil(10 * (95 / 40)) = ceil(10 * 2.375) = ceil(23.75) = 24"
            - "24 is within 2-100 range, so no clamping"
          solution: |-
            desiredReplicas = ceil(10 * (95 / 40))
                            = ceil(10 * 2.375)
                            = ceil(23.75)
                            = 24
        - id: v11
          title: Multi-Metric — Highest Wins
          description: >-
            An HPA has two metrics: CPU target 50% and memory target 70%. Currently there are <strong>4 replicas</strong>
            using 60% CPU and 90% memory. Calculate each metric's desired count. Which one does the HPA use?
          hints:
            - "CPU: ceil(4 * (60/50)) = ceil(4.8) = 5"
            - "Memory: ceil(4 * (90/70)) = ceil(5.14) = 6"
            - "HPA picks the highest: 6"
          solution: |-
            CPU:    ceil(4 * (60 / 50)) = ceil(4.8) = 5
            Memory: ceil(4 * (90 / 70)) = ceil(5.14) = 6

            HPA uses the highest: 6 replicas.
        - id: v12
          title: KEDA Queue-Based Calculation
          description: >-
            A KEDA ScaledObject targets a RabbitMQ queue with <code>queueLength: "5"</code> (1 replica per 5 messages).
            The queue currently has <strong>37 messages</strong>. How many replicas does KEDA create?
          hints:
            - "KEDA divides: ceil(37 / 5) = ceil(7.4) = 8"
          solution: |-
            desiredReplicas = ceil(37 / 5)
                            = ceil(7.4)
                            = 8

    - id: warmup_3
      concept: Scaling Behavior
      variants:
        - id: v1
          title: Basic scaleDown Stabilization
          description: >-
            Write an HPA <code>behavior</code> block that sets the scale-down stabilization window to 5 minutes
            (300 seconds). This prevents flapping after traffic drops.
          hints:
            - "The <code>behavior</code> key is at the same level as <code>metrics</code> in the HPA spec"
            - "Use <code>scaleDown.stabilizationWindowSeconds: 300</code>"
          solution: |-
            behavior:
              scaleDown:
                stabilizationWindowSeconds: 300
        - id: v2
          title: scaleUp Stabilization
          description: >-
            Write a <code>behavior</code> block that sets the scale-up stabilization window to 30 seconds.
            This prevents over-scaling from short metric spikes.
          hints:
            - "Use <code>scaleUp.stabilizationWindowSeconds: 30</code>"
          solution: |-
            behavior:
              scaleUp:
                stabilizationWindowSeconds: 30
        - id: v3
          title: Limit scaleUp Rate by Pods
          description: >-
            Write a <code>behavior</code> block that limits scale-up to at most <strong>4 Pods</strong> per 60-second
            period.
          hints:
            - "Use a policy with <code>type: Pods</code> and <code>value: 4</code>"
            - "<code>periodSeconds</code> defines the time window for the policy"
          solution: |-
            behavior:
              scaleUp:
                policies:
                - type: Pods
                  value: 4
                  periodSeconds: 60
        - id: v4
          title: Limit scaleUp Rate by Percent
          description: >-
            Write a <code>behavior</code> block that limits scale-up to at most <strong>100%</strong> of current replicas
            per 60-second period (i.e., at most double the current count each minute).
          hints:
            - "Use <code>type: Percent</code> and <code>value: 100</code>"
          solution: |-
            behavior:
              scaleUp:
                policies:
                - type: Percent
                  value: 100
                  periodSeconds: 60
        - id: v5
          title: Combined scaleUp Policies with Max
          description: >-
            Write a <code>behavior.scaleUp</code> block with two policies: add at most 4 Pods OR double the count
            (100%) per 60 seconds. Use <code>selectPolicy: Max</code> so the HPA picks whichever allows more Pods.
          hints:
            - "List both policies under <code>policies</code>"
            - "<code>selectPolicy: Max</code> takes the more aggressive policy"
          solution: |-
            behavior:
              scaleUp:
                policies:
                - type: Pods
                  value: 4
                  periodSeconds: 60
                - type: Percent
                  value: 100
                  periodSeconds: 60
                selectPolicy: Max
        - id: v6
          title: Conservative scaleDown Policy
          description: >-
            Write a <code>behavior.scaleDown</code> block with a 300-second stabilization window and a policy
            that removes at most 2 Pods per 60 seconds. Use <code>selectPolicy: Min</code> for conservative scaling.
          hints:
            - "<code>selectPolicy: Min</code> picks the more conservative (fewer removals) policy"
          solution: |-
            behavior:
              scaleDown:
                stabilizationWindowSeconds: 300
                policies:
                - type: Pods
                  value: 2
                  periodSeconds: 60
                selectPolicy: Min
        - id: v7
          title: Disable scaleDown Entirely
          description: >-
            Write a <code>behavior</code> block that completely disables scale-down. The HPA can scale up but
            will never remove Pods. Use <code>selectPolicy: Disabled</code>.
          hints:
            - "<code>selectPolicy: Disabled</code> turns off that scaling direction entirely"
          solution: |-
            behavior:
              scaleDown:
                selectPolicy: Disabled
        - id: v8
          title: Full Aggressive Up / Conservative Down
          description: >-
            Write a complete <code>behavior</code> block that scales up aggressively (30s stabilization, max 10 Pods or
            200% per 60s, pick Max) and scales down conservatively (300s stabilization, max 1 Pod per 120s, pick Min).
          hints:
            - "Combine <code>scaleUp</code> and <code>scaleDown</code> in the same behavior block"
            - "Scale up: use <code>selectPolicy: Max</code> for aggressive"
            - "Scale down: use <code>selectPolicy: Min</code> for conservative"
          solution: |-
            behavior:
              scaleUp:
                stabilizationWindowSeconds: 30
                policies:
                - type: Pods
                  value: 10
                  periodSeconds: 60
                - type: Percent
                  value: 200
                  periodSeconds: 60
                selectPolicy: Max
              scaleDown:
                stabilizationWindowSeconds: 300
                policies:
                - type: Pods
                  value: 1
                  periodSeconds: 120
                selectPolicy: Min
        - id: v9
          title: Limit scaleDown by Percent
          description: >-
            Write a <code>behavior.scaleDown</code> block that removes at most <strong>10%</strong> of current
            Pods per 60-second period with a 600-second stabilization window.
          hints:
            - "Use <code>type: Percent</code> with <code>value: 10</code>"
          solution: |-
            behavior:
              scaleDown:
                stabilizationWindowSeconds: 600
                policies:
                - type: Percent
                  value: 10
                  periodSeconds: 60
        - id: v10
          title: Zero Stabilization for scaleUp
          description: >-
            Write a <code>behavior.scaleUp</code> block with zero stabilization window (scale immediately when
            metrics spike) and allow up to 50% growth per 30 seconds.
          hints:
            - "Set <code>stabilizationWindowSeconds: 0</code> for instant scale-up"
          solution: |-
            behavior:
              scaleUp:
                stabilizationWindowSeconds: 0
                policies:
                - type: Percent
                  value: 50
                  periodSeconds: 30

    - id: warmup_4
      concept: KEDA
      variants:
        - id: v1
          title: KEDA ScaledObject — RabbitMQ
          description: >-
            Write a KEDA <code>ScaledObject</code> YAML that scales the Deployment <code>worker</code> based on a
            RabbitMQ queue named <code>tasks</code>. Scale 1 replica per 5 messages, min 0
            (scale to zero), max 30. Poll every 15 seconds with a 300-second cooldown.
          hints:
            - "Use <code>apiVersion: keda.sh/v1alpha1</code> and <code>kind: ScaledObject</code>"
            - "The trigger type is <code>rabbitmq</code> with <code>queueName</code> and <code>queueLength</code>"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: rabbitmq-worker
            spec:
              scaleTargetRef:
                name: worker
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 30
              triggers:
              - type: rabbitmq
                metadata:
                  queueName: tasks
                  queueLength: "5"
                  host: amqp://user:pass@rabbitmq.default.svc.cluster.local:5672
        - id: v2
          title: KEDA ScaledObject — Cron (Business Hours)
          description: >-
            Write a KEDA ScaledObject that scales Deployment <code>web</code> to 10 replicas during business hours
            (8 AM to 6 PM, Monday through Friday, Eastern time). Outside those hours, it can scale to zero.
          hints:
            - "Use the <code>cron</code> trigger type"
            - "Cron start/end use cron expressions: <code>0 8 * * 1-5</code> for 8 AM Mon-Fri"
            - "<code>desiredReplicas</code> is a string in the metadata"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: business-hours
            spec:
              scaleTargetRef:
                name: web
              minReplicaCount: 0
              maxReplicaCount: 10
              triggers:
              - type: cron
                metadata:
                  timezone: America/New_York
                  start: 0 8 * * 1-5
                  end: 0 18 * * 1-5
                  desiredReplicas: "10"
        - id: v3
          title: KEDA ScaledObject — AWS SQS
          description: >-
            Write a KEDA ScaledObject that scales Deployment <code>order-processor</code> based on an
            AWS SQS queue named <code>orders</code> in region <code>us-east-1</code>. Target 10 messages
            per replica, max 20 replicas, scale to zero when idle.
          hints:
            - "Trigger type is <code>aws-sqs-queue</code>"
            - "Use <code>queueURL</code> and <code>queueLength</code> in metadata"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: order-processor-scaler
            spec:
              scaleTargetRef:
                name: order-processor
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 20
              triggers:
              - type: aws-sqs-queue
                metadata:
                  queueURL: https://sqs.us-east-1.amazonaws.com/123456789012/orders
                  queueLength: "10"
                  awsRegion: us-east-1
        - id: v4
          title: KEDA ScaledObject — Kafka Consumer Lag
          description: >-
            Write a KEDA ScaledObject that scales Deployment <code>event-consumer</code> based on Kafka
            consumer group lag for topic <code>events</code>, consumer group <code>event-group</code>.
            Target lag threshold of 100, max 15 replicas.
          hints:
            - "Trigger type is <code>kafka</code>"
            - "Key metadata fields: <code>bootstrapServers</code>, <code>consumerGroup</code>, <code>topic</code>, <code>lagThreshold</code>"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: kafka-event-consumer
            spec:
              scaleTargetRef:
                name: event-consumer
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 15
              triggers:
              - type: kafka
                metadata:
                  bootstrapServers: kafka.default.svc.cluster.local:9092
                  consumerGroup: event-group
                  topic: events
                  lagThreshold: "100"
        - id: v5
          title: KEDA ScaledObject — Prometheus
          description: >-
            Write a KEDA ScaledObject that scales Deployment <code>api</code> based on a Prometheus query.
            Scale when the rate of HTTP requests exceeds 500 per second. Use the PromQL query
            <code>sum(rate(http_requests_total{app="api"}[2m]))</code>.
          hints:
            - "Trigger type is <code>prometheus</code>"
            - "Key fields: <code>serverAddress</code>, <code>query</code>, <code>threshold</code>"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: api-prometheus-scaler
            spec:
              scaleTargetRef:
                name: api
              pollingInterval: 15
              cooldownPeriod: 120
              minReplicaCount: 1
              maxReplicaCount: 20
              triggers:
              - type: prometheus
                metadata:
                  serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
                  query: sum(rate(http_requests_total{app="api"}[2m]))
                  threshold: "500"
        - id: v6
          title: KEDA ScaledObject — Redis List
          description: >-
            Write a KEDA ScaledObject that scales Deployment <code>job-runner</code> based on the length
            of a Redis list named <code>job_queue</code>. Target 3 messages per replica, max 10 replicas.
          hints:
            - "Trigger type is <code>redis-lists</code>"
            - "Use <code>listName</code> and <code>listLength</code> in metadata"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: redis-job-scaler
            spec:
              scaleTargetRef:
                name: job-runner
              pollingInterval: 10
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 10
              triggers:
              - type: redis-lists
                metadata:
                  address: redis.default.svc.cluster.local:6379
                  listName: job_queue
                  listLength: "3"
        - id: v7
          title: KEDA ScaledObject — PostgreSQL Query
          description: >-
            Write a KEDA ScaledObject that scales Deployment <code>order-worker</code> based on a PostgreSQL
            query that counts pending orders. Scale when there are more than 5 pending orders per replica, max 8 replicas.
          hints:
            - "Trigger type is <code>postgresql</code>"
            - "Use <code>query</code> and <code>targetQueryValue</code> in metadata"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: order-worker-scaler
            spec:
              scaleTargetRef:
                name: order-worker
              pollingInterval: 30
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 8
              triggers:
              - type: postgresql
                metadata:
                  connectionFromEnv: PG_CONNECTION
                  query: "SELECT COUNT(*) FROM orders WHERE status = 'pending'"
                  targetQueryValue: "5"
        - id: v8
          title: KEDA with Multiple Triggers
          description: >-
            Write a KEDA ScaledObject for Deployment <code>hybrid-worker</code> that combines two triggers:
            a RabbitMQ queue (<code>tasks</code>, 5 messages per replica) and a cron schedule (scale to 5
            replicas during 9 AM-5 PM UTC weekdays). Max 20 replicas.
          hints:
            - "List multiple triggers in the <code>triggers</code> array"
            - "KEDA evaluates all triggers and uses the highest desired replica count"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: hybrid-worker-scaler
            spec:
              scaleTargetRef:
                name: hybrid-worker
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 20
              triggers:
              - type: rabbitmq
                metadata:
                  queueName: tasks
                  queueLength: "5"
                  host: amqp://user:pass@rabbitmq.default.svc.cluster.local:5672
              - type: cron
                metadata:
                  timezone: UTC
                  start: 0 9 * * 1-5
                  end: 0 17 * * 1-5
                  desiredReplicas: "5"
        - id: v9
          title: KEDA Install Commands
          description: >-
            Write the Helm commands to install KEDA into a cluster. Add the KEDA Helm repo, update it,
            and install KEDA into the <code>keda</code> namespace.
          hints:
            - "Three steps: <code>helm repo add</code>, <code>helm repo update</code>, <code>helm install</code>"
            - "Use <code>--namespace keda --create-namespace</code>"
          solution: |-
            helm repo add kedacore https://kedacore.github.io/charts
            helm repo update
            helm install keda kedacore/keda --namespace keda --create-namespace
        - id: v10
          title: KEDA ScaledObject — HTTP Trigger
          description: >-
            Write a KEDA ScaledObject that scales Deployment <code>web-api</code> based on concurrent
            HTTP request count. Scale when there are more than 100 concurrent requests per replica,
            min 1, max 25 replicas.
          hints:
            - "KEDA's HTTP add-on uses the <code>http</code> trigger type (requires KEDA HTTP add-on)"
            - "Not all KEDA installations include the HTTP add-on by default"
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: web-api-http-scaler
            spec:
              scaleTargetRef:
                name: web-api
              pollingInterval: 10
              cooldownPeriod: 120
              minReplicaCount: 1
              maxReplicaCount: 25
              triggers:
              - type: prometheus
                metadata:
                  serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
                  query: sum(nginx_ingress_controller_requests{service="web-api"})
                  threshold: "100"

  challenges:
    - id: challenge_1
      block: 1
      difficulty: 2
      concept: HPA Basics
      variants:
        - id: v1
          title: Production Web App HPA
          description: >-
            Design a complete HPA for a production web application Deployment called <code>frontend</code>.
            Requirements:<br>
            - Target 60% CPU utilization<br>
            - Min 3 replicas (for HA), max 25<br>
            - Scale up fast: 0s stabilization, up to 5 Pods or 100% growth per minute, pick Max<br>
            - Scale down slow: 300s stabilization, remove at most 2 Pods per minute, pick Min<br>
            Write the complete HPA YAML.
          functionSignature: "HorizontalPodAutoscaler (autoscaling/v2)"
          testCases:
            - input: "Steady 30% CPU, 3 replicas"
              output: "No scaling — 30% is below 60% target"
            - input: "Spike to 120% CPU, 3 replicas"
              output: "Scale to ceil(3 * 120/60) = 6 replicas"
            - input: "Drop to 10% CPU, 6 replicas"
              output: "After 300s, scale down by at most 2 Pods per minute"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                A production HPA needs both metrics and behavior policies. The behavior block controls
                how fast scaling happens in each direction.
            - title: "\U0001F4A1 Hint"
              content: >-
                Structure: <code>scaleTargetRef</code> → <code>minReplicas/maxReplicas</code> → <code>metrics</code>
                → <code>behavior</code> with both <code>scaleUp</code> and <code>scaleDown</code> sections.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>behavior:
                  scaleUp:
                    stabilizationWindowSeconds: 0
                    policies:
                    - type: Pods ...
                    - type: Percent ...
                    selectPolicy: Max
                  scaleDown:
                    stabilizationWindowSeconds: 300
                    policies:
                    - type: Pods ...
                    selectPolicy: Min</pre>
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: frontend
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: frontend
              minReplicas: 3
              maxReplicas: 25
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 60
              behavior:
                scaleUp:
                  stabilizationWindowSeconds: 0
                  policies:
                  - type: Pods
                    value: 5
                    periodSeconds: 60
                  - type: Percent
                    value: 100
                    periodSeconds: 60
                  selectPolicy: Max
                scaleDown:
                  stabilizationWindowSeconds: 300
                  policies:
                  - type: Pods
                    value: 2
                    periodSeconds: 60
                  selectPolicy: Min
          difficulty: 2
        - id: v2
          title: Multi-Metric API Server HPA
          description: >-
            Design an HPA for a Deployment called <code>api-server</code> with the following requirements:<br>
            - Target both 50% CPU and 80% memory utilization<br>
            - Min 2, max 20 replicas<br>
            - Use the default scaling behavior (no custom behavior block needed)<br>
            When both metrics are evaluated, the HPA uses the one that produces the highest replica count.
          functionSignature: "HorizontalPodAutoscaler (autoscaling/v2)"
          testCases:
            - input: "4 replicas, 60% CPU, 40% memory"
              output: "CPU: ceil(4*60/50)=5, Memory: ceil(4*40/80)=2 → HPA picks 5"
            - input: "4 replicas, 30% CPU, 95% memory"
              output: "CPU: ceil(4*30/50)=3, Memory: ceil(4*95/80)=5 → HPA picks 5"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Multi-metric HPAs list multiple entries in the <code>metrics</code> array. Each metric is
                evaluated independently and the highest desired count wins.
            - title: "\U0001F4A1 Hint"
              content: >-
                Both metrics use <code>type: Resource</code>. One has <code>name: cpu</code>, the other
                <code>name: memory</code>. Each has its own <code>averageUtilization</code> target.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>metrics:
                - type: Resource
                  resource:
                    name: cpu
                    target: ...
                - type: Resource
                  resource:
                    name: memory
                    target: ...</pre>
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: api-server
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: api-server
              minReplicas: 2
              maxReplicas: 20
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 50
              - type: Resource
                resource:
                  name: memory
                  target:
                    type: Utilization
                    averageUtilization: 80
          difficulty: 2
        - id: v3
          title: Batch Worker with Disabled Scale-Down
          description: >-
            Design an HPA for a batch processing Deployment called <code>batch-worker</code>:<br>
            - Target 40% CPU (workers should stay ahead of load)<br>
            - Min 1, max 50 replicas<br>
            - Scale up aggressively: 0s stabilization, allow 200% growth per 30s<br>
            - Scale down disabled: once scaled up, manual intervention is needed to scale down<br>
            This pattern is used when losing workers mid-batch is costly.
          functionSignature: "HorizontalPodAutoscaler (autoscaling/v2)"
          testCases:
            - input: "1 replica, 120% CPU"
              output: "Scale to ceil(1*120/40) = 3"
            - input: "10 replicas, 5% CPU"
              output: "No scale-down (disabled), stays at 10"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Disabling scale-down is done with <code>selectPolicy: Disabled</code> in the
                <code>behavior.scaleDown</code> section. This is useful for batch jobs.
            - title: "\U0001F4A1 Hint"
              content: >-
                Set <code>behavior.scaleDown.selectPolicy: Disabled</code>. The HPA will never reduce
                replicas — you must do it manually or delete the HPA.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>behavior:
                  scaleUp:
                    stabilizationWindowSeconds: 0
                    policies:
                    - type: Percent
                      value: 200
                      periodSeconds: 30
                  scaleDown:
                    selectPolicy: Disabled</pre>
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: batch-worker
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: batch-worker
              minReplicas: 1
              maxReplicas: 50
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 40
              behavior:
                scaleUp:
                  stabilizationWindowSeconds: 0
                  policies:
                  - type: Percent
                    value: 200
                    periodSeconds: 30
                scaleDown:
                  selectPolicy: Disabled
          difficulty: 3
        - id: v4
          title: Mixed Metrics with Custom and Resource
          description: >-
            Design an HPA for Deployment <code>checkout</code> with three metrics:<br>
            - 50% CPU utilization<br>
            - 70% memory utilization<br>
            - Custom Pod metric <code>http_requests_per_second</code> with average value target of 500<br>
            Min 3, max 30 replicas. Include a behavior block with 60s scale-up stabilization and 600s scale-down
            stabilization.
          functionSignature: "HorizontalPodAutoscaler (autoscaling/v2)"
          testCases:
            - input: "5 replicas, 30% CPU, 40% mem, 800 rps average"
              output: "CPU: ceil(5*30/50)=3, Mem: ceil(5*40/70)=3, RPS: ceil(5*800/500)=8 → HPA picks 8"
            - input: "8 replicas, 80% CPU, 50% mem, 300 rps average"
              output: "CPU: ceil(8*80/50)=13, Mem: ceil(8*50/70)=6, RPS: ceil(8*300/500)=5 → HPA picks 13"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                You can mix <code>type: Resource</code> and <code>type: Pods</code> metrics in the same HPA.
                The HPA evaluates all metrics and uses the highest desired count.
            - title: "\U0001F4A1 Hint"
              content: >-
                The custom metric uses <code>type: Pods</code> with <code>pods.metric.name</code> and
                <code>pods.target.type: AverageValue</code>.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>metrics:
                - type: Resource     # CPU
                - type: Resource     # Memory
                - type: Pods         # Custom metric
                  pods:
                    metric:
                      name: http_requests_per_second
                    target:
                      type: AverageValue
                      averageValue: 500</pre>
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: checkout
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: checkout
              minReplicas: 3
              maxReplicas: 30
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 50
              - type: Resource
                resource:
                  name: memory
                  target:
                    type: Utilization
                    averageUtilization: 70
              - type: Pods
                pods:
                  metric:
                    name: http_requests_per_second
                  target:
                    type: AverageValue
                    averageValue: 500
              behavior:
                scaleUp:
                  stabilizationWindowSeconds: 60
                scaleDown:
                  stabilizationWindowSeconds: 600
          difficulty: 3
        - id: v5
          title: HPA with VPA Recommendations Only
          description: >-
            Design a setup for Deployment <code>new-service</code> where:<br>
            - An HPA handles horizontal scaling on CPU (target 50%, min 2, max 12)<br>
            - A VPA in <code>Off</code> mode provides resource request recommendations without
              actually changing Pods (to avoid conflict with HPA)<br>
            Write both the HPA YAML and the VPA YAML.
          functionSignature: "HorizontalPodAutoscaler + VerticalPodAutoscaler"
          testCases:
            - input: "VPA recommends CPU: 200m, Memory: 256Mi"
              output: "VPA shows recommendations only; HPA controls replica count"
            - input: "HPA sees 80% CPU on 2 replicas"
              output: "HPA scales to ceil(2*80/50)=4 replicas"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The key is the VPA's <code>updateMode: "Off"</code>. This means the VPA will calculate and
                display recommendations but never apply them. This is safe to use alongside an HPA on CPU.
            - title: "\U0001F4A1 Hint"
              content: >-
                Write two separate YAML documents. The VPA uses <code>apiVersion: autoscaling.k8s.io/v1</code>
                and <code>kind: VerticalPodAutoscaler</code>.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre># HPA targets CPU for horizontal scaling
                # VPA in Off mode observes and recommends
                # No conflict because VPA never modifies Pods</pre>
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: new-service
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: new-service
              minReplicas: 2
              maxReplicas: 12
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 50
            ---
            apiVersion: autoscaling.k8s.io/v1
            kind: VerticalPodAutoscaler
            metadata:
              name: new-service-vpa
            spec:
              targetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: new-service
              updatePolicy:
                updateMode: "Off"
          difficulty: 3
        - id: v6
          title: Gradual Scale-Down for Stateful Connections
          description: >-
            Design an HPA for a WebSocket server Deployment called <code>ws-gateway</code>:<br>
            - Target 60% CPU, min 4, max 40<br>
            - Scale up: 0s stabilization, 4 Pods or 50% per 60s, Max policy<br>
            - Scale down very slowly: 600s stabilization, remove at most 1 Pod per 120s, Min policy<br>
            The slow scale-down protects active WebSocket connections from being terminated too quickly.
          functionSignature: "HorizontalPodAutoscaler (autoscaling/v2)"
          testCases:
            - input: "10 replicas at 90% CPU"
              output: "Scale to ceil(10*90/60) = 15, limited by scaleUp policies"
            - input: "15 replicas at 20% CPU"
              output: "After 600s, remove 1 Pod every 120s"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                WebSocket servers need very conservative scale-down because each connection is long-lived.
                Terminating a Pod drops all its connections.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use a long stabilization window (600s) and remove at most 1 Pod per 120-second period.
                <code>selectPolicy: Min</code> ensures conservative behavior.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>scaleDown:
                  stabilizationWindowSeconds: 600
                  policies:
                  - type: Pods
                    value: 1
                    periodSeconds: 120
                  selectPolicy: Min</pre>
          solution: |-
            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: ws-gateway
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: ws-gateway
              minReplicas: 4
              maxReplicas: 40
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 60
              behavior:
                scaleUp:
                  stabilizationWindowSeconds: 0
                  policies:
                  - type: Pods
                    value: 4
                    periodSeconds: 60
                  - type: Percent
                    value: 50
                    periodSeconds: 60
                  selectPolicy: Max
                scaleDown:
                  stabilizationWindowSeconds: 600
                  policies:
                  - type: Pods
                    value: 1
                    periodSeconds: 120
                  selectPolicy: Min
          difficulty: 4
        - id: v7
          title: VPA Auto Mode for New Workload
          description: >-
            Write a VPA YAML in <code>Auto</code> mode for a Deployment called <code>ml-inference</code>.
            Constrain the <code>ml-inference</code> container to between 500m-4 CPU and 512Mi-8Gi memory.
            Control both CPU and memory.
          functionSignature: "VerticalPodAutoscaler (autoscaling.k8s.io/v1)"
          testCases:
            - input: "Container using 200m CPU and 256Mi memory"
              output: "VPA recommends at least 500m CPU (minAllowed) and adjusts memory"
            - input: "Container using 6 CPU"
              output: "VPA caps recommendation at 4 CPU (maxAllowed)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                VPA in Auto mode will evict and recreate Pods with updated resource requests. The
                <code>resourcePolicy</code> sets min/max bounds for recommendations.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use <code>containerPolicies</code> under <code>resourcePolicy</code> to set
                <code>minAllowed</code>, <code>maxAllowed</code>, and <code>controlledResources</code>.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>resourcePolicy:
                  containerPolicies:
                  - containerName: ml-inference
                    minAllowed:
                      cpu: ...
                      memory: ...
                    maxAllowed:
                      cpu: ...
                      memory: ...</pre>
          solution: |-
            apiVersion: autoscaling.k8s.io/v1
            kind: VerticalPodAutoscaler
            metadata:
              name: ml-inference-vpa
            spec:
              targetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: ml-inference
              updatePolicy:
                updateMode: "Auto"
              resourcePolicy:
                containerPolicies:
                - containerName: ml-inference
                  minAllowed:
                    cpu: 500m
                    memory: 512Mi
                  maxAllowed:
                    cpu: 4
                    memory: 8Gi
                  controlledResources: ["cpu", "memory"]
          difficulty: 2

    - id: challenge_2
      block: 1
      difficulty: 1
      concept: HPA Algorithm
      variants:
        - id: v1
          title: Step-by-Step Scale-Up Prediction
          description: >-
            A Deployment <code>web</code> has this HPA configuration:<br>
            - Target: 50% CPU<br>
            - Min: 2, Max: 12<br>
            - Current: 3 replicas, average 85% CPU<br><br>
            Calculate the desired replica count step by step. Show the formula with values substituted.
          functionSignature: "desiredReplicas = ceil(currentReplicas * (currentMetric / targetMetric))"
          testCases:
            - input: "3 replicas, 85% avg CPU, target 50%"
              output: "ceil(3 * (85/50)) = ceil(3 * 1.7) = ceil(5.1) = 6"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The ratio 85/50 = 1.7 means we're using 70% more CPU than target. We need approximately
                70% more replicas to bring the average down.
            - title: "\U0001F4A1 Hint"
              content: >-
                Substitute into the formula: ceil(3 * (85 / 50)). Remember ceil rounds UP to the next integer.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Compute ratio: currentMetric / targetMetric
                2. Multiply by currentReplicas
                3. Apply ceil()
                4. Clamp between min and max</pre>
          solution: |-
            desiredReplicas = ceil(currentReplicas * (currentMetric / targetMetric))
                            = ceil(3 * (85 / 50))
                            = ceil(3 * 1.7)
                            = ceil(5.1)
                            = 6

            6 is within [2, 12] — no clamping needed.
            Result: scale from 3 → 6 replicas.
          difficulty: 1
        - id: v2
          title: Scale-Down with Clamping
          description: >-
            HPA config:<br>
            - Target: 70% memory<br>
            - Min: 3, Max: 15<br>
            - Current: 10 replicas, average 15% memory<br><br>
            Calculate the desired count. Will it be clamped?
          functionSignature: "desiredReplicas = ceil(currentReplicas * (currentMetric / targetMetric))"
          testCases:
            - input: "10 replicas, 15% avg memory, target 70%, min 3"
              output: "ceil(10 * (15/70)) = ceil(2.14) = 3 (clamped to min)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The ratio 15/70 is very small (0.214). This means the workload is massively over-provisioned.
                But the HPA can't go below minReplicas.
            - title: "\U0001F4A1 Hint"
              content: >-
                ceil(10 * 0.214) = ceil(2.14) = 3. Is 3 within the min/max range?
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. ratio = 15/70 = 0.214
                2. desired = ceil(10 * 0.214) = ceil(2.14) = 3
                3. Check: min=3, max=15 → 3 is at the minimum boundary
                4. Result: 3 (happens to match min exactly)</pre>
          solution: |-
            desiredReplicas = ceil(10 * (15 / 70))
                            = ceil(10 * 0.2143)
                            = ceil(2.143)
                            = 3

            3 equals minReplicas (3) — at the lower boundary.
            Result: scale from 10 → 3 replicas.
          difficulty: 1
        - id: v3
          title: Multiple Metrics — Which Wins?
          description: >-
            HPA config:<br>
            - Metrics: 50% CPU and 80% memory<br>
            - Min: 2, Max: 20<br>
            - Current: 6 replicas, 70% CPU average, 60% memory average<br><br>
            Calculate the desired count for each metric separately. Which metric does the HPA use?
          functionSignature: "desiredReplicas = max(ceil(...CPU...), ceil(...Memory...))"
          testCases:
            - input: "6 replicas, 70% CPU (target 50%), 60% memory (target 80%)"
              output: "CPU: ceil(6*70/50)=9, Memory: ceil(6*60/80)=5 → HPA uses 9"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                With multiple metrics, the HPA calculates desired replicas for each metric independently and
                picks the highest value. This ensures no metric is over the target.
            - title: "\U0001F4A1 Hint"
              content: >-
                CPU: ceil(6 * 70/50). Memory: ceil(6 * 60/80). The higher one wins.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>For each metric:
                  desired = ceil(current * (actual / target))
                Final = max(all desired values)
                Then clamp to [min, max]</pre>
          solution: |-
            CPU:    ceil(6 * (70 / 50)) = ceil(6 * 1.4) = ceil(8.4) = 9
            Memory: ceil(6 * (60 / 80)) = ceil(6 * 0.75) = ceil(4.5) = 5

            HPA picks the highest: 9 replicas.
            9 is within [2, 20] — no clamping.
            Result: scale from 6 → 9 replicas.
          difficulty: 2
        - id: v4
          title: Max Replicas Capped
          description: >-
            HPA config:<br>
            - Target: 50% CPU<br>
            - Min: 2, Max: 8<br>
            - Current: 5 replicas, average 120% CPU<br><br>
            Calculate the desired count. What happens when it exceeds maxReplicas?
          functionSignature: "desiredReplicas = ceil(currentReplicas * (currentMetric / targetMetric))"
          testCases:
            - input: "5 replicas, 120% CPU, target 50%, max 8"
              output: "ceil(5 * 120/50) = ceil(12) = 12, clamped to max 8"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                When the formula produces a number higher than maxReplicas, the HPA clamps it. The app
                will still be over target — this is when you need to raise maxReplicas or optimize the app.
            - title: "\U0001F4A1 Hint"
              content: >-
                ceil(5 * 2.4) = ceil(12) = 12. But max is 8. The HPA hits the ceiling.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. desired = ceil(5 * (120/50)) = 12
                2. 12 > maxReplicas (8) → clamp to 8
                3. HPA condition: ScalingLimited = True
                4. You'll see this in kubectl describe hpa</pre>
          solution: |-
            desiredReplicas = ceil(5 * (120 / 50))
                            = ceil(5 * 2.4)
                            = ceil(12.0)
                            = 12

            Clamped to maxReplicas: 8.
            HPA condition ScalingLimited will be True.
            Result: scale from 5 → 8 replicas (maxed out).
          difficulty: 2
        - id: v5
          title: KEDA Queue-Based Replica Prediction
          description: >-
            A KEDA ScaledObject targets a RabbitMQ queue with <code>queueLength: "10"</code> (1 replica per
            10 messages). The queue has <strong>73 messages</strong>. Min 0, max 20.<br><br>
            How many replicas does KEDA create?
          functionSignature: "desiredReplicas = ceil(queueDepth / queueLength)"
          testCases:
            - input: "73 messages in queue, queueLength target 10, max 20"
              output: "ceil(73/10) = ceil(7.3) = 8"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                KEDA divides the current queue depth by the queueLength target to determine how many
                replicas are needed to process the backlog.
            - title: "\U0001F4A1 Hint"
              content: >-
                ceil(73 / 10) = ceil(7.3) = 8. Check against max of 20.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>desired = ceil(queueDepth / queueLength)
                        = ceil(73 / 10)
                        = 8
                8 ≤ 20 (max) → no clamping</pre>
          solution: |-
            desiredReplicas = ceil(73 / 10)
                            = ceil(7.3)
                            = 8

            8 is within [0, 20] — no clamping.
            Result: KEDA scales to 8 replicas.
          difficulty: 1
        - id: v6
          title: Behavior Policy Rate Limiting
          description: >-
            An HPA currently has <strong>4 replicas</strong> and the formula says scale to <strong>15</strong>.
            The scaleUp behavior allows at most 3 Pods per 60-second period.<br><br>
            How many replicas exist after the first scaling period? After the second? How long to reach 15?
          functionSignature: "Replicas per period with scaleUp.policies"
          testCases:
            - input: "4 replicas, desired 15, scaleUp max 3 Pods per 60s"
              output: "Period 1: 4→7, Period 2: 7→10, Period 3: 10→13, Period 4: 13→15"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The behavior policy limits how fast the HPA can scale. Even though it wants 15 replicas
                immediately, it can only add 3 per minute.
            - title: "\U0001F4A1 Hint"
              content: >-
                Each period: add min(3, remaining). Period 1: 4+3=7. Period 2: 7+3=10. Keep going until 15.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Period 1 (0-60s):   4 + 3 = 7
                Period 2 (60-120s):  7 + 3 = 10
                Period 3 (120-180s): 10 + 3 = 13
                Period 4 (180-240s): 13 + 2 = 15 (only need 2 more)
                Total time: ~4 minutes</pre>
          solution: |-
            Period 1 (0-60s):    4 + 3 = 7 replicas
            Period 2 (60-120s):  7 + 3 = 10 replicas
            Period 3 (120-180s): 10 + 3 = 13 replicas
            Period 4 (180-240s): 13 + 2 = 15 replicas

            Total time to reach desired: ~4 minutes (4 periods).
          difficulty: 3
        - id: v7
          title: Scale-Down with Stabilization Delay
          description: >-
            An HPA has 8 replicas and the formula says scale to 3. The scaleDown behavior has:<br>
            - stabilizationWindowSeconds: 300<br>
            - Policy: max 2 Pods per 60s<br><br>
            Describe the timeline: when does scale-down start, and how long to reach 3?
          functionSignature: "Timeline with stabilization + rate limiting"
          testCases:
            - input: "8 replicas, desired 3, 300s stabilization, max 2 per 60s"
              output: "Wait 300s, then: 8→6→4→3 over ~3 periods"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                First, the HPA waits the stabilization window (300s). Then it applies the rate limit policy.
                This is why scale-down is slow by design.
            - title: "\U0001F4A1 Hint"
              content: >-
                300s waiting + (3 periods * 60s each) = ~480 seconds total (~8 minutes).
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>0-300s: Stabilization window (no action)
                300-360s: 8 - 2 = 6
                360-420s: 6 - 2 = 4
                420-480s: 4 - 1 = 3 (only need to remove 1 more)
                Total: ~8 minutes from metric drop to target</pre>
          solution: |-
            0-300s:    Stabilization window — HPA takes no action
            300-360s:  Period 1: 8 - 2 = 6 replicas
            360-420s:  Period 2: 6 - 2 = 4 replicas
            420-480s:  Period 3: 4 - 1 = 3 replicas (only 1 needed)

            Total time: ~8 minutes from metric drop to reaching 3 replicas.
            This is intentional — slow scale-down prevents flapping.
          difficulty: 2
        - id: v8
          title: Percent-Based scaleUp Policy
          description: >-
            An HPA currently has <strong>8 replicas</strong> and desires <strong>30</strong>. The scaleUp behavior
            has a single Percent policy: 50% per 60 seconds.<br><br>
            Trace the replica count over each period until 30 is reached.
          functionSignature: "Replicas per period with Percent policy"
          testCases:
            - input: "8 replicas, desired 30, scaleUp 50% per 60s"
              output: "8→12→18→27→30 over 4 periods"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                With a Percent policy, the number of Pods added each period grows because 50% of a larger
                number is more Pods. This gives exponential-style growth.
            - title: "\U0001F4A1 Hint"
              content: >-
                Period 1: 8 + ceil(8*0.5) = 8+4 = 12. Period 2: 12 + ceil(12*0.5) = 12+6 = 18. Continue...
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Each period: new = current + ceil(current * 50%)
                P1: 8 + 4 = 12
                P2: 12 + 6 = 18
                P3: 18 + 9 = 27
                P4: 27 + 3 = 30 (capped at desired)</pre>
          solution: |-
            Period 1: 8 + ceil(8 * 0.50) = 8 + 4 = 12
            Period 2: 12 + ceil(12 * 0.50) = 12 + 6 = 18
            Period 3: 18 + ceil(18 * 0.50) = 18 + 9 = 27
            Period 4: 27 + 3 = 30 (only 3 more needed)

            Total: 4 periods (~4 minutes) to reach 30 replicas.
          difficulty: 3

    - id: challenge_3
      block: 2
      difficulty: 2
      concept: KEDA
      variants:
        - id: v1
          title: Queue Consumer — HPA vs KEDA
          description: >-
            You have a Deployment <code>email-sender</code> that processes emails from a RabbitMQ queue.
            Requirements:<br>
            - Scale to zero when the queue is empty (no emails to send)<br>
            - Scale up based on queue depth, 1 replica per 10 messages<br>
            - Max 15 replicas<br><br>
            Can a standard HPA meet these requirements? If not, write the appropriate KEDA ScaledObject.
            Explain why you chose this autoscaler.
          functionSignature: "ScaledObject (keda.sh/v1alpha1)"
          testCases:
            - input: "Queue empty for 5 minutes"
              output: "KEDA scales to 0 replicas after cooldown"
            - input: "50 messages arrive in queue"
              output: "KEDA scales to ceil(50/10) = 5 replicas"
            - input: "Can standard HPA scale to zero?"
              output: "No — HPA minReplicas must be >= 1"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Standard HPA cannot scale to zero (minReplicas must be at least 1). It also cannot
                natively watch a RabbitMQ queue. KEDA solves both problems.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use a KEDA ScaledObject with <code>minReplicaCount: 0</code> and a <code>rabbitmq</code>
                trigger. Set <code>cooldownPeriod</code> to avoid immediate scale-to-zero.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Standard HPA limitations:
                  - Cannot scale to 0
                  - Cannot watch external queues natively
                KEDA advantages:
                  - Scale to zero
                  - 60+ trigger types including RabbitMQ
                  - Creates and manages HPA behind the scenes</pre>
          solution: |-
            # Standard HPA cannot meet these requirements because:
            # 1. HPA minReplicas must be >= 1 (cannot scale to zero)
            # 2. HPA cannot natively watch RabbitMQ queue depth

            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: email-sender-scaler
            spec:
              scaleTargetRef:
                name: email-sender
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 15
              triggers:
              - type: rabbitmq
                metadata:
                  queueName: emails
                  queueLength: "10"
                  host: amqp://user:pass@rabbitmq.default.svc.cluster.local:5672
          difficulty: 2
        - id: v2
          title: Stateless Web App — HPA is Enough
          description: >-
            You have a Deployment <code>product-api</code> that serves REST API requests. Requirements:<br>
            - Scale based on CPU utilization (target 60%)<br>
            - Always have at least 2 replicas for HA<br>
            - Max 10 replicas<br>
            - Standard behavior is fine<br><br>
            Which autoscaler should you use? Write the configuration. Explain why KEDA is not needed here.
          functionSignature: "HorizontalPodAutoscaler (autoscaling/v2)"
          testCases:
            - input: "CPU at 90% with 2 replicas"
              output: "HPA scales to ceil(2*90/60) = 3"
            - input: "Why not KEDA?"
              output: "No need — CPU-based scaling is built into HPA, no external metrics needed"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                KEDA is for event-driven, external-metric, or scale-to-zero scenarios. A simple CPU-based
                web app is exactly what the standard HPA was designed for.
            - title: "\U0001F4A1 Hint"
              content: >-
                Write a standard <code>autoscaling/v2</code> HPA. No need for KEDA, VPA, or custom metrics.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Decision tree:
                  Scale on CPU/memory? → HPA
                  Scale on external events? → KEDA
                  Right-size resource requests? → VPA
                  Need more nodes? → Cluster Autoscaler</pre>
          solution: |-
            # Standard HPA is the right choice because:
            # - CPU-based scaling is built-in (no external metrics)
            # - Min 2 replicas (no scale-to-zero needed)
            # - Simple, well-understood, no extra dependencies

            apiVersion: autoscaling/v2
            kind: HorizontalPodAutoscaler
            metadata:
              name: product-api
            spec:
              scaleTargetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: product-api
              minReplicas: 2
              maxReplicas: 10
              metrics:
              - type: Resource
                resource:
                  name: cpu
                  target:
                    type: Utilization
                    averageUtilization: 60
          difficulty: 2
        - id: v3
          title: Unknown Resource Needs — VPA Recommendation
          description: >-
            You're deploying a new service <code>ml-pipeline</code> and have no idea what CPU/memory it needs.
            You want to observe its real usage for a week and get recommendations without disrupting the
            running Pods.<br><br>
            Which autoscaler helps here? Write the configuration. Explain why HPA is not the right tool.
          functionSignature: "VerticalPodAutoscaler (autoscaling.k8s.io/v1)"
          testCases:
            - input: "Service runs for a week"
              output: "VPA recommends target CPU: 250m, memory: 384Mi"
            - input: "Why not HPA?"
              output: "HPA adjusts replica count, not resource requests — wrong problem"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The problem is "what resources should I request?" not "how many replicas do I need?"
                VPA in Off mode answers this without touching your running Pods.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use VPA with <code>updateMode: "Off"</code>. After a week, run <code>kubectl describe vpa</code>
                to see Target, Lower Bound, and Upper Bound recommendations.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Problem: "What resources does my app need?"
                  → VPA (vertical — adjust requests)
                  → Not HPA (horizontal — adjust replicas)
                  → Use Off mode first, then switch to Auto when confident</pre>
          solution: |-
            # VPA in Off mode is the right choice because:
            # - The problem is right-sizing resources, not scaling replicas
            # - Off mode only generates recommendations, never modifies Pods
            # - After gaining confidence, you can switch to Auto mode

            apiVersion: autoscaling.k8s.io/v1
            kind: VerticalPodAutoscaler
            metadata:
              name: ml-pipeline-vpa
            spec:
              targetRef:
                apiVersion: apps/v1
                kind: Deployment
                name: ml-pipeline
              updatePolicy:
                updateMode: "Off"
              resourcePolicy:
                containerPolicies:
                - containerName: ml-pipeline
                  minAllowed:
                    cpu: 50m
                    memory: 64Mi
                  maxAllowed:
                    cpu: 8
                    memory: 16Gi
                  controlledResources: ["cpu", "memory"]
          difficulty: 2
        - id: v4
          title: Cron-Based Pre-Scaling for Predictable Traffic
          description: >-
            Your e-commerce Deployment <code>storefront</code> gets predictable traffic spikes every weekday
            from 9 AM to 9 PM (Pacific time). You want:<br>
            - 10 replicas during business hours<br>
            - Scale to zero overnight and on weekends to save costs<br>
            - Also scale based on CPU (target 60%) during business hours in case traffic exceeds predictions<br><br>
            Design the KEDA ScaledObject with both cron and CPU-based triggers.
          functionSignature: "ScaledObject (keda.sh/v1alpha1)"
          testCases:
            - input: "Tuesday 10 AM Pacific, 30% CPU"
              output: "10 replicas (cron trigger keeps minimum)"
            - input: "Tuesday 10 AM Pacific, 95% CPU on 10 replicas"
              output: "Scales beyond 10 based on CPU metric"
            - input: "Sunday 3 AM Pacific"
              output: "0 replicas (outside cron window, no load)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                KEDA can combine multiple triggers. The cron trigger sets a floor during business hours,
                while CPU scaling handles unexpected spikes above the floor.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use two triggers: a <code>cron</code> trigger for the schedule and a <code>cpu</code>
                trigger (or Prometheus-based) for reactive scaling.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>triggers:
                - type: cron        # Predictable floor
                  metadata:
                    start/end: business hours
                    desiredReplicas: "10"
                - type: cpu         # Reactive ceiling
                  metadata:
                    type: Utilization
                    value: "60"</pre>
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: storefront-scaler
            spec:
              scaleTargetRef:
                name: storefront
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 30
              triggers:
              - type: cron
                metadata:
                  timezone: America/Los_Angeles
                  start: 0 9 * * 1-5
                  end: 0 21 * * 1-5
                  desiredReplicas: "10"
              - type: cpu
                metadata:
                  type: Utilization
                  value: "60"
          difficulty: 3
        - id: v5
          title: Multi-Queue Processing Architecture
          description: >-
            You have three Deployments, each processing a different queue:<br>
            - <code>order-worker</code>: processes from SQS queue <code>orders</code> (1 replica per 5 messages, max 20)<br>
            - <code>notification-worker</code>: processes from RabbitMQ queue <code>notifications</code>
              (1 replica per 20 messages, max 10)<br>
            - <code>analytics-worker</code>: processes from Kafka topic <code>events</code>, consumer group
              <code>analytics</code> (lag threshold 500, max 8)<br><br>
            All should scale to zero when idle. Write all three KEDA ScaledObjects.
          functionSignature: "3x ScaledObject (keda.sh/v1alpha1)"
          testCases:
            - input: "orders queue: 25 messages"
              output: "order-worker: ceil(25/5) = 5 replicas"
            - input: "All queues empty"
              output: "All three Deployments at 0 replicas"
            - input: "Kafka lag: 2000 on analytics topic"
              output: "analytics-worker: ceil(2000/500) = 4 replicas"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Each Deployment gets its own ScaledObject. Each ScaledObject has a different trigger type
                matched to its queue technology.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use <code>aws-sqs-queue</code>, <code>rabbitmq</code>, and <code>kafka</code> trigger types
                respectively. Each ScaledObject targets a different Deployment.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>One ScaledObject per Deployment:
                  order-worker → aws-sqs-queue trigger
                  notification-worker → rabbitmq trigger
                  analytics-worker → kafka trigger
                All with minReplicaCount: 0</pre>
          solution: |-
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: order-worker-scaler
            spec:
              scaleTargetRef:
                name: order-worker
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 20
              triggers:
              - type: aws-sqs-queue
                metadata:
                  queueURL: https://sqs.us-east-1.amazonaws.com/123456789012/orders
                  queueLength: "5"
                  awsRegion: us-east-1
            ---
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: notification-worker-scaler
            spec:
              scaleTargetRef:
                name: notification-worker
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 10
              triggers:
              - type: rabbitmq
                metadata:
                  queueName: notifications
                  queueLength: "20"
                  host: amqp://user:pass@rabbitmq.default.svc.cluster.local:5672
            ---
            apiVersion: keda.sh/v1alpha1
            kind: ScaledObject
            metadata:
              name: analytics-worker-scaler
            spec:
              scaleTargetRef:
                name: analytics-worker
              pollingInterval: 15
              cooldownPeriod: 300
              minReplicaCount: 0
              maxReplicaCount: 8
              triggers:
              - type: kafka
                metadata:
                  bootstrapServers: kafka.default.svc.cluster.local:9092
                  consumerGroup: analytics
                  topic: events
                  lagThreshold: "500"
          difficulty: 3
        - id: v6
          title: HPA + Cluster Autoscaler Interaction
          description: >-
            Your cluster has 3 nodes, each with 4 CPU cores. The HPA for Deployment <code>compute-heavy</code>
            (requests 1 CPU per Pod) just scaled from 8 to 14 replicas. Only 12 Pods fit on existing nodes
            (the rest are used by system Pods).<br><br>
            Describe what happens next. Which autoscalers are involved and in what order?
            What configuration ensures this works?
          functionSignature: "HPA + Cluster Autoscaler interaction"
          testCases:
            - input: "HPA wants 14 replicas, only 12 fit"
              output: "2 Pods stuck Pending → Cluster Autoscaler adds node → Pods scheduled"
            - input: "What prerequisite is needed?"
              output: "Pods must have resource requests set for the scheduler to calculate fit"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                HPA creates Pod replicas. If nodes are full, Pods are stuck Pending. The Cluster Autoscaler
                watches for Pending Pods and adds nodes. They work together naturally.
            - title: "\U0001F4A1 Hint"
              content: >-
                The sequence is: HPA scales up → scheduler can't place 2 Pods → Pods Pending →
                Cluster Autoscaler detects → adds node → scheduler places Pods.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. HPA: 8 → 14 replicas
                2. Scheduler: 12 fit, 2 Pending (Insufficient cpu)
                3. Cluster Autoscaler: detects Pending Pods
                4. CA: adds new node to node group
                5. New node joins cluster (~2-5 min)
                6. Scheduler: places 2 Pending Pods on new node
                Prerequisite: resource requests on all Pods</pre>
          solution: |-
            Sequence of events:

            1. HPA scales Deployment from 8 → 14 replicas
            2. Scheduler places 12 Pods on existing 3 nodes (4 CPU each = 12 CPU total, minus system overhead)
            3. 2 Pods stuck in Pending state with reason "Insufficient cpu"
            4. Cluster Autoscaler detects Pending Pods (checks every ~10s)
            5. CA calculates: Pending Pods need 2 CPU → 1 new node (4 CPU) is sufficient
            6. CA requests new node from cloud provider (AWS ASG / GCP MIG)
            7. New node provisions and joins cluster (~2-5 minutes)
            8. Scheduler places 2 Pending Pods on new node

            Prerequisites:
            - Pods MUST have resource requests (cpu: 1) for scheduler to calculate fit
            - Cluster Autoscaler must be installed and configured with node group
            - Node group must have room to grow (max size > current size)
          difficulty: 3
