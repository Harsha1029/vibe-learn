conceptLinks:
  Volume Types: "#lesson-volume-types"
  PersistentVolumes: "#lesson-persistent-volumes"
  PersistentVolumeClaims: "#lesson-persistent-volume-claims"
  StorageClasses: "#lesson-storage-classes"
  Dynamic Provisioning: "#lesson-dynamic-provisioning"
  StatefulSet Storage: "#lesson-statefulset-storage"
sharedContent: {}
variants:
  warmups:
    - id: warmup_1
      concept: Volume Types
      variants:
        - id: v1
          title: emptyDir Scratch Volume
          description: >-
            Write a Pod YAML named <code>scratch-pod</code> with a single container <code>busybox</code> that mounts
            an <code>emptyDir</code> volume named <code>scratch</code> at <code>/tmp/scratch</code>.
          hints:
            - "Define the volume under <code>spec.volumes</code> with <code>emptyDir: {}</code>."
            - "Mount it in the container using <code>volumeMounts</code> with <code>mountPath</code> and <code>name</code>."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: scratch-pod
            spec:
              containers:
                - name: busybox
                  image: busybox
                  volumeMounts:
                    - name: scratch
                      mountPath: /tmp/scratch
              volumes:
                - name: scratch
                  emptyDir: {}
        - id: v2
          title: emptyDir Shared Between Containers
          description: >-
            Write a Pod YAML named <code>sidecar-pod</code> with two containers: <code>writer</code> (image
            <code>busybox</code>) and <code>reader</code> (image <code>busybox</code>). Both share an
            <code>emptyDir</code> volume named <code>shared-data</code> mounted at <code>/data</code>.
          hints:
            - "Both containers reference the same volume name in their <code>volumeMounts</code>."
            - "The volume is defined once under <code>spec.volumes</code>."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: sidecar-pod
            spec:
              containers:
                - name: writer
                  image: busybox
                  volumeMounts:
                    - name: shared-data
                      mountPath: /data
                - name: reader
                  image: busybox
                  volumeMounts:
                    - name: shared-data
                      mountPath: /data
              volumes:
                - name: shared-data
                  emptyDir: {}
        - id: v3
          title: emptyDir with Memory Medium
          description: >-
            Write a Pod YAML named <code>tmpfs-pod</code> with container <code>app</code> (image <code>nginx</code>)
            that mounts an <code>emptyDir</code> volume backed by RAM (medium Memory) named <code>cache</code> at
            <code>/cache</code> with a <code>sizeLimit</code> of <code>128Mi</code>.
          hints:
            - "Set <code>emptyDir.medium</code> to <code>\"Memory\"</code>."
            - "Use <code>emptyDir.sizeLimit</code> to cap the size."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: tmpfs-pod
            spec:
              containers:
                - name: app
                  image: nginx
                  volumeMounts:
                    - name: cache
                      mountPath: /cache
              volumes:
                - name: cache
                  emptyDir:
                    medium: Memory
                    sizeLimit: 128Mi
        - id: v4
          title: hostPath Volume
          description: >-
            Write a Pod YAML named <code>log-pod</code> with container <code>logger</code> (image
            <code>busybox</code>) that mounts the host directory <code>/var/log</code> at <code>/host-logs</code>
            using a <code>hostPath</code> volume named <code>host-logs</code> with type <code>Directory</code>.
          hints:
            - "Use <code>hostPath.path</code> for the host directory and <code>hostPath.type</code> for validation."
            - "Type <code>Directory</code> ensures the path must already exist as a directory on the host."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: log-pod
            spec:
              containers:
                - name: logger
                  image: busybox
                  volumeMounts:
                    - name: host-logs
                      mountPath: /host-logs
              volumes:
                - name: host-logs
                  hostPath:
                    path: /var/log
                    type: Directory
        - id: v5
          title: hostPath with DirectoryOrCreate
          description: >-
            Write a Pod YAML named <code>data-pod</code> with container <code>app</code> (image <code>alpine</code>)
            that mounts host path <code>/mnt/data</code> at <code>/data</code> using a <code>hostPath</code> volume
            named <code>data-vol</code> with type <code>DirectoryOrCreate</code>.
          hints:
            - "<code>DirectoryOrCreate</code> creates the directory on the host if it does not exist."
            - "The volume name must match between <code>volumes</code> and <code>volumeMounts</code>."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: data-pod
            spec:
              containers:
                - name: app
                  image: alpine
                  volumeMounts:
                    - name: data-vol
                      mountPath: /data
              volumes:
                - name: data-vol
                  hostPath:
                    path: /mnt/data
                    type: DirectoryOrCreate
        - id: v6
          title: hostPath for Docker Socket
          description: >-
            Write a Pod YAML named <code>docker-pod</code> with container <code>docker-cli</code> (image
            <code>docker:cli</code>) that mounts the Docker socket <code>/var/run/docker.sock</code> at the same
            path inside the container. Use hostPath type <code>Socket</code> and volume name <code>docker-sock</code>.
          hints:
            - "Use <code>hostPath.type: Socket</code> to validate the path is a Unix socket."
            - "The mount path and host path can be the same value."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: docker-pod
            spec:
              containers:
                - name: docker-cli
                  image: docker:cli
                  volumeMounts:
                    - name: docker-sock
                      mountPath: /var/run/docker.sock
              volumes:
                - name: docker-sock
                  hostPath:
                    path: /var/run/docker.sock
                    type: Socket
        - id: v7
          title: Multiple Volumes in One Pod
          description: >-
            Write a Pod YAML named <code>multi-vol-pod</code> with container <code>app</code> (image
            <code>nginx</code>) that mounts two volumes: an <code>emptyDir</code> named <code>tmp</code> at
            <code>/tmp</code> and a <code>hostPath</code> named <code>config</code> at <code>/etc/app-config</code>
            pointing to host path <code>/etc/myapp</code>.
          hints:
            - "List multiple entries under <code>spec.volumes</code>."
            - "List multiple entries under <code>container.volumeMounts</code>, each referencing a different volume name."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: multi-vol-pod
            spec:
              containers:
                - name: app
                  image: nginx
                  volumeMounts:
                    - name: tmp
                      mountPath: /tmp
                    - name: config
                      mountPath: /etc/app-config
              volumes:
                - name: tmp
                  emptyDir: {}
                - name: config
                  hostPath:
                    path: /etc/myapp
        - id: v8
          title: emptyDir for Init Container Handoff
          description: >-
            Write a Pod YAML named <code>init-pod</code> with an init container <code>setup</code> (image
            <code>busybox</code>, command <code>["sh", "-c", "echo ready > /work/status"]</code>) and a main
            container <code>app</code> (image <code>nginx</code>). Both share an <code>emptyDir</code> volume named
            <code>workdir</code> mounted at <code>/work</code>.
          hints:
            - "Init containers are defined under <code>spec.initContainers</code>."
            - "Init containers run to completion before main containers start, so data written to the shared volume is available."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: init-pod
            spec:
              initContainers:
                - name: setup
                  image: busybox
                  command: ["sh", "-c", "echo ready > /work/status"]
                  volumeMounts:
                    - name: workdir
                      mountPath: /work
              containers:
                - name: app
                  image: nginx
                  volumeMounts:
                    - name: workdir
                      mountPath: /work
              volumes:
                - name: workdir
                  emptyDir: {}
        - id: v9
          title: Read-Only Volume Mount
          description: >-
            Write a Pod YAML named <code>readonly-pod</code> with container <code>app</code> (image
            <code>nginx</code>) that mounts a <code>hostPath</code> volume named <code>certs</code> (host path
            <code>/etc/ssl/certs</code>) at <code>/certs</code> as <strong>read-only</strong>.
          hints:
            - "Add <code>readOnly: true</code> to the <code>volumeMounts</code> entry."
            - "This prevents the container from writing to the mounted path."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: readonly-pod
            spec:
              containers:
                - name: app
                  image: nginx
                  volumeMounts:
                    - name: certs
                      mountPath: /certs
                      readOnly: true
              volumes:
                - name: certs
                  hostPath:
                    path: /etc/ssl/certs
        - id: v10
          title: emptyDir with SubPath Mount
          description: >-
            Write a Pod YAML named <code>subpath-pod</code> with container <code>app</code> (image
            <code>nginx</code>) that uses an <code>emptyDir</code> volume named <code>data</code>. Mount it twice:
            at <code>/data/logs</code> with <code>subPath: logs</code> and at <code>/data/cache</code> with
            <code>subPath: cache</code>.
          hints:
            - "Use <code>subPath</code> in <code>volumeMounts</code> to mount a subdirectory of the volume."
            - "You can reference the same volume name in multiple mount entries with different subPaths."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: subpath-pod
            spec:
              containers:
                - name: app
                  image: nginx
                  volumeMounts:
                    - name: data
                      mountPath: /data/logs
                      subPath: logs
                    - name: data
                      mountPath: /data/cache
                      subPath: cache
              volumes:
                - name: data
                  emptyDir: {}
        - id: v11
          title: hostPath FileOrCreate
          description: >-
            Write a Pod YAML named <code>file-pod</code> with container <code>app</code> (image <code>busybox</code>)
            that mounts a single file from the host at <code>/etc/hostname-override</code>. Use <code>hostPath</code>
            volume named <code>hostname-file</code> with path <code>/etc/hostname</code> and type
            <code>FileOrCreate</code>.
          hints:
            - "<code>FileOrCreate</code> creates the file on the host if it does not exist."
            - "This is useful for mounting individual config files rather than directories."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: file-pod
            spec:
              containers:
                - name: app
                  image: busybox
                  volumeMounts:
                    - name: hostname-file
                      mountPath: /etc/hostname-override
              volumes:
                - name: hostname-file
                  hostPath:
                    path: /etc/hostname
                    type: FileOrCreate
    - id: warmup_2
      concept: PersistentVolumes
      variants:
        - id: v1
          title: Basic PersistentVolume
          description: >-
            Write a PersistentVolume YAML named <code>pv-data</code> with <code>5Gi</code> capacity, access mode
            <code>ReadWriteOnce</code>, hostPath <code>/mnt/data</code>, and reclaim policy <code>Retain</code>.
          hints:
            - "PersistentVolume is a cluster-scoped resource (no namespace)."
            - "Access modes go under <code>spec.accessModes</code> as a list."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-data
            spec:
              capacity:
                storage: 5Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              hostPath:
                path: /mnt/data
        - id: v2
          title: PV with ReadWriteMany
          description: >-
            Write a PersistentVolume YAML named <code>pv-shared</code> with <code>10Gi</code> capacity, access mode
            <code>ReadWriteMany</code>, NFS source at server <code>nfs.example.com</code> path <code>/exports/data</code>,
            and reclaim policy <code>Retain</code>.
          hints:
            - "<code>ReadWriteMany</code> allows the volume to be mounted read-write by many nodes."
            - "NFS is defined with <code>nfs.server</code> and <code>nfs.path</code>."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-shared
            spec:
              capacity:
                storage: 10Gi
              accessModes:
                - ReadWriteMany
              persistentVolumeReclaimPolicy: Retain
              nfs:
                server: nfs.example.com
                path: /exports/data
        - id: v3
          title: PV with Recycle Policy
          description: >-
            Write a PersistentVolume YAML named <code>pv-temp</code> with <code>2Gi</code> capacity, access mode
            <code>ReadWriteOnce</code>, hostPath <code>/mnt/temp</code>, and reclaim policy <code>Recycle</code>.
          hints:
            - "<code>Recycle</code> performs a basic scrub (<code>rm -rf /thevolume/*</code>) when the PVC is deleted."
            - "Note: Recycle is deprecated in favor of dynamic provisioning. It only works with NFS and hostPath."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-temp
            spec:
              capacity:
                storage: 2Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Recycle
              hostPath:
                path: /mnt/temp
        - id: v4
          title: PV with Delete Policy
          description: >-
            Write a PersistentVolume YAML named <code>pv-cloud</code> with <code>50Gi</code> capacity, access mode
            <code>ReadWriteOnce</code>, reclaim policy <code>Delete</code>, and a <code>storageClassName</code> of
            <code>standard</code>. Use hostPath <code>/mnt/cloud</code>.
          hints:
            - "<code>Delete</code> means the underlying storage is removed when the PVC is deleted."
            - "<code>storageClassName</code> associates this PV with a specific StorageClass."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-cloud
            spec:
              capacity:
                storage: 50Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Delete
              storageClassName: standard
              hostPath:
                path: /mnt/cloud
        - id: v5
          title: PV with ReadOnlyMany
          description: >-
            Write a PersistentVolume YAML named <code>pv-readonly</code> with <code>1Gi</code> capacity, access mode
            <code>ReadOnlyMany</code>, hostPath <code>/mnt/readonly</code>, and reclaim policy <code>Retain</code>.
          hints:
            - "<code>ReadOnlyMany</code> allows the volume to be mounted read-only by many nodes."
            - "This is useful for shared configuration or static assets."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-readonly
            spec:
              capacity:
                storage: 1Gi
              accessModes:
                - ReadOnlyMany
              persistentVolumeReclaimPolicy: Retain
              hostPath:
                path: /mnt/readonly
        - id: v6
          title: PV with Multiple Access Modes
          description: >-
            Write a PersistentVolume YAML named <code>pv-multi</code> with <code>20Gi</code> capacity that supports
            both <code>ReadWriteOnce</code> and <code>ReadOnlyMany</code> access modes, hostPath
            <code>/mnt/multi</code>, and reclaim policy <code>Retain</code>.
          hints:
            - "A PV can list multiple access modes it supports."
            - "The actual access mode used is determined by the PVC that binds to it."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-multi
            spec:
              capacity:
                storage: 20Gi
              accessModes:
                - ReadWriteOnce
                - ReadOnlyMany
              persistentVolumeReclaimPolicy: Retain
              hostPath:
                path: /mnt/multi
        - id: v7
          title: PV with Labels
          description: >-
            Write a PersistentVolume YAML named <code>pv-labeled</code> with <code>5Gi</code> capacity, access mode
            <code>ReadWriteOnce</code>, hostPath <code>/mnt/labeled</code>, reclaim policy <code>Retain</code>, and
            labels <code>tier: fast</code> and <code>environment: production</code>.
          hints:
            - "Labels on PVs allow PVCs to select specific volumes using <code>selector.matchLabels</code>."
            - "Labels go under <code>metadata.labels</code>."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-labeled
              labels:
                tier: fast
                environment: production
            spec:
              capacity:
                storage: 5Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              hostPath:
                path: /mnt/labeled
        - id: v8
          title: PV with StorageClassName
          description: >-
            Write a PersistentVolume YAML named <code>pv-ssd</code> with <code>100Gi</code> capacity, access mode
            <code>ReadWriteOnce</code>, hostPath <code>/mnt/ssd</code>, reclaim policy <code>Retain</code>, and
            <code>storageClassName: fast-ssd</code>.
          hints:
            - "<code>storageClassName</code> is how PVCs find matching PVs (along with capacity and access modes)."
            - "Set an empty string <code>storageClassName: \"\"</code> to opt out of dynamic provisioning."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-ssd
            spec:
              capacity:
                storage: 100Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: fast-ssd
              hostPath:
                path: /mnt/ssd
        - id: v9
          title: PV with Node Affinity
          description: >-
            Write a PersistentVolume YAML named <code>pv-local</code> with <code>10Gi</code> capacity, access mode
            <code>ReadWriteOnce</code>, <code>storageClassName: local-storage</code>, hostPath
            <code>/mnt/local</code>, reclaim policy <code>Retain</code>, and a <code>nodeAffinity</code> requiring
            the node label <code>kubernetes.io/hostname: node-1</code>.
          hints:
            - "Node affinity on a PV constrains which node the volume is accessible from."
            - "Use <code>spec.nodeAffinity.required.nodeSelectorTerms</code>."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-local
            spec:
              capacity:
                storage: 10Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: local-storage
              hostPath:
                path: /mnt/local
              nodeAffinity:
                required:
                  nodeSelectorTerms:
                    - matchExpressions:
                        - key: kubernetes.io/hostname
                          operator: In
                          values:
                            - node-1
        - id: v10
          title: Large Capacity PV
          description: >-
            Write a PersistentVolume YAML named <code>pv-warehouse</code> with <code>500Gi</code> capacity, access
            mode <code>ReadWriteOnce</code>, hostPath <code>/mnt/warehouse</code>, reclaim policy <code>Delete</code>,
            and <code>storageClassName: bulk</code>.
          hints:
            - "Capacity is specified under <code>spec.capacity.storage</code> using standard Kubernetes resource quantities."
            - "Common units: <code>Ki</code>, <code>Mi</code>, <code>Gi</code>, <code>Ti</code>."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-warehouse
            spec:
              capacity:
                storage: 500Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Delete
              storageClassName: bulk
              hostPath:
                path: /mnt/warehouse
    - id: warmup_3
      concept: PersistentVolumeClaims
      variants:
        - id: v1
          title: Basic PVC
          description: >-
            Write a PersistentVolumeClaim YAML named <code>my-pvc</code> in namespace <code>default</code> requesting
            <code>1Gi</code> of storage with access mode <code>ReadWriteOnce</code>.
          hints:
            - "PVCs are namespaced resources."
            - "Request size goes under <code>spec.resources.requests.storage</code>."
          solution: |-
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: my-pvc
              namespace: default
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
        - id: v2
          title: PVC with StorageClass
          description: >-
            Write a PersistentVolumeClaim YAML named <code>fast-pvc</code> requesting <code>10Gi</code> with access
            mode <code>ReadWriteOnce</code> and <code>storageClassName: fast-ssd</code>.
          hints:
            - "Set <code>spec.storageClassName</code> to request a specific StorageClass."
            - "The PVC will only bind to PVs with a matching StorageClass."
          solution: |-
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: fast-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: fast-ssd
              resources:
                requests:
                  storage: 10Gi
        - id: v3
          title: PVC with Selector
          description: >-
            Write a PersistentVolumeClaim YAML named <code>prod-pvc</code> requesting <code>5Gi</code> with access
            mode <code>ReadWriteOnce</code> and a label selector matching <code>environment: production</code>.
          hints:
            - "Use <code>spec.selector.matchLabels</code> to bind to PVs with specific labels."
            - "This is useful when you have pre-provisioned PVs with labels."
          solution: |-
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: prod-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 5Gi
              selector:
                matchLabels:
                  environment: production
        - id: v4
          title: Pod Using a PVC
          description: >-
            Write a Pod YAML named <code>pvc-pod</code> with container <code>app</code> (image <code>nginx</code>)
            that mounts an existing PVC named <code>my-pvc</code> at <code>/usr/share/nginx/html</code> using volume
            name <code>web-content</code>.
          hints:
            - "Use <code>persistentVolumeClaim.claimName</code> in the volume definition."
            - "The volume name in <code>volumes</code> must match the name in <code>volumeMounts</code>."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: pvc-pod
            spec:
              containers:
                - name: app
                  image: nginx
                  volumeMounts:
                    - name: web-content
                      mountPath: /usr/share/nginx/html
              volumes:
                - name: web-content
                  persistentVolumeClaim:
                    claimName: my-pvc
        - id: v5
          title: PVC with ReadWriteMany
          description: >-
            Write a PersistentVolumeClaim YAML named <code>shared-pvc</code> requesting <code>20Gi</code> with access
            mode <code>ReadWriteMany</code> and <code>storageClassName: nfs</code>.
          hints:
            - "<code>ReadWriteMany</code> requires a storage backend that supports it, such as NFS."
            - "Most cloud block storage only supports <code>ReadWriteOnce</code>."
          solution: |-
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: shared-pvc
            spec:
              accessModes:
                - ReadWriteMany
              storageClassName: nfs
              resources:
                requests:
                  storage: 20Gi
        - id: v6
          title: Pod with Read-Only PVC Mount
          description: >-
            Write a Pod YAML named <code>reader-pod</code> with container <code>reader</code> (image
            <code>busybox</code>) that mounts PVC <code>data-pvc</code> at <code>/data</code> as
            <strong>read-only</strong>. Use volume name <code>data-vol</code>.
          hints:
            - "Set <code>readOnly: true</code> in the <code>volumeMounts</code> entry."
            - "You can also set <code>readOnly: true</code> on the <code>persistentVolumeClaim</code> volume definition."
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: reader-pod
            spec:
              containers:
                - name: reader
                  image: busybox
                  volumeMounts:
                    - name: data-vol
                      mountPath: /data
                      readOnly: true
              volumes:
                - name: data-vol
                  persistentVolumeClaim:
                    claimName: data-pvc
                    readOnly: true
        - id: v7
          title: PVC without StorageClass (default)
          description: >-
            Write a PersistentVolumeClaim YAML named <code>default-pvc</code> requesting <code>2Gi</code> with access
            mode <code>ReadWriteOnce</code>. Do not specify a <code>storageClassName</code> so the cluster default is
            used.
          hints:
            - "Omitting <code>storageClassName</code> entirely uses the cluster's default StorageClass."
            - "To explicitly avoid any StorageClass, set <code>storageClassName: \"\"</code>."
          solution: |-
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: default-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 2Gi
        - id: v8
          title: PVC Explicitly No StorageClass
          description: >-
            Write a PersistentVolumeClaim YAML named <code>static-pvc</code> requesting <code>5Gi</code> with access
            mode <code>ReadWriteOnce</code> that explicitly opts out of dynamic provisioning by setting
            <code>storageClassName</code> to an empty string.
          hints:
            - "Setting <code>storageClassName: \"\"</code> means the PVC will only bind to manually-created PVs with no StorageClass."
            - "This is different from omitting the field, which uses the default StorageClass."
          solution: |-
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: static-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: ""
              resources:
                requests:
                  storage: 5Gi
        - id: v9
          title: Deployment with PVC
          description: >-
            Write a Deployment YAML named <code>web-deploy</code> with 1 replica running container <code>nginx</code>
            (image <code>nginx:1.25</code>) that mounts PVC <code>web-pvc</code> at
            <code>/usr/share/nginx/html</code>. Use volume name <code>html</code>.
          hints:
            - "Volume and volumeMount definitions go inside the Pod template spec."
            - "Be careful: with replicas > 1 and <code>ReadWriteOnce</code>, Pods may fail to schedule on different nodes."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: web-deploy
            spec:
              replicas: 1
              selector:
                matchLabels:
                  app: web
              template:
                metadata:
                  labels:
                    app: web
                spec:
                  containers:
                    - name: nginx
                      image: nginx:1.25
                      volumeMounts:
                        - name: html
                          mountPath: /usr/share/nginx/html
                  volumes:
                    - name: html
                      persistentVolumeClaim:
                        claimName: web-pvc
        - id: v10
          title: PV and PVC Pair
          description: >-
            Write both a PersistentVolume named <code>app-pv</code> (10Gi, ReadWriteOnce, hostPath
            <code>/mnt/app</code>, Retain) and a matching PersistentVolumeClaim named <code>app-pvc</code> (10Gi,
            ReadWriteOnce). Both should have <code>storageClassName: manual</code>.
          hints:
            - "For static binding, both PV and PVC must have the same <code>storageClassName</code>."
            - "The PVC's requested size must be less than or equal to the PV's capacity."
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: app-pv
            spec:
              capacity:
                storage: 10Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: manual
              hostPath:
                path: /mnt/app
            ---
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: app-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: manual
              resources:
                requests:
                  storage: 10Gi
        - id: v11
          title: PVC with matchExpressions Selector
          description: >-
            Write a PersistentVolumeClaim YAML named <code>tier-pvc</code> requesting <code>8Gi</code> with access
            mode <code>ReadWriteOnce</code> and a selector using <code>matchExpressions</code> to match PVs where
            label <code>tier</code> is either <code>fast</code> or <code>premium</code>.
          hints:
            - "Use <code>spec.selector.matchExpressions</code> with operator <code>In</code>."
            - "matchExpressions gives more flexibility than matchLabels."
          solution: |-
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: tier-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 8Gi
              selector:
                matchExpressions:
                  - key: tier
                    operator: In
                    values:
                      - fast
                      - premium
    - id: warmup_4
      concept: StorageClasses
      variants:
        - id: v1
          title: Basic StorageClass
          description: >-
            Write a StorageClass YAML named <code>standard</code> using provisioner
            <code>kubernetes.io/no-provisioner</code> with reclaim policy <code>Retain</code> and volume binding mode
            <code>WaitForFirstConsumer</code>.
          hints:
            - "StorageClass uses apiVersion <code>storage.k8s.io/v1</code>."
            - "<code>WaitForFirstConsumer</code> delays binding until a Pod using the PVC is scheduled."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: standard
            provisioner: kubernetes.io/no-provisioner
            reclaimPolicy: Retain
            volumeBindingMode: WaitForFirstConsumer
        - id: v2
          title: AWS EBS StorageClass
          description: >-
            Write a StorageClass YAML named <code>fast-ebs</code> using provisioner
            <code>ebs.csi.aws.com</code> with parameters <code>type: gp3</code>, reclaim policy
            <code>Delete</code>, and volume binding mode <code>WaitForFirstConsumer</code>.
          hints:
            - "Cloud provisioners take backend-specific parameters."
            - "<code>gp3</code> is the current-generation AWS EBS general purpose SSD type."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: fast-ebs
            provisioner: ebs.csi.aws.com
            parameters:
              type: gp3
            reclaimPolicy: Delete
            volumeBindingMode: WaitForFirstConsumer
        - id: v3
          title: GCE PD StorageClass
          description: >-
            Write a StorageClass YAML named <code>ssd-gce</code> using provisioner
            <code>pd.csi.storage.gke.io</code> with parameters <code>type: pd-ssd</code>, reclaim policy
            <code>Delete</code>, and volume binding mode <code>WaitForFirstConsumer</code>.
          hints:
            - "GKE uses the <code>pd.csi.storage.gke.io</code> CSI driver."
            - "<code>pd-ssd</code> provides SSD-backed persistent disks."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: ssd-gce
            provisioner: pd.csi.storage.gke.io
            parameters:
              type: pd-ssd
            reclaimPolicy: Delete
            volumeBindingMode: WaitForFirstConsumer
        - id: v4
          title: Default StorageClass
          description: >-
            Write a StorageClass YAML named <code>default-sc</code> using provisioner
            <code>ebs.csi.aws.com</code> with parameters <code>type: gp3</code> and reclaim policy
            <code>Delete</code>. Mark it as the cluster default using the appropriate annotation.
          hints:
            - "Add annotation <code>storageclass.kubernetes.io/is-default-class: \"true\"</code>."
            - "Only one StorageClass should be the default in a cluster."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: default-sc
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
            provisioner: ebs.csi.aws.com
            parameters:
              type: gp3
            reclaimPolicy: Delete
        - id: v5
          title: PVC Referencing a StorageClass
          description: >-
            Write a PVC YAML named <code>dynamic-pvc</code> requesting <code>20Gi</code> with access mode
            <code>ReadWriteOnce</code> and <code>storageClassName: fast-ebs</code>. This will trigger dynamic
            provisioning.
          hints:
            - "When a PVC references a StorageClass with a dynamic provisioner, a PV is automatically created."
            - "The PVC does not need a selector when using dynamic provisioning."
          solution: |-
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: dynamic-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: fast-ebs
              resources:
                requests:
                  storage: 20Gi
        - id: v6
          title: StorageClass with Immediate Binding
          description: >-
            Write a StorageClass YAML named <code>immediate-sc</code> using provisioner <code>ebs.csi.aws.com</code>
            with parameters <code>type: gp3</code>, reclaim policy <code>Delete</code>, and volume binding mode
            <code>Immediate</code>.
          hints:
            - "<code>Immediate</code> means volumes are provisioned as soon as the PVC is created."
            - "This can cause issues in multi-zone clusters if the volume is in a different zone than the Pod."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: immediate-sc
            provisioner: ebs.csi.aws.com
            parameters:
              type: gp3
            reclaimPolicy: Delete
            volumeBindingMode: Immediate
        - id: v7
          title: StorageClass with AllowVolumeExpansion
          description: >-
            Write a StorageClass YAML named <code>expandable-sc</code> using provisioner <code>ebs.csi.aws.com</code>
            with parameters <code>type: gp3</code>, reclaim policy <code>Delete</code>, and
            <code>allowVolumeExpansion: true</code>.
          hints:
            - "<code>allowVolumeExpansion: true</code> lets PVCs increase their size after creation."
            - "Not all provisioners support volume expansion."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: expandable-sc
            provisioner: ebs.csi.aws.com
            parameters:
              type: gp3
            reclaimPolicy: Delete
            allowVolumeExpansion: true
        - id: v8
          title: StorageClass with Mount Options
          description: >-
            Write a StorageClass YAML named <code>nfs-sc</code> using provisioner
            <code>nfs.csi.k8s.io</code> with mount options <code>hard</code> and <code>nfsvers=4.1</code>, and
            reclaim policy <code>Delete</code>.
          hints:
            - "Mount options are specified as a list under <code>mountOptions</code>."
            - "These options are passed to the mount command when the volume is attached."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: nfs-sc
            provisioner: nfs.csi.k8s.io
            reclaimPolicy: Delete
            mountOptions:
              - hard
              - nfsvers=4.1
        - id: v9
          title: StorageClass with Retain Policy
          description: >-
            Write a StorageClass YAML named <code>retain-sc</code> using provisioner <code>ebs.csi.aws.com</code>
            with parameters <code>type: gp3</code>, <code>encrypted: "true"</code>, reclaim policy
            <code>Retain</code>, and <code>volumeBindingMode: WaitForFirstConsumer</code>.
          hints:
            - "<code>Retain</code> keeps the underlying volume even after the PVC is deleted."
            - "StorageClass parameters are all strings, so <code>\"true\"</code> needs quotes."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: retain-sc
            provisioner: ebs.csi.aws.com
            parameters:
              type: gp3
              encrypted: "true"
            reclaimPolicy: Retain
            volumeBindingMode: WaitForFirstConsumer
        - id: v10
          title: StorageClass and PVC Combo
          description: >-
            Write a StorageClass named <code>local-path</code> (provisioner
            <code>rancher.io/local-path</code>, reclaimPolicy <code>Delete</code>, volumeBindingMode
            <code>WaitForFirstConsumer</code>) and a PVC named <code>local-pvc</code> requesting <code>5Gi</code>
            with access mode <code>ReadWriteOnce</code> referencing that StorageClass.
          hints:
            - "The PVC's <code>storageClassName</code> must match the StorageClass <code>metadata.name</code>."
            - "<code>rancher.io/local-path</code> is the provisioner used by k3s and Rancher's local-path-provisioner."
          solution: |-
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: local-path
            provisioner: rancher.io/local-path
            reclaimPolicy: Delete
            volumeBindingMode: WaitForFirstConsumer
            ---
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: local-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: local-path
              resources:
                requests:
                  storage: 5Gi
  challenges:
    - id: challenge_1
      block: 1
      difficulty: 2
      concept: PersistentVolumes
      variants:
        - id: v1
          title: "PV-PVC Binding: Capacity Match"
          description: >-
            Given the following PV and PVC, will the PVC bind successfully? Explain why or why not.<br><br>
            <pre>
            # PV
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-a
            spec:
              capacity:
                storage: 5Gi
              accessModes:
                - ReadWriteOnce
              storageClassName: manual
              hostPath:
                path: /mnt/a

            # PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: pvc-a
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: manual
              resources:
                requests:
                  storage: 10Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PVC requests 10Gi, PV offers 5Gi"
              output: "PVC stays Pending. The PV capacity (5Gi) is less than the PVC request (10Gi)."
          hints:
            - title: "\U0001F914 Think about it"
              content: "A PVC can only bind to a PV whose capacity is greater than or equal to the request."
            - title: "\U0001F4A1 Hint"
              content: "Compare <code>spec.capacity.storage</code> on the PV with <code>spec.resources.requests.storage</code> on the PVC."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Binding requires ALL of:
                1. PV capacity >= PVC request
                2. Access modes match
                3. StorageClass matches
                4. PV is Available (not already Bound)</pre>
          solution: |-
            The PVC will NOT bind. The PVC requests 10Gi but the PV only
            has 5Gi capacity. A PVC cannot bind to a PV with less storage
            than requested. The PVC will remain in Pending state.
          difficulty: 1
        - id: v2
          title: "PV-PVC Binding: Access Mode Mismatch"
          description: >-
            Given the following PV and PVC, will the PVC bind successfully?<br><br>
            <pre>
            # PV
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-b
            spec:
              capacity:
                storage: 10Gi
              accessModes:
                - ReadOnlyMany
              storageClassName: manual
              hostPath:
                path: /mnt/b

            # PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: pvc-b
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: manual
              resources:
                requests:
                  storage: 5Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PVC wants ReadWriteOnce, PV only supports ReadOnlyMany"
              output: "PVC stays Pending. The PV does not support the ReadWriteOnce access mode."
          hints:
            - title: "\U0001F914 Think about it"
              content: "The PV's access modes must include all the modes requested by the PVC."
            - title: "\U0001F4A1 Hint"
              content: "ReadOnlyMany and ReadWriteOnce are different access modes. The PV must support what the PVC asks for."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Access mode check:
                PV supports: ReadOnlyMany
                PVC requests: ReadWriteOnce
                ReadWriteOnce is NOT in the PV's list -> no match</pre>
          solution: |-
            The PVC will NOT bind. The PVC requests ReadWriteOnce access mode
            but the PV only supports ReadOnlyMany. The PV's access modes must
            be a superset of the PVC's requested modes. The PVC will remain
            in Pending state.
          difficulty: 1
        - id: v3
          title: "PV-PVC Binding: StorageClass Mismatch"
          description: >-
            Given the following PV and PVC, will the PVC bind successfully?<br><br>
            <pre>
            # PV
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-c
            spec:
              capacity:
                storage: 10Gi
              accessModes:
                - ReadWriteOnce
              storageClassName: fast
              hostPath:
                path: /mnt/c

            # PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: pvc-c
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: slow
              resources:
                requests:
                  storage: 5Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PV storageClassName is 'fast', PVC storageClassName is 'slow'"
              output: "PVC stays Pending. StorageClass names do not match."
          hints:
            - title: "\U0001F914 Think about it"
              content: "StorageClassName must match exactly between PV and PVC for static binding."
            - title: "\U0001F4A1 Hint"
              content: "Compare the <code>storageClassName</code> field on both resources. They must be identical."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>StorageClass matching:
                PV:  storageClassName: fast
                PVC: storageClassName: slow
                "fast" != "slow" -> no binding</pre>
          solution: |-
            The PVC will NOT bind. The PV has storageClassName "fast" but the
            PVC requests storageClassName "slow". For static provisioning,
            the storageClassName must match exactly. The PVC will remain
            in Pending state.
          difficulty: 1
        - id: v4
          title: "PV-PVC Binding: Successful Match"
          description: >-
            Given the following PV and PVC, will the PVC bind successfully? If so, explain the match.<br><br>
            <pre>
            # PV
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-d
            spec:
              capacity:
                storage: 20Gi
              accessModes:
                - ReadWriteOnce
                - ReadOnlyMany
              storageClassName: standard
              hostPath:
                path: /mnt/d

            # PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: pvc-d
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: standard
              resources:
                requests:
                  storage: 10Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PVC requests 10Gi RWO from 'standard', PV offers 20Gi RWO+ROX from 'standard'"
              output: "PVC binds to PV. Capacity (20Gi >= 10Gi), access mode (RWO supported), and storageClassName match."
          hints:
            - title: "\U0001F914 Think about it"
              content: "Check all three binding criteria: capacity, access modes, and storageClassName."
            - title: "\U0001F4A1 Hint"
              content: "The PV can have more capacity and more access modes than the PVC requires. It only needs to satisfy the PVC's minimum requirements."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Binding check:
                1. Capacity: 20Gi >= 10Gi -> OK
                2. Access: PVC wants RWO, PV supports [RWO, ROX] -> OK
                3. StorageClass: both "standard" -> OK
                Result: BIND</pre>
          solution: |-
            Yes, the PVC will bind successfully to the PV.
            - Capacity: PV has 20Gi which is >= PVC request of 10Gi
            - Access mode: PVC requests ReadWriteOnce, PV supports it
            - StorageClass: both are "standard"
            The PVC gets the full 20Gi PV even though it only requested 10Gi.
          difficulty: 2
        - id: v5
          title: "PV-PVC Binding: Multiple PVs"
          description: >-
            Two PVs exist. Which one does the PVC bind to?<br><br>
            <pre>
            # PV-1
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-small
            spec:
              capacity:
                storage: 5Gi
              accessModes:
                - ReadWriteOnce
              storageClassName: manual
              hostPath:
                path: /mnt/small

            # PV-2
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: pv-large
            spec:
              capacity:
                storage: 50Gi
              accessModes:
                - ReadWriteOnce
              storageClassName: manual
              hostPath:
                path: /mnt/large

            # PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: pvc-mid
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: manual
              resources:
                requests:
                  storage: 8Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PVC requests 8Gi; PV-small is 5Gi, PV-large is 50Gi"
              output: "PVC binds to pv-large (50Gi). pv-small (5Gi) is too small. Kubernetes picks the smallest PV that satisfies the request."
          hints:
            - title: "\U0001F914 Think about it"
              content: "When multiple PVs match, Kubernetes selects the one with the smallest capacity that still satisfies the PVC."
            - title: "\U0001F4A1 Hint"
              content: "pv-small has 5Gi which is less than 8Gi, so it cannot satisfy the PVC. Only pv-large qualifies."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>PV selection:
                1. Filter PVs by accessModes, storageClassName
                2. Filter by capacity >= request
                3. Pick smallest qualifying PV
                pv-small: 5Gi < 8Gi -> excluded
                pv-large: 50Gi >= 8Gi -> selected</pre>
          solution: |-
            The PVC binds to pv-large.
            - pv-small (5Gi) cannot satisfy the 8Gi request
            - pv-large (50Gi) satisfies capacity, access mode, and storageClassName
            Kubernetes picks the smallest PV that meets all requirements.
            Since only pv-large qualifies, it is selected. Note that 42Gi
            of the PV's capacity is effectively wasted.
          difficulty: 2
        - id: v6
          title: "PV-PVC Binding: Best Fit Selection"
          description: >-
            Three PVs exist, all with storageClassName <code>standard</code> and access mode
            <code>ReadWriteOnce</code>. Which does the PVC bind to?<br><br>
            <pre>
            pv-10  -> capacity: 10Gi
            pv-20  -> capacity: 20Gi
            pv-100 -> capacity: 100Gi

            # PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: pvc-15
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: standard
              resources:
                requests:
                  storage: 15Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PVC requests 15Gi; PVs are 10Gi, 20Gi, 100Gi"
              output: "PVC binds to pv-20 (20Gi). It is the smallest PV that satisfies the 15Gi request."
          hints:
            - title: "\U0001F914 Think about it"
              content: "Kubernetes tries to minimize wasted capacity by choosing the smallest qualifying PV."
            - title: "\U0001F4A1 Hint"
              content: "10Gi < 15Gi (too small), 20Gi >= 15Gi (qualifies), 100Gi >= 15Gi (qualifies but larger). The best fit is the smallest that works."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Best fit algorithm:
                pv-10:  10Gi < 15Gi -> too small
                pv-20:  20Gi >= 15Gi -> candidate (waste: 5Gi)
                pv-100: 100Gi >= 15Gi -> candidate (waste: 85Gi)
                Winner: pv-20 (least waste)</pre>
          solution: |-
            The PVC binds to pv-20.
            - pv-10 (10Gi) is too small for the 15Gi request
            - pv-20 (20Gi) and pv-100 (100Gi) both qualify
            - Kubernetes selects pv-20 as the best fit because it is the
              smallest PV that satisfies the request, minimizing wasted capacity.
          difficulty: 2
        - id: v7
          title: "PV-PVC Binding: Already Bound PV"
          description: >-
            Two PVs exist with storageClassName <code>manual</code> and ReadWriteOnce. <code>pv-alpha</code>
            (10Gi) is already Bound to another PVC. <code>pv-beta</code> (10Gi) is Available. A new PVC
            requests 5Gi. Which PV does it bind to?<br><br>
            <pre>
            pv-alpha -> 10Gi, status: Bound
            pv-beta  -> 10Gi, status: Available

            # New PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: new-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: manual
              resources:
                requests:
                  storage: 5Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "pv-alpha is Bound, pv-beta is Available, PVC requests 5Gi"
              output: "PVC binds to pv-beta. pv-alpha is already Bound and cannot be used."
          hints:
            - title: "\U0001F914 Think about it"
              content: "A PV in Bound state is already claimed by another PVC. Only Available PVs can accept new bindings."
            - title: "\U0001F4A1 Hint"
              content: "PV status is one of: Available, Bound, Released, Failed. Only Available PVs participate in binding."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>PV lifecycle states:
                Available -> can be bound to a PVC
                Bound     -> already claimed (skip)
                Released  -> claim deleted, needs reclaim
                Failed    -> automatic reclamation failed</pre>
          solution: |-
            The PVC binds to pv-beta.
            - pv-alpha is already in Bound state and cannot accept new claims
            - pv-beta is Available with 10Gi >= 5Gi, matching access mode and
              storageClassName
            A PV can only be bound to one PVC at a time (one-to-one relationship).
          difficulty: 3
        - id: v8
          title: "PV-PVC Binding: Write the Missing PV"
          description: >-
            A PVC is stuck in Pending state. Write a PersistentVolume that will satisfy it.<br><br>
            <pre>
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: db-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: database
              resources:
                requests:
                  storage: 25Gi
            </pre>
          functionSignature: PersistentVolume
          testCases:
            - input: "PVC needs 25Gi, ReadWriteOnce, storageClassName: database"
              output: "A PV with capacity >= 25Gi, ReadWriteOnce, storageClassName: database"
          hints:
            - title: "\U0001F914 Think about it"
              content: "The PV must match or exceed every requirement of the PVC: capacity, access modes, and storageClassName."
            - title: "\U0001F4A1 Hint"
              content: "Create a PV with at least 25Gi, include ReadWriteOnce in accessModes, and set storageClassName to \"database\"."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>To satisfy a PVC, the PV needs:
                - capacity.storage >= 25Gi
                - accessModes includes ReadWriteOnce
                - storageClassName: database
                - status: Available (not bound)</pre>
          solution: |-
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: db-pv
            spec:
              capacity:
                storage: 25Gi
              accessModes:
                - ReadWriteOnce
              persistentVolumeReclaimPolicy: Retain
              storageClassName: database
              hostPath:
                path: /mnt/database
          difficulty: 3
    - id: challenge_2
      block: 2
      difficulty: 3
      concept: StatefulSet Storage
      variants:
        - id: v1
          title: "StatefulSet with volumeClaimTemplates: Basic"
          description: >-
            Write a StatefulSet YAML named <code>web</code> with 3 replicas running <code>nginx:1.25</code>.
            Each replica should get its own <code>1Gi</code> PVC via <code>volumeClaimTemplates</code> with access
            mode <code>ReadWriteOnce</code> and storageClassName <code>standard</code>. Mount the volume at
            <code>/usr/share/nginx/html</code>. Use serviceName <code>web-svc</code>.
          functionSignature: StatefulSet
          testCases:
            - input: "3 replicas, 1Gi each, storageClassName: standard"
              output: "PVCs: web-data-web-0, web-data-web-1, web-data-web-2 (each 1Gi)"
          hints:
            - title: "\U0001F914 Think about it"
              content: "StatefulSets create a PVC per replica using the template. The PVC name is <code>{volumeClaimTemplate.name}-{statefulset.name}-{ordinal}</code>."
            - title: "\U0001F4A1 Hint"
              content: "The <code>volumeClaimTemplates</code> field is at the same level as <code>spec.template</code>, not inside it. The volume mount name must match the template metadata name."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>StatefulSet structure:
                spec:
                  serviceName: ...
                  replicas: N
                  template:
                    spec:
                      containers:
                        - volumeMounts:
                            - name: <matches-template-name>
                  volumeClaimTemplates:
                    - metadata:
                        name: <template-name>
                      spec: ...</pre>
          solution: |-
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: web
            spec:
              serviceName: web-svc
              replicas: 3
              selector:
                matchLabels:
                  app: web
              template:
                metadata:
                  labels:
                    app: web
                spec:
                  containers:
                    - name: nginx
                      image: nginx:1.25
                      volumeMounts:
                        - name: web-data
                          mountPath: /usr/share/nginx/html
              volumeClaimTemplates:
                - metadata:
                    name: web-data
                  spec:
                    accessModes:
                      - ReadWriteOnce
                    storageClassName: standard
                    resources:
                      requests:
                        storage: 1Gi
          difficulty: 3
        - id: v2
          title: "StatefulSet: Database Cluster"
          description: >-
            Write a StatefulSet YAML named <code>postgres</code> with 3 replicas running
            <code>postgres:16</code>. Each replica gets a <code>10Gi</code> PVC (storageClassName
            <code>fast-ssd</code>, ReadWriteOnce) mounted at <code>/var/lib/postgresql/data</code>. Set
            serviceName to <code>postgres-svc</code> and environment variable <code>PGDATA</code> to
            <code>/var/lib/postgresql/data/pgdata</code>.
          functionSignature: StatefulSet
          testCases:
            - input: "3 Postgres replicas, 10Gi SSD each"
              output: "PVCs: pgdata-postgres-0, pgdata-postgres-1, pgdata-postgres-2 (each 10Gi fast-ssd)"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Databases need stable storage. StatefulSets guarantee that each replica keeps its own persistent volume across restarts."
            - title: "\U0001F4A1 Hint"
              content: "PostgreSQL uses <code>PGDATA</code> to specify the data subdirectory. Set it as an environment variable. The volume mount is the parent directory."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Database StatefulSet checklist:
                - serviceName for stable DNS
                - volumeClaimTemplates for per-replica storage
                - PGDATA env var pointing to a subdirectory
                - Appropriate storageClassName for performance</pre>
          solution: |-
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: postgres
            spec:
              serviceName: postgres-svc
              replicas: 3
              selector:
                matchLabels:
                  app: postgres
              template:
                metadata:
                  labels:
                    app: postgres
                spec:
                  containers:
                    - name: postgres
                      image: postgres:16
                      env:
                        - name: PGDATA
                          value: /var/lib/postgresql/data/pgdata
                      volumeMounts:
                        - name: pgdata
                          mountPath: /var/lib/postgresql/data
              volumeClaimTemplates:
                - metadata:
                    name: pgdata
                  spec:
                    accessModes:
                      - ReadWriteOnce
                    storageClassName: fast-ssd
                    resources:
                      requests:
                        storage: 10Gi
          difficulty: 3
        - id: v3
          title: "StatefulSet: Redis Cluster"
          description: >-
            Write a StatefulSet YAML named <code>redis</code> with 6 replicas running <code>redis:7</code>. Each
            replica gets a <code>2Gi</code> PVC (storageClassName <code>standard</code>, ReadWriteOnce) mounted at
            <code>/data</code>. Use serviceName <code>redis-svc</code>. Add command
            <code>["redis-server", "--appendonly", "yes"]</code> for AOF persistence.
          functionSignature: StatefulSet
          testCases:
            - input: "6 Redis replicas with AOF persistence, 2Gi each"
              output: "PVCs: data-redis-0 through data-redis-5, each 2Gi with appendonly enabled"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Redis with AOF persistence writes an append-only file to disk. Each replica needs its own volume to avoid data corruption."
            - title: "\U0001F4A1 Hint"
              content: "Use <code>command</code> in the container spec to pass Redis CLI flags. The volume template name becomes part of the PVC name."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Redis cluster StatefulSet:
                - 6 replicas (3 masters + 3 replicas typical)
                - AOF enabled via command args
                - Per-replica persistent storage
                - Headless service for peer discovery</pre>
          solution: |-
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: redis
            spec:
              serviceName: redis-svc
              replicas: 6
              selector:
                matchLabels:
                  app: redis
              template:
                metadata:
                  labels:
                    app: redis
                spec:
                  containers:
                    - name: redis
                      image: redis:7
                      command: ["redis-server", "--appendonly", "yes"]
                      volumeMounts:
                        - name: data
                          mountPath: /data
              volumeClaimTemplates:
                - metadata:
                    name: data
                  spec:
                    accessModes:
                      - ReadWriteOnce
                    storageClassName: standard
                    resources:
                      requests:
                        storage: 2Gi
          difficulty: 3
        - id: v4
          title: "StatefulSet: Multiple Volume Templates"
          description: >-
            Write a StatefulSet YAML named <code>elastic</code> with 3 replicas running
            <code>elasticsearch:8</code>. Each replica gets two PVCs: <code>data</code> (50Gi, fast-ssd) mounted
            at <code>/usr/share/elasticsearch/data</code> and <code>logs</code> (5Gi, standard) mounted at
            <code>/usr/share/elasticsearch/logs</code>. Use serviceName <code>elastic-svc</code>.
          functionSignature: StatefulSet
          testCases:
            - input: "3 replicas, 2 volume templates: data (50Gi) and logs (5Gi)"
              output: "6 PVCs total: data-elastic-{0,1,2} (50Gi) and logs-elastic-{0,1,2} (5Gi)"
          hints:
            - title: "\U0001F914 Think about it"
              content: "A StatefulSet can have multiple <code>volumeClaimTemplates</code> entries, each creating a separate PVC per replica."
            - title: "\U0001F4A1 Hint"
              content: "List two entries under <code>volumeClaimTemplates</code>. Each has its own metadata.name, which is referenced in the container's volumeMounts."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Multiple volume templates:
                volumeClaimTemplates:
                  - metadata: { name: data }
                    spec: { ... 50Gi, fast-ssd }
                  - metadata: { name: logs }
                    spec: { ... 5Gi, standard }
                Each replica gets both PVCs.</pre>
          solution: |-
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: elastic
            spec:
              serviceName: elastic-svc
              replicas: 3
              selector:
                matchLabels:
                  app: elastic
              template:
                metadata:
                  labels:
                    app: elastic
                spec:
                  containers:
                    - name: elasticsearch
                      image: elasticsearch:8
                      volumeMounts:
                        - name: data
                          mountPath: /usr/share/elasticsearch/data
                        - name: logs
                          mountPath: /usr/share/elasticsearch/logs
              volumeClaimTemplates:
                - metadata:
                    name: data
                  spec:
                    accessModes:
                      - ReadWriteOnce
                    storageClassName: fast-ssd
                    resources:
                      requests:
                        storage: 50Gi
                - metadata:
                    name: logs
                  spec:
                    accessModes:
                      - ReadWriteOnce
                    storageClassName: standard
                    resources:
                      requests:
                        storage: 5Gi
          difficulty: 4
        - id: v5
          title: "StatefulSet: Kafka Brokers"
          description: >-
            Write a StatefulSet YAML named <code>kafka</code> with 3 replicas running
            <code>confluentinc/cp-kafka:7.5.0</code>. Each replica gets a <code>100Gi</code> PVC (storageClassName
            <code>fast-ssd</code>, ReadWriteOnce) mounted at <code>/var/lib/kafka/data</code>. Use serviceName
            <code>kafka-headless</code>.
          functionSignature: StatefulSet
          testCases:
            - input: "3 Kafka brokers, 100Gi SSD each"
              output: "PVCs: kafka-data-kafka-0, kafka-data-kafka-1, kafka-data-kafka-2 (each 100Gi fast-ssd)"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Kafka brokers store log segments on disk. Each broker needs its own dedicated, high-performance storage."
            - title: "\U0001F4A1 Hint"
              content: "Kafka is a classic StatefulSet use case: stable network identity + stable persistent storage per broker."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Kafka StatefulSet:
                - Headless service for broker discovery
                - Per-broker persistent storage
                - SSD for write-heavy workloads
                - volumeClaimTemplates for automatic PVC creation</pre>
          solution: |-
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: kafka
            spec:
              serviceName: kafka-headless
              replicas: 3
              selector:
                matchLabels:
                  app: kafka
              template:
                metadata:
                  labels:
                    app: kafka
                spec:
                  containers:
                    - name: kafka
                      image: confluentinc/cp-kafka:7.5.0
                      volumeMounts:
                        - name: kafka-data
                          mountPath: /var/lib/kafka/data
              volumeClaimTemplates:
                - metadata:
                    name: kafka-data
                  spec:
                    accessModes:
                      - ReadWriteOnce
                    storageClassName: fast-ssd
                    resources:
                      requests:
                        storage: 100Gi
          difficulty: 3
        - id: v6
          title: "StatefulSet: MySQL with Init Container"
          description: >-
            Write a StatefulSet YAML named <code>mysql</code> with 3 replicas running <code>mysql:8.0</code>.
            Include an init container <code>init-mysql</code> (image <code>busybox</code>) that shares the data
            volume. Each replica gets a <code>20Gi</code> PVC (storageClassName <code>standard</code>,
            ReadWriteOnce) mounted at <code>/var/lib/mysql</code> in both the init container and main container.
            Use serviceName <code>mysql-svc</code> and set env <code>MYSQL_ALLOW_EMPTY_PASSWORD: "yes"</code>.
          functionSignature: StatefulSet
          testCases:
            - input: "3 MySQL replicas with init container, 20Gi each"
              output: "PVCs: mysql-data-mysql-{0,1,2}, init container and main container share the volume"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Init containers in a StatefulSet can access the same volumeClaimTemplate volumes. This is useful for initialization tasks."
            - title: "\U0001F4A1 Hint"
              content: "List the init container under <code>spec.template.spec.initContainers</code> with the same volumeMount name as the main container."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Init container + StatefulSet:
                initContainers:
                  - name: init-mysql
                    volumeMounts:
                      - name: mysql-data  # same name
                containers:
                  - name: mysql
                    volumeMounts:
                      - name: mysql-data  # same name
                volumeClaimTemplates:
                  - metadata: { name: mysql-data }</pre>
          solution: |-
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: mysql
            spec:
              serviceName: mysql-svc
              replicas: 3
              selector:
                matchLabels:
                  app: mysql
              template:
                metadata:
                  labels:
                    app: mysql
                spec:
                  initContainers:
                    - name: init-mysql
                      image: busybox
                      command: ["sh", "-c", "echo initializing"]
                      volumeMounts:
                        - name: mysql-data
                          mountPath: /var/lib/mysql
                  containers:
                    - name: mysql
                      image: mysql:8.0
                      env:
                        - name: MYSQL_ALLOW_EMPTY_PASSWORD
                          value: "yes"
                      volumeMounts:
                        - name: mysql-data
                          mountPath: /var/lib/mysql
              volumeClaimTemplates:
                - metadata:
                    name: mysql-data
                  spec:
                    accessModes:
                      - ReadWriteOnce
                    storageClassName: standard
                    resources:
                      requests:
                        storage: 20Gi
          difficulty: 4
    - id: challenge_3
      block: 2
      difficulty: 2
      concept: Dynamic Provisioning
      variants:
        - id: v1
          title: "Troubleshooting: No Matching PV"
          description: >-
            A PVC is stuck in Pending. No StorageClass exists and no PVs have been created. The PVC
            has <code>storageClassName: ""</code>. What is wrong and how do you fix it?<br><br>
            <pre>
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: stuck-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: ""
              resources:
                requests:
                  storage: 5Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PVC with storageClassName: '', no PVs exist"
              output: "Pending: empty storageClassName disables dynamic provisioning. Must create a PV manually with no storageClassName."
          hints:
            - title: "\U0001F914 Think about it"
              content: "An empty string for storageClassName is different from omitting it. It means 'do not use any StorageClass'."
            - title: "\U0001F4A1 Hint"
              content: "With <code>storageClassName: \"\"</code>, dynamic provisioning is disabled. The PVC will only bind to PVs that also have no storageClassName."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Fix options:
                1. Create a PV with no storageClassName that matches
                2. Remove storageClassName: "" to use the default SC
                3. Set storageClassName to an existing SC name</pre>
          solution: |-
            The PVC is Pending because storageClassName: "" explicitly opts out
            of dynamic provisioning, and no PVs exist without a storageClassName.

            Fix: Create a PV with matching capacity, access mode, and no
            storageClassName:

            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: manual-pv
            spec:
              capacity:
                storage: 5Gi
              accessModes:
                - ReadWriteOnce
              hostPath:
                path: /mnt/data
          difficulty: 2
        - id: v2
          title: "Troubleshooting: Wrong Access Mode"
          description: >-
            A PVC is stuck in Pending. One PV exists. Identify the problem.<br><br>
            <pre>
            # PV
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: nfs-pv
            spec:
              capacity:
                storage: 50Gi
              accessModes:
                - ReadWriteMany
              storageClassName: nfs
              nfs:
                server: nfs.example.com
                path: /exports/data

            # PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: app-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: nfs
              resources:
                requests:
                  storage: 10Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PV supports ReadWriteMany, PVC requests ReadWriteOnce"
              output: "Pending: access mode mismatch. PV has RWX but PVC requests RWO."
          hints:
            - title: "\U0001F914 Think about it"
              content: "ReadWriteMany and ReadWriteOnce are distinct access modes. A PV must support the exact mode the PVC requests."
            - title: "\U0001F4A1 Hint"
              content: "Even though RWX seems 'more permissive' than RWO, Kubernetes requires an exact access mode match. The PV must list ReadWriteOnce explicitly."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Fix options:
                1. Add ReadWriteOnce to the PV's accessModes list
                2. Change PVC to request ReadWriteMany
                Which fix depends on the application's actual needs.</pre>
          solution: |-
            The PVC is Pending due to access mode mismatch. The PV only
            supports ReadWriteMany but the PVC requests ReadWriteOnce.

            Fix option 1 - Add RWO to PV:
              accessModes:
                - ReadWriteMany
                - ReadWriteOnce

            Fix option 2 - Change PVC to RWX:
              accessModes:
                - ReadWriteMany
          difficulty: 2
        - id: v3
          title: "Troubleshooting: Wrong StorageClass"
          description: >-
            A PVC is stuck in Pending. A PV exists with plenty of capacity. What is wrong?<br><br>
            <pre>
            # PV
            apiVersion: v1
            kind: PersistentVolume
            metadata:
              name: big-pv
            spec:
              capacity:
                storage: 100Gi
              accessModes:
                - ReadWriteOnce
              storageClassName: premium
              hostPath:
                path: /mnt/premium

            # PVC
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: app-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: standard
              resources:
                requests:
                  storage: 10Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PV storageClassName: premium, PVC storageClassName: standard"
              output: "Pending: storageClassName mismatch. PV is 'premium' but PVC requests 'standard'."
          hints:
            - title: "\U0001F914 Think about it"
              content: "StorageClassName must match exactly between PV and PVC for static binding."
            - title: "\U0001F4A1 Hint"
              content: "The PV has storageClassName 'premium' but the PVC requests 'standard'. These are different strings."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Fix options:
                1. Change PV storageClassName to "standard"
                2. Change PVC storageClassName to "premium"
                3. Create a new PV with storageClassName "standard"</pre>
          solution: |-
            The PVC is Pending because of a storageClassName mismatch.
            The PV has storageClassName "premium" but the PVC requests
            "standard".

            Fix: Either change the PV's storageClassName to "standard" or
            change the PVC's storageClassName to "premium", depending on
            which StorageClass is correct for the use case.
          difficulty: 2
        - id: v4
          title: "Troubleshooting: PV in Released State"
          description: >-
            A PVC is stuck in Pending. A PV with matching capacity, access mode, and storageClassName exists but
            its status is <code>Released</code>. What happened and how do you fix it?<br><br>
            <pre>
            kubectl get pv
            NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     STORAGECLASS
            old-pv   10Gi       RWO            Retain           Released   manual
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PV status is Released with Retain policy"
              output: "Pending: Released PV cannot accept new bindings. Must clear the claimRef or create a new PV."
          hints:
            - title: "\U0001F914 Think about it"
              content: "A PV in Released state had its previous PVC deleted. With Retain policy, the PV keeps its data but won't accept new claims automatically."
            - title: "\U0001F4A1 Hint"
              content: "To reuse a Released PV, you must clear its <code>spec.claimRef</code> field. This returns it to Available state."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Fix a Released PV:
                kubectl patch pv old-pv -p '{"spec":{"claimRef": null}}'
                This clears the old claim reference and sets
                status back to Available.</pre>
          solution: |-
            The PVC is Pending because the PV is in Released state. A Released
            PV retains data from its previous claim but won't accept new PVCs
            until the claimRef is cleared.

            Fix: Clear the claimRef to make the PV Available again:
            kubectl patch pv old-pv -p '{"spec":{"claimRef": null}}'

            Warning: The PV still contains data from the previous claim.
            Verify this is acceptable before binding a new PVC.
          difficulty: 3
        - id: v5
          title: "Troubleshooting: Missing StorageClass for Dynamic Provisioning"
          description: >-
            A PVC references storageClassName <code>fast-ssd</code> but no StorageClass with that name exists in the
            cluster. The PVC is Pending. Write the StorageClass that would fix it.<br><br>
            <pre>
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: dynamic-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: fast-ssd
              resources:
                requests:
                  storage: 20Gi
            </pre>
          functionSignature: StorageClass
          testCases:
            - input: "PVC references non-existent storageClassName: fast-ssd"
              output: "Create a StorageClass named fast-ssd with a valid provisioner"
          hints:
            - title: "\U0001F914 Think about it"
              content: "For dynamic provisioning, the referenced StorageClass must exist and have a valid provisioner."
            - title: "\U0001F4A1 Hint"
              content: "Create a StorageClass with <code>metadata.name: fast-ssd</code> and an appropriate provisioner for your cluster."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Dynamic provisioning requires:
                1. StorageClass exists with matching name
                2. Provisioner is installed and running
                3. Provisioner can create volumes of requested size</pre>
          solution: |-
            The PVC is Pending because no StorageClass named "fast-ssd" exists.

            Fix: Create the StorageClass:

            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: fast-ssd
            provisioner: ebs.csi.aws.com
            parameters:
              type: gp3
            reclaimPolicy: Delete
            volumeBindingMode: WaitForFirstConsumer
            allowVolumeExpansion: true
          difficulty: 2
        - id: v6
          title: "Troubleshooting: Capacity Too Large"
          description: >-
            A developer's PVC is Pending. Two PVs exist. Diagnose the issue.<br><br>
            <pre>
            kubectl get pv
            NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      STORAGECLASS
            pv-a    5Gi        RWO            Retain           Available   app
            pv-b    10Gi       RWO            Retain           Available   app

            kubectl get pvc
            NAME      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS
            big-pvc   Pending                                       app

            # PVC spec
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: big-pvc
            spec:
              accessModes:
                - ReadWriteOnce
              storageClassName: app
              resources:
                requests:
                  storage: 50Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "PVC requests 50Gi, available PVs are 5Gi and 10Gi"
              output: "Pending: no PV has enough capacity. Both PVs are smaller than the 50Gi request."
          hints:
            - title: "\U0001F914 Think about it"
              content: "The PVC can only bind to a PV whose capacity is greater than or equal to the request."
            - title: "\U0001F4A1 Hint"
              content: "50Gi is larger than both 5Gi and 10Gi. Either reduce the PVC request or create a larger PV."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Fix options:
                1. Create a PV with >= 50Gi capacity
                2. Reduce PVC request to <= 10Gi
                3. Set up dynamic provisioning for the 'app' StorageClass</pre>
          solution: |-
            The PVC is Pending because no PV has sufficient capacity.
            The PVC requests 50Gi but the largest available PV is only 10Gi.

            Fix options:
            1. Create a PV with at least 50Gi:
               spec:
                 capacity:
                   storage: 50Gi
                 storageClassName: app

            2. Reduce the PVC request if the application can use less storage.

            3. Configure dynamic provisioning for the "app" StorageClass.
          difficulty: 2
        - id: v7
          title: "Troubleshooting: Namespace Mismatch"
          description: >-
            A Pod cannot start because its PVC is not found. The PVC exists but in a different namespace.
            What is wrong?<br><br>
            <pre>
            # Pod in namespace "production"
            apiVersion: v1
            kind: Pod
            metadata:
              name: app-pod
              namespace: production
            spec:
              containers:
                - name: app
                  image: nginx
                  volumeMounts:
                    - name: data
                      mountPath: /data
              volumes:
                - name: data
                  persistentVolumeClaim:
                    claimName: app-pvc

            # PVC in namespace "default"
            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: app-pvc
              namespace: default
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 5Gi
            </pre>
          functionSignature: PersistentVolumeClaim
          testCases:
            - input: "Pod in 'production' references PVC in 'default'"
              output: "Pod fails: PVCs are namespaced. A Pod can only use PVCs in its own namespace."
          hints:
            - title: "\U0001F914 Think about it"
              content: "PVCs are namespaced resources. A Pod can only reference PVCs within the same namespace."
            - title: "\U0001F4A1 Hint"
              content: "The Pod is in namespace 'production' but the PVC is in namespace 'default'. Create the PVC in 'production' or move the Pod."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Namespace rules for storage:
                - PVs are cluster-scoped (no namespace)
                - PVCs are namespaced
                - Pods reference PVCs in SAME namespace
                - Cross-namespace PVC access is NOT possible</pre>
          solution: |-
            The Pod cannot find the PVC because they are in different namespaces.
            The Pod is in "production" but the PVC is in "default".
            PVCs are namespaced resources and a Pod can only use PVCs
            in its own namespace.

            Fix: Create the PVC in the "production" namespace:

            apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: app-pvc
              namespace: production
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 5Gi
          difficulty: 3
