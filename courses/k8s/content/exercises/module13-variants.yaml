conceptLinks:
  Liveness Probes: "#liveness-probes"
  Readiness Probes: "#readiness-probes"
  Startup Probes: "#startup-probes"
  Probe Mechanisms: "#probe-mechanisms"
  Probe Parameters: "#probe-parameters-explained"
  Logging: "#container-logging"
  Metrics Server: "#metrics-server"
  Events: "#events"
  Probe Best Practices: "#probe-configuration-best-practices"
sharedContent: {}
variants:
  warmups:
    - id: warmup_1
      concept: Liveness Probes
      variants:
        - id: v1
          title: HTTP GET Liveness Probe
          description: >-
            Write a <code>livenessProbe</code> that sends an HTTP GET request to <code>/healthz</code> on port
            <code>8080</code>. Wait 10 seconds before the first check, then check every 15 seconds.
          hints:
            - "Use <code>httpGet</code> with <code>path</code> and <code>port</code> fields."
            - "<code>initialDelaySeconds</code> controls the wait before the first probe."
            - "<code>periodSeconds</code> controls how often the probe runs."
          solution: |-
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8080
              initialDelaySeconds: 10
              periodSeconds: 15
        - id: v2
          title: TCP Socket Liveness Probe
          description: >-
            Write a <code>livenessProbe</code> that checks if a TCP connection can be established on port
            <code>5432</code>. Wait 15 seconds before the first check, then check every 20 seconds.
          hints:
            - "Use <code>tcpSocket</code> with a <code>port</code> field."
            - "TCP probes succeed if the connection is established, fail if refused or timed out."
          solution: |-
            livenessProbe:
              tcpSocket:
                port: 5432
              initialDelaySeconds: 15
              periodSeconds: 20
        - id: v3
          title: Exec Command Liveness Probe
          description: >-
            Write a <code>livenessProbe</code> that runs <code>cat /tmp/healthy</code> inside the container.
            Check every 5 seconds with a 5-second initial delay.
          hints:
            - "Use <code>exec</code> with a <code>command</code> list."
            - "Exit code 0 means success, non-zero means failure."
          solution: |-
            livenessProbe:
              exec:
                command:
                - cat
                - /tmp/healthy
              initialDelaySeconds: 5
              periodSeconds: 5
        - id: v4
          title: gRPC Liveness Probe
          description: >-
            Write a <code>livenessProbe</code> for a gRPC service running on port <code>50051</code>. Set the
            initial delay to 10 seconds and the period to 10 seconds.
          hints:
            - "Use <code>grpc</code> with a <code>port</code> field."
            - "The container must implement the gRPC Health Checking Protocol."
          solution: |-
            livenessProbe:
              grpc:
                port: 50051
              initialDelaySeconds: 10
              periodSeconds: 10
        - id: v5
          title: Liveness Probe with Failure Threshold
          description: >-
            Write a <code>livenessProbe</code> using HTTP GET to <code>/health</code> on port <code>3000</code>.
            The container should be restarted only after 5 consecutive failures. Check every 10 seconds with a
            3-second timeout.
          hints:
            - "<code>failureThreshold</code> controls how many consecutive failures trigger a restart."
            - "<code>timeoutSeconds</code> sets how long the probe waits for a response."
          solution: |-
            livenessProbe:
              httpGet:
                path: /health
                port: 3000
              periodSeconds: 10
              timeoutSeconds: 3
              failureThreshold: 5
        - id: v6
          title: Liveness Probe with Custom HTTP Header
          description: >-
            Write a <code>livenessProbe</code> that sends an HTTP GET to <code>/healthz</code> on port
            <code>8080</code> with a custom header <code>X-Probe-Type: liveness</code>. Check every 10 seconds.
          hints:
            - "Use <code>httpHeaders</code> inside <code>httpGet</code> to add custom headers."
            - "Each header needs a <code>name</code> and <code>value</code>."
          solution: |-
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8080
                httpHeaders:
                - name: X-Probe-Type
                  value: liveness
              periodSeconds: 10
        - id: v7
          title: Liveness Probe on Named Port
          description: >-
            Write a container spec with a named port <code>http</code> on <code>8080</code>, and a
            <code>livenessProbe</code> that references the port by name instead of number.
          hints:
            - "Define <code>containerPort</code> with a <code>name</code> field."
            - "In the probe, use the port name string instead of the number."
          solution: |-
            ports:
            - name: http
              containerPort: 8080
            livenessProbe:
              httpGet:
                path: /healthz
                port: http
              periodSeconds: 10
        - id: v8
          title: Liveness Exec with Script Check
          description: >-
            Write a <code>livenessProbe</code> using exec that runs a shell command to check if a PID file
            exists and the process is still running: <code>sh -c "kill -0 $(cat /var/run/app.pid)"</code>.
            Check every 10 seconds, timeout after 5 seconds, fail after 3 attempts.
          hints:
            - "Use <code>exec.command</code> to run a shell command with <code>sh -c</code>."
            - "<code>kill -0</code> checks if a process exists without actually sending a signal."
          solution: |-
            livenessProbe:
              exec:
                command:
                - sh
                - -c
                - "kill -0 $(cat /var/run/app.pid)"
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
        - id: v9
          title: Liveness Probe with All Parameters
          description: >-
            Write a <code>livenessProbe</code> using HTTP GET to <code>/livez</code> on port <code>8080</code>
            with ALL probe parameters set: initial delay 20s, period 15s, timeout 5s, failure threshold 4,
            success threshold 1.
          hints:
            - "All five timing parameters are: <code>initialDelaySeconds</code>, <code>periodSeconds</code>, <code>timeoutSeconds</code>, <code>failureThreshold</code>, <code>successThreshold</code>."
            - "For liveness probes, <code>successThreshold</code> must be 1."
          solution: |-
            livenessProbe:
              httpGet:
                path: /livez
                port: 8080
              initialDelaySeconds: 20
              periodSeconds: 15
              timeoutSeconds: 5
              failureThreshold: 4
              successThreshold: 1
        - id: v10
          title: Liveness TCP Probe for Redis
          description: >-
            Write a <code>livenessProbe</code> for a Redis container running on port <code>6379</code> using a
            TCP socket check. Wait 10 seconds before the first check, then check every 15 seconds with a
            2-second timeout.
          hints:
            - "Redis doesn't expose an HTTP endpoint by default, so TCP is appropriate."
            - "Use <code>tcpSocket</code> with port <code>6379</code>."
          solution: |-
            livenessProbe:
              tcpSocket:
                port: 6379
              initialDelaySeconds: 10
              periodSeconds: 15
              timeoutSeconds: 2
        - id: v11
          title: Liveness Exec with Redis PING
          description: >-
            Write a <code>livenessProbe</code> for a Redis container that uses exec to run
            <code>redis-cli ping</code>. Redis responds with <code>PONG</code> (exit code 0) when healthy.
            Check every 10 seconds with a 5-second timeout.
          hints:
            - "Use <code>exec.command</code> to run <code>redis-cli</code> followed by <code>ping</code>."
            - "<code>redis-cli ping</code> returns exit code 0 on success."
          solution: |-
            livenessProbe:
              exec:
                command:
                - redis-cli
                - ping
              periodSeconds: 10
              timeoutSeconds: 5
        - id: v12
          title: Liveness HTTP Probe on HTTPS Port
          description: >-
            Write a <code>livenessProbe</code> using an HTTP GET to <code>/healthz</code> on port
            <code>8443</code> with the <code>scheme</code> set to <code>HTTPS</code>. Check every 10 seconds
            with a 3-second timeout.
          hints:
            - "Add a <code>scheme</code> field inside <code>httpGet</code>."
            - "Valid schemes are <code>HTTP</code> (default) and <code>HTTPS</code>."
          solution: |-
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8443
                scheme: HTTPS
              periodSeconds: 10
              timeoutSeconds: 3
        - id: v13
          title: Liveness Probe for Database with Exec
          description: >-
            Write a <code>livenessProbe</code> for a PostgreSQL container that runs
            <code>pg_isready -U postgres</code> to check if the database is accepting connections.
            Wait 30 seconds before the first check, then check every 10 seconds.
          hints:
            - "<code>pg_isready</code> is a PostgreSQL utility that returns exit code 0 if the server is accepting connections."
            - "Use <code>exec.command</code> with the command and flags as separate list items."
          solution: |-
            livenessProbe:
              exec:
                command:
                - pg_isready
                - -U
                - postgres
              initialDelaySeconds: 30
              periodSeconds: 10
    - id: warmup_2
      concept: Readiness Probes
      variants:
        - id: v1
          title: HTTP GET Readiness Probe
          description: >-
            Write a <code>readinessProbe</code> that sends an HTTP GET to <code>/ready</code> on port
            <code>8080</code>. Check every 5 seconds with an initial delay of 5 seconds.
          hints:
            - "Readiness probes use the same syntax as liveness probes, just under <code>readinessProbe</code>."
            - "Unlike liveness, readiness failure removes the Pod from Service endpoints instead of restarting it."
          solution: |-
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              initialDelaySeconds: 5
              periodSeconds: 5
        - id: v2
          title: Readiness Probe with Failure Threshold
          description: >-
            Write a <code>readinessProbe</code> that checks <code>/ready</code> on port <code>8080</code>
            every 5 seconds. Remove the Pod from endpoints after 3 consecutive failures, and add it back after
            2 consecutive successes.
          hints:
            - "<code>failureThreshold</code> controls how many failures before the Pod is removed from endpoints."
            - "<code>successThreshold</code> can be greater than 1 for readiness probes (unlike liveness probes)."
          solution: |-
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              periodSeconds: 5
              failureThreshold: 3
              successThreshold: 2
        - id: v3
          title: TCP Socket Readiness Probe
          description: >-
            Write a <code>readinessProbe</code> that checks if port <code>3306</code> (MySQL) is accepting
            connections. Check every 10 seconds with a 5-second timeout.
          hints:
            - "Use <code>tcpSocket</code> with the MySQL port."
            - "TCP readiness probes are common for databases and message brokers."
          solution: |-
            readinessProbe:
              tcpSocket:
                port: 3306
              periodSeconds: 10
              timeoutSeconds: 5
        - id: v4
          title: Readiness vs Liveness - Separate Endpoints
          description: >-
            Write both a <code>readinessProbe</code> checking <code>/ready</code> and a
            <code>livenessProbe</code> checking <code>/livez</code>, both on port <code>8080</code>.
            Readiness checks every 5s, liveness checks every 10s.
          hints:
            - "Use different endpoints: <code>/ready</code> for readiness (can check dependencies), <code>/livez</code> for liveness (process-only check)."
            - "Both probes are defined at the same level inside the container spec."
          solution: |-
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              periodSeconds: 5
            livenessProbe:
              httpGet:
                path: /livez
                port: 8080
              periodSeconds: 10
        - id: v5
          title: Exec Readiness Probe
          description: >-
            Write a <code>readinessProbe</code> that runs <code>sh -c "test -f /app/cache-warmed"</code> to
            check if a cache warming step has completed. Check every 5 seconds.
          hints:
            - "The <code>test -f</code> command returns exit code 0 if the file exists."
            - "This pattern is useful for apps that need to warm caches before accepting traffic."
          solution: |-
            readinessProbe:
              exec:
                command:
                - sh
                - -c
                - "test -f /app/cache-warmed"
              periodSeconds: 5
        - id: v6
          title: Readiness Probe with Short Timeout
          description: >-
            Write a <code>readinessProbe</code> that checks <code>/health/ready</code> on port
            <code>9090</code>. Use a 1-second timeout so slow responses also count as not ready. Check every
            3 seconds with a failure threshold of 2.
          hints:
            - "<code>timeoutSeconds</code> means the probe counts as failed if no response within that time."
            - "Aggressive readiness timeouts are safer than aggressive liveness timeouts because they don't cause restarts."
          solution: |-
            readinessProbe:
              httpGet:
                path: /health/ready
                port: 9090
              periodSeconds: 3
              timeoutSeconds: 1
              failureThreshold: 2
        - id: v7
          title: gRPC Readiness Probe
          description: >-
            Write a <code>readinessProbe</code> for a gRPC service on port <code>50051</code> with service
            name <code>myapp.HealthCheck</code>. Check every 5 seconds.
          hints:
            - "Use <code>grpc</code> with <code>port</code> and optional <code>service</code> fields."
            - "The <code>service</code> field specifies which gRPC service to health-check."
          solution: |-
            readinessProbe:
              grpc:
                port: 50051
                service: myapp.HealthCheck
              periodSeconds: 5
        - id: v8
          title: Readiness Probe for Worker Pod
          description: >-
            A worker Pod connects to a message queue on startup. Write a <code>readinessProbe</code> that runs
            <code>sh -c "test -S /var/run/mq.sock"</code> to verify the message queue socket exists. Wait 10
            seconds before the first check, then check every 5 seconds.
          hints:
            - "<code>test -S</code> checks if a Unix socket file exists."
            - "Workers that consume from queues should only be considered ready once connected."
          solution: |-
            readinessProbe:
              exec:
                command:
                - sh
                - -c
                - "test -S /var/run/mq.sock"
              initialDelaySeconds: 10
              periodSeconds: 5
        - id: v9
          title: Readiness Probe with All Parameters
          description: >-
            Write a <code>readinessProbe</code> that checks <code>/ready</code> on port <code>8080</code>
            with ALL parameters: initial delay 10s, period 5s, timeout 2s, failure threshold 3,
            success threshold 2.
          hints:
            - "Unlike liveness probes, readiness probes allow <code>successThreshold</code> values greater than 1."
            - "A <code>successThreshold</code> of 2 means the Pod must pass two consecutive checks before it's added back to endpoints."
          solution: |-
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              initialDelaySeconds: 10
              periodSeconds: 5
              timeoutSeconds: 2
              failureThreshold: 3
              successThreshold: 2
        - id: v10
          title: Readiness Probe - Why Not Liveness?
          description: >-
            Your app connects to a Redis cache on startup. When Redis goes down temporarily, the app can't
            serve responses. Write the correct probe type and explain why. Use HTTP GET to <code>/ready</code>
            on port <code>8080</code>, checking every 5 seconds.
          hints:
            - "If Redis is down, restarting the app won't fix anything -- Redis is still down."
            - "Use a readiness probe: removes the Pod from endpoints but doesn't restart it."
            - "When Redis recovers, the readiness probe passes and the Pod receives traffic again."
          solution: |-
            # Use readinessProbe, NOT livenessProbe.
            # A dependency being down is a readiness issue, not a liveness issue.
            # Restarting the Pod won't bring Redis back -- it would just
            # cause a thundering herd when Redis recovers.
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              periodSeconds: 5
              failureThreshold: 3
        - id: v11
          title: Readiness for Rolling Update Safety
          description: >-
            Write a <code>readinessProbe</code> for an app that takes about 20 seconds to load configuration
            from a ConfigMap and establish database connections. Use HTTP GET to <code>/ready</code> on port
            <code>8080</code>.
          hints:
            - "Set <code>initialDelaySeconds</code> to give the app time to start."
            - "Without a readiness probe, rolling updates send traffic to Pods that aren't ready yet."
          solution: |-
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              initialDelaySeconds: 20
              periodSeconds: 5
              failureThreshold: 3
    - id: warmup_3
      concept: Startup Probes
      variants:
        - id: v1
          title: Basic Startup Probe
          description: >-
            Write a <code>startupProbe</code> using HTTP GET to <code>/healthz</code> on port
            <code>8080</code>. Allow up to 60 seconds for the app to start (check every 5 seconds).
          hints:
            - "Max startup time = <code>failureThreshold * periodSeconds</code>."
            - "60 seconds / 5 seconds per check = 12 failures before the container is killed."
          solution: |-
            startupProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 5
              failureThreshold: 12
        - id: v2
          title: Startup Probe for Spring Boot App
          description: >-
            A Spring Boot application takes up to 3 minutes to start (loading context, running migrations).
            Write a <code>startupProbe</code> using HTTP GET to <code>/actuator/health</code> on port
            <code>8080</code> that allows enough time.
          hints:
            - "3 minutes = 180 seconds. Choose <code>periodSeconds</code> and <code>failureThreshold</code> that multiply to at least 180."
            - "Example: periodSeconds 10 * failureThreshold 18 = 180 seconds."
          solution: |-
            startupProbe:
              httpGet:
                path: /actuator/health
                port: 8080
              periodSeconds: 10
              failureThreshold: 18
        - id: v3
          title: Calculate Max Startup Time
          description: >-
            A startup probe has <code>periodSeconds: 10</code> and <code>failureThreshold: 30</code>. What is
            the maximum time the application has to start before the container is killed and restarted?
          hints:
            - "Max startup time = <code>failureThreshold * periodSeconds</code>."
            - "Multiply the two values together."
          solution: |-
            # Max startup time = failureThreshold * periodSeconds
            # = 30 * 10
            # = 300 seconds (5 minutes)
            #
            # If the app hasn't passed the startup probe
            # within 300 seconds, the container is killed
            # and restarted.
        - id: v4
          title: Startup Probe with TCP Check
          description: >-
            Write a <code>startupProbe</code> for a database that listens on port <code>5432</code> once
            initialized. Allow up to 90 seconds for startup. Use a TCP socket check.
          hints:
            - "TCP probes are good for services that don't have an HTTP health endpoint during startup."
            - "Choose period and threshold so that period * threshold >= 90."
          solution: |-
            startupProbe:
              tcpSocket:
                port: 5432
              periodSeconds: 5
              failureThreshold: 18
        - id: v5
          title: Startup Probe with Exec
          description: >-
            Write a <code>startupProbe</code> that runs <code>sh -c "test -f /app/initialized"</code> to
            check if the app has completed its initialization. Allow up to 120 seconds.
          hints:
            - "The app creates <code>/app/initialized</code> when it finishes its startup routine."
            - "Choose period and threshold so their product is at least 120."
          solution: |-
            startupProbe:
              exec:
                command:
                - sh
                - -c
                - "test -f /app/initialized"
              periodSeconds: 10
              failureThreshold: 12
        - id: v6
          title: Startup + Liveness Combination
          description: >-
            Write both a <code>startupProbe</code> and a <code>livenessProbe</code> for an app on port
            <code>8080</code>. The app takes up to 2 minutes to start. Once started, check liveness every 10
            seconds. Both probes use HTTP GET to <code>/healthz</code>.
          hints:
            - "The startup probe protects the liveness probe from killing a slow-starting container."
            - "Once the startup probe succeeds, it never runs again -- liveness takes over."
          solution: |-
            startupProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 10
              failureThreshold: 12
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 10
              failureThreshold: 3
        - id: v7
          title: Calculate Startup Probe Settings
          description: >-
            Your ML model server takes up to 5 minutes to load the model into memory. Write a
            <code>startupProbe</code> using HTTP GET to <code>/health</code> on port <code>8501</code>.
            Choose appropriate <code>periodSeconds</code> and <code>failureThreshold</code>.
          hints:
            - "5 minutes = 300 seconds."
            - "A reasonable check interval might be every 10 seconds: 300 / 10 = 30 failure threshold."
          solution: |-
            startupProbe:
              httpGet:
                path: /health
                port: 8501
              periodSeconds: 10
              failureThreshold: 30
        - id: v8
          title: Why Startup Probe Over initialDelaySeconds
          description: >-
            Rewrite this liveness probe to use a startup probe instead of a long
            <code>initialDelaySeconds</code>. The original config is:<br>
            <code>livenessProbe: httpGet /healthz port 8080, initialDelaySeconds: 180, periodSeconds: 10</code>
          hints:
            - "Move the 180-second wait into a startup probe: <code>periodSeconds * failureThreshold >= 180</code>."
            - "The liveness probe no longer needs a long <code>initialDelaySeconds</code> because the startup probe handles the wait."
          solution: |-
            startupProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 10
              failureThreshold: 18
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 10
              failureThreshold: 3
        - id: v9
          title: All Three Probes Together
          description: >-
            Write all three probes for a web app on port <code>8080</code>. Startup allows 2 minutes
            (check <code>/healthz</code>), liveness checks <code>/livez</code> every 10s, readiness
            checks <code>/ready</code> every 5s.
          hints:
            - "Startup and liveness can share the same endpoint (<code>/healthz</code>) or use separate ones."
            - "Readiness should use a separate endpoint that checks dependencies."
            - "Startup probe parameters: 2 min = 120s. Example: period 10 * threshold 12 = 120."
          solution: |-
            startupProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 10
              failureThreshold: 12
            livenessProbe:
              httpGet:
                path: /livez
                port: 8080
              periodSeconds: 10
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              periodSeconds: 5
              failureThreshold: 3
        - id: v10
          title: Startup Probe for Legacy App
          description: >-
            A legacy Java application has an unpredictable startup time between 1 and 10 minutes. Write a
            <code>startupProbe</code> using exec to run <code>curl -f http://localhost:8080/healthz</code>.
            Allow up to 10 minutes for startup.
          hints:
            - "10 minutes = 600 seconds. Example: periodSeconds 15 * failureThreshold 40 = 600."
            - "Using <code>curl -f</code> via exec is a workaround when the container image has curl but the app doesn't have a standard health endpoint."
          solution: |-
            startupProbe:
              exec:
                command:
                - curl
                - -f
                - http://localhost:8080/healthz
              periodSeconds: 15
              failureThreshold: 40
        - id: v11
          title: Startup Probe Max Time Calculation
          description: >-
            Given these startup probe settings, calculate the maximum startup time and explain what happens
            if the app exceeds it:<br>
            <code>periodSeconds: 5</code>, <code>failureThreshold: 24</code>, <code>timeoutSeconds: 3</code>
          hints:
            - "Max startup time = failureThreshold * periodSeconds."
            - "The timeoutSeconds counts as a failure for each individual check but doesn't change the max time formula."
          solution: |-
            # Max startup time = failureThreshold * periodSeconds
            # = 24 * 5
            # = 120 seconds (2 minutes)
            #
            # timeoutSeconds (3) means each individual probe attempt
            # fails if no response within 3 seconds. But the overall
            # max startup window is still 120 seconds.
            #
            # If the app hasn't passed the startup probe within
            # 120 seconds, the container is killed and restarted.
            # The Pod stays on the same node.
    - id: warmup_4
      concept: Logging
      variants:
        - id: v1
          title: View Pod Logs
          description: Write the <code>kubectl</code> command to view the logs of a Pod named <code>web-app</code>.
          hints:
            - "Use <code>kubectl logs</code> followed by the Pod name."
          solution: |-
            kubectl logs web-app
        - id: v2
          title: Logs from Specific Container
          description: >-
            A Pod named <code>my-pod</code> has two containers: <code>app</code> and <code>sidecar</code>.
            Write the command to view logs from only the <code>sidecar</code> container.
          hints:
            - "Use the <code>-c</code> flag to specify a container name."
          solution: |-
            kubectl logs my-pod -c sidecar
        - id: v3
          title: Previous Container Logs
          description: >-
            A Pod named <code>api-server</code> is in <code>CrashLoopBackOff</code>. Write the command to
            view the logs from the container that just crashed.
          hints:
            - "Use <code>--previous</code> to get logs from the previous (crashed) container instance."
            - "This is essential for debugging CrashLoopBackOff."
          solution: |-
            kubectl logs api-server --previous
        - id: v4
          title: Follow / Stream Logs
          description: >-
            Write the command to stream logs from a Pod named <code>worker</code> in real-time.
          hints:
            - "Use <code>-f</code> (follow) to stream logs as they are produced."
            - "Press Ctrl+C to stop streaming."
          solution: |-
            kubectl logs worker -f
        - id: v5
          title: Logs Since a Time Period
          description: >-
            Write the command to view only the last 30 minutes of logs from a Pod named <code>api-server</code>.
          hints:
            - "Use <code>--since</code> with a duration value like <code>30m</code>."
          solution: |-
            kubectl logs api-server --since=30m
        - id: v6
          title: Tail Last N Lines
          description: >-
            Write the command to view only the last 100 lines of logs from a Pod named <code>worker</code>.
          hints:
            - "Use <code>--tail</code> followed by the number of lines."
          solution: |-
            kubectl logs worker --tail=100
        - id: v7
          title: Logs by Label Selector
          description: >-
            Write the command to view logs from all Pods with the label <code>app=web</code>.
          hints:
            - "Use <code>-l</code> to select Pods by label."
          solution: |-
            kubectl logs -l app=web
        - id: v8
          title: Combined Flags
          description: >-
            Write the command to stream the last 50 lines of logs from the <code>nginx</code> container in a
            Pod named <code>frontend</code>, showing only logs from the last hour.
          hints:
            - "Combine <code>-c</code>, <code>-f</code>, <code>--tail</code>, and <code>--since</code> flags."
            - "All these flags can be used together in a single command."
          solution: |-
            kubectl logs frontend -c nginx -f --tail=50 --since=1h
        - id: v9
          title: Logs from All Containers in Labeled Pods
          description: >-
            Write the command to view logs from all containers in all Pods matching <code>app=api</code>.
          hints:
            - "Use <code>-l</code> for label selection and <code>--all-containers</code> for multi-container Pods."
          solution: |-
            kubectl logs -l app=api --all-containers
        - id: v10
          title: Logs from a Deployment
          description: >-
            Write the command to view logs from the <code>web</code> Deployment.
          hints:
            - "Use <code>deployment/</code> prefix to target a Deployment instead of a specific Pod."
            - "This picks one Pod from the Deployment."
          solution: |-
            kubectl logs deployment/web
        - id: v11
          title: Previous Container Logs with Container Flag
          description: >-
            A multi-container Pod named <code>backend</code> has containers <code>app</code> and
            <code>envoy</code>. The <code>app</code> container just crashed. Write the command to see its
            crash logs.
          hints:
            - "Combine <code>-c</code> to select the container and <code>--previous</code> to get crash logs."
          solution: |-
            kubectl logs backend -c app --previous
        - id: v12
          title: Logs Since Specific Time
          description: >-
            Write the command to view logs from Pod <code>api-server</code> since January 15, 2025 at 10:00
            UTC.
          hints:
            - "Use <code>--since-time</code> with an RFC3339 timestamp."
          solution: |-
            kubectl logs api-server --since-time="2025-01-15T10:00:00Z"
  challenges:
    - id: challenge_1
      block: 1
      difficulty: 2
      concept: Probe Design
      variants:
        - id: v1
          title: Web API with Database Dependency
          description: >-
            A web API runs on port <code>8080</code>, connects to a PostgreSQL database, and takes about 15
            seconds to initialize. Design probes for this container. The app exposes <code>/livez</code>
            (checks only the process) and <code>/ready</code> (checks the database connection).
          functionSignature: "container probe spec (all three probes)"
          testCases:
            - input: "App starts in 15s, database goes down temporarily for 60s"
              output: "During DB outage: Pod removed from endpoints (readiness fails), NOT restarted. When DB recovers, Pod rejoins endpoints."
            - input: "App deadlocks after running for 2 hours"
              output: "Liveness probe fails, container is restarted"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Which checks should go in readiness vs liveness? The key rule: never check dependencies in
                liveness probes.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use <code>/livez</code> for liveness (process-only check) and <code>/ready</code> for
                readiness (includes DB check). Add a startup probe since the app takes 15s to init.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Startup probe: protects against killing during init
                2. Liveness probe: /livez endpoint (process-only)
                3. Readiness probe: /ready endpoint (checks DB)</pre>
          solution: |-
            startupProbe:
              httpGet:
                path: /livez
                port: 8080
              periodSeconds: 5
              failureThreshold: 6       # 5 * 6 = 30s max startup
            livenessProbe:
              httpGet:
                path: /livez
                port: 8080
              periodSeconds: 10
              timeoutSeconds: 3
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              periodSeconds: 5
              timeoutSeconds: 3
              failureThreshold: 3
          difficulty: 2
        - id: v2
          title: Spring Boot Application
          description: >-
            A Spring Boot app on port <code>8080</code> takes between 30 seconds and 2 minutes to start
            (loading Spring context and running Flyway migrations). It exposes
            <code>/actuator/health/liveness</code> and <code>/actuator/health/readiness</code>. Design all
            three probes.
          functionSignature: "container probe spec (all three probes)"
          testCases:
            - input: "App starts in 45 seconds"
              output: "Startup probe succeeds at ~45s, liveness and readiness begin immediately"
            - input: "App takes 90 seconds to start"
              output: "Startup probe continues checking, liveness is delayed, no premature restart"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Without a startup probe, a liveness check starting at 10s would kill the container before
                Spring Boot finishes loading. How do you protect slow-starting apps?
            - title: "\U0001F4A1 Hint"
              content: >-
                The startup probe must allow up to 2 minutes (120s). Example: periodSeconds 5 *
                failureThreshold 24 = 120s.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Startup: generous window (2 min), same endpoint as liveness
                2. Liveness: /actuator/health/liveness
                3. Readiness: /actuator/health/readiness</pre>
          solution: |-
            startupProbe:
              httpGet:
                path: /actuator/health/liveness
                port: 8080
              periodSeconds: 5
              failureThreshold: 24      # 5 * 24 = 120s max startup
            livenessProbe:
              httpGet:
                path: /actuator/health/liveness
                port: 8080
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /actuator/health/readiness
                port: 8080
              periodSeconds: 5
              timeoutSeconds: 3
              failureThreshold: 3
          difficulty: 2
        - id: v3
          title: Redis Cache Server
          description: >-
            A Redis container runs on port <code>6379</code>. It does not have an HTTP endpoint. Design
            probes using TCP and exec approaches. Use <code>redis-cli ping</code> for liveness and TCP
            socket for readiness.
          functionSignature: "container probe spec (liveness + readiness)"
          testCases:
            - input: "Redis process hangs and stops responding to commands"
              output: "Liveness exec (redis-cli ping) fails, container restarts"
            - input: "Redis is loading RDB snapshot and not yet accepting connections"
              output: "TCP readiness fails, Pod removed from endpoints until port 6379 accepts connections"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Redis doesn't have HTTP endpoints. What probe mechanisms can you use for non-HTTP services?
            - title: "\U0001F4A1 Hint"
              content: >-
                <code>redis-cli ping</code> returns PONG (exit 0) when Redis is responding. TCP socket
                on 6379 verifies port is accepting connections.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Liveness: exec redis-cli ping (checks Redis is responsive)
                2. Readiness: tcpSocket on 6379 (port accepting connections)</pre>
          solution: |-
            livenessProbe:
              exec:
                command:
                - redis-cli
                - ping
              periodSeconds: 10
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              tcpSocket:
                port: 6379
              periodSeconds: 5
              failureThreshold: 3
          difficulty: 2
        - id: v4
          title: ML Model Server with Long Startup
          description: >-
            An ML model serving container on port <code>8501</code> takes up to 5 minutes to load a large
            model into GPU memory. Once loaded, it exposes <code>/v1/models/mymodel</code> which returns 200
            if the model is ready. Design all three probes.
          functionSignature: "container probe spec (all three probes)"
          testCases:
            - input: "Model loads in 3 minutes"
              output: "Startup probe succeeds at ~3 min, liveness/readiness start immediately"
            - input: "Model takes 6 minutes to load (exceeds 5 min)"
              output: "Startup probe fails after 5 min, container is killed and restarted"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                5 minutes = 300 seconds. The startup probe must allow at least that long. What are
                reasonable periodSeconds and failureThreshold values?
            - title: "\U0001F4A1 Hint"
              content: >-
                Startup: periodSeconds 10 * failureThreshold 30 = 300s. Liveness and readiness can use the
                model endpoint or a separate health endpoint.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Startup: 5 min window for model loading
                2. Liveness: basic process check
                3. Readiness: model endpoint (returns 200 when model is loaded)</pre>
          solution: |-
            startupProbe:
              httpGet:
                path: /v1/models/mymodel
                port: 8501
              periodSeconds: 10
              failureThreshold: 30      # 10 * 30 = 300s (5 min)
            livenessProbe:
              httpGet:
                path: /v1/models/mymodel
                port: 8501
              periodSeconds: 15
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /v1/models/mymodel
                port: 8501
              periodSeconds: 5
              timeoutSeconds: 3
              failureThreshold: 2
          difficulty: 3
        - id: v5
          title: gRPC Microservice
          description: >-
            A gRPC microservice runs on port <code>50051</code> and takes about 10 seconds to start. It
            implements the gRPC Health Checking Protocol with services <code>""</code> (overall health)
            and <code>myapp.OrderService</code> (service-specific). Design probes using gRPC.
          functionSignature: "container probe spec (all three probes)"
          testCases:
            - input: "Service starts in 8 seconds"
              output: "Startup succeeds, liveness (overall) and readiness (OrderService) begin"
            - input: "OrderService stops serving but process is alive"
              output: "Readiness fails (OrderService unhealthy), Pod removed from endpoints; liveness (overall) still passes"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                gRPC probes can target specific services. Which service should readiness check vs liveness?
            - title: "\U0001F4A1 Hint"
              content: >-
                Liveness: check overall health (empty service name). Readiness: check the specific service
                that handles traffic.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Startup: gRPC overall health, 30s window
                2. Liveness: gRPC overall health (is the process alive?)
                3. Readiness: gRPC OrderService health (can it serve?)</pre>
          solution: |-
            startupProbe:
              grpc:
                port: 50051
              periodSeconds: 5
              failureThreshold: 6       # 5 * 6 = 30s max startup
            livenessProbe:
              grpc:
                port: 50051
              periodSeconds: 10
              failureThreshold: 3
            readinessProbe:
              grpc:
                port: 50051
                service: myapp.OrderService
              periodSeconds: 5
              failureThreshold: 3
          difficulty: 3
        - id: v6
          title: Background Worker (No HTTP/gRPC)
          description: >-
            A background worker container processes jobs from a queue. It has no network ports. It writes
            a heartbeat file at <code>/tmp/heartbeat</code> every 30 seconds and creates
            <code>/tmp/ready</code> when connected to the queue. Design probes using exec commands.
          functionSignature: "container probe spec (liveness + readiness)"
          testCases:
            - input: "Worker connects to queue in 5s, heartbeat is written every 30s"
              output: "Readiness passes when /tmp/ready exists, liveness passes as long as heartbeat is fresh"
            - input: "Worker deadlocks, stops writing heartbeat"
              output: "After failureThreshold, liveness exec fails, container restarts"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                For containers with no network ports, exec probes are the only option. How do you check if
                a file was recently modified?
            - title: "\U0001F4A1 Hint"
              content: >-
                Liveness: use <code>find /tmp/heartbeat -mmin -1</code> to check if the file was modified
                in the last minute. Readiness: use <code>test -f /tmp/ready</code>.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Liveness: exec check if heartbeat file is recent
                2. Readiness: exec check if ready file exists</pre>
          solution: |-
            livenessProbe:
              exec:
                command:
                - sh
                - -c
                - "find /tmp/heartbeat -mmin -1 | grep -q heartbeat"
              periodSeconds: 30
              timeoutSeconds: 5
              failureThreshold: 3
            readinessProbe:
              exec:
                command:
                - sh
                - -c
                - "test -f /tmp/ready"
              periodSeconds: 5
              failureThreshold: 3
          difficulty: 3
        - id: v7
          title: Multi-Port Service (API + Metrics)
          description: >-
            A container exposes an API on port <code>8080</code> and Prometheus metrics on port
            <code>9090</code>. The API has <code>/healthz</code> and <code>/ready</code> endpoints. Design
            probes that check the API port (not the metrics port). Allow 30 seconds for startup.
          functionSignature: "container probe spec (all three probes)"
          testCases:
            - input: "API starts in 10 seconds, metrics port is up immediately"
              output: "Startup probe checks API port 8080, succeeds at ~10s"
            - input: "Metrics port crashes but API is still healthy"
              output: "All probes still pass because they check port 8080"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Probes should check the primary service port, not secondary ports like metrics. Which port
                matters for determining if the app can serve traffic?
            - title: "\U0001F4A1 Hint"
              content: >-
                All probes should target port 8080 (the API). Use <code>/healthz</code> for liveness/startup,
                <code>/ready</code> for readiness.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Startup: httpGet /healthz on 8080, 30s window
                2. Liveness: httpGet /healthz on 8080
                3. Readiness: httpGet /ready on 8080</pre>
          solution: |-
            startupProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 5
              failureThreshold: 6       # 5 * 6 = 30s
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 10
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              periodSeconds: 5
              failureThreshold: 3
          difficulty: 2
        - id: v8
          title: Sidecar Container Probe Design
          description: >-
            A Pod has two containers: <code>app</code> (port 8080, HTTP endpoints <code>/healthz</code> and
            <code>/ready</code>) and <code>envoy</code> sidecar proxy (port 15021, HTTP endpoint
            <code>/healthz/ready</code>). Design probes for BOTH containers. The app takes 20 seconds to
            start; the envoy sidecar starts in 2 seconds.
          functionSignature: "probe specs for both containers"
          testCases:
            - input: "App starts in 15s, envoy in 2s"
              output: "Both containers pass all probes, Pod is Ready"
            - input: "App is healthy but envoy crashes"
              output: "Envoy liveness fails, envoy container restarts; app container is unaffected"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Each container in a Pod has its own probes. The Pod is Ready only when ALL containers pass
                their readiness probes.
            - title: "\U0001F4A1 Hint"
              content: >-
                App probes target port 8080, envoy probes target port 15021. The app needs a startup probe;
                envoy starts fast so it may not.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>app container:
                  startup: /healthz on 8080, 30s window
                  liveness: /healthz on 8080
                  readiness: /ready on 8080
                envoy container:
                  liveness: /healthz/ready on 15021
                  readiness: /healthz/ready on 15021</pre>
          solution: |-
            # app container probes:
            startupProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 5
              failureThreshold: 6       # 5 * 6 = 30s
            livenessProbe:
              httpGet:
                path: /healthz
                port: 8080
              periodSeconds: 10
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /ready
                port: 8080
              periodSeconds: 5
              failureThreshold: 3

            # envoy container probes:
            livenessProbe:
              httpGet:
                path: /healthz/ready
                port: 15021
              periodSeconds: 10
              failureThreshold: 3
            readinessProbe:
              httpGet:
                path: /healthz/ready
                port: 15021
              periodSeconds: 5
              failureThreshold: 3
          difficulty: 4
    - id: challenge_2
      block: 1
      difficulty: 1
      concept: Probe Behavior Prediction
      variants:
        - id: v1
          title: Liveness Failure After Startup
          description: >-
            A container has this config:<br>
            <code>livenessProbe: httpGet /healthz port 8080, periodSeconds: 10, failureThreshold: 3</code><br><br>
            The app starts normally, runs for 5 minutes, then the <code>/healthz</code> endpoint starts
            returning HTTP 500. What happens and when?
          functionSignature: "Describe the sequence of events"
          testCases:
            - input: "/healthz returns 500 starting at t=5m"
              output: "At ~5m30s (3 failures * 10s period), container is restarted"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The liveness probe checks every 10 seconds. How many failures does it take before the
                container is restarted?
            - title: "\U0001F4A1 Hint"
              content: >-
                After <code>failureThreshold</code> (3) consecutive failures at <code>periodSeconds</code>
                (10) intervals, the container is killed and restarted. That's ~30 seconds after the first
                failure.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. t=5m00s: /healthz returns 500  failure 1
                2. t=5m10s: /healthz returns 500  failure 2
                3. t=5m20s: /healthz returns 500  failure 3
                4. Container is killed and restarted
                5. RESTARTS count increments to 1</pre>
          solution: |-
            # Sequence of events:
            # 1. t=5m00s: Liveness probe gets HTTP 500  failure count = 1
            # 2. t=5m10s: Liveness probe gets HTTP 500  failure count = 2
            # 3. t=5m20s: Liveness probe gets HTTP 500  failure count = 3 (= failureThreshold)
            # 4. Kubelet kills the container and restarts it
            # 5. RESTARTS column increments to 1
            # 6. If the restarted container is healthy, everything resumes normally
            # 7. If it keeps failing, CrashLoopBackOff with increasing backoff
          difficulty: 1
        - id: v2
          title: Readiness Failure During Database Outage
          description: >-
            A Pod has:<br>
            <code>readinessProbe: httpGet /ready port 8080, periodSeconds: 5, failureThreshold: 3</code><br>
            <code>livenessProbe: httpGet /livez port 8080, periodSeconds: 10, failureThreshold: 3</code><br><br>
            The <code>/ready</code> endpoint checks the database connection. The <code>/livez</code> endpoint
            only checks if the process is running. The database goes down for 2 minutes. What happens?
          functionSignature: "Describe the sequence of events"
          testCases:
            - input: "Database goes down at t=0, recovers at t=2m"
              output: "Pod is removed from Service endpoints within ~15s, NOT restarted; rejoins endpoints after DB recovers"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Which probe checks the database? What is the consequence of that probe failing vs the other?
            - title: "\U0001F4A1 Hint"
              content: >-
                Readiness (checks DB) will fail  Pod removed from Service endpoints. Liveness (checks
                process) will pass  no restart. The Pod stays running but receives no traffic.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. DB goes down
                2. /ready fails  after 3 failures (15s), Pod removed from endpoints
                3. /livez still passes  no restart
                4. DB recovers  /ready passes  Pod added back to endpoints</pre>
          solution: |-
            # Sequence of events:
            # 1. t=0s: Database goes down
            # 2. t=5s: Readiness probe  /ready fails (DB unreachable)  failure 1
            # 3. t=10s: Readiness fails  failure 2
            # 4. t=15s: Readiness fails  failure 3  Pod removed from Service endpoints
            # 5. No traffic is routed to this Pod
            # 6. Liveness probe continues to pass (/livez only checks process)
            # 7. Container is NOT restarted
            # 8. t=2m: Database recovers
            # 9. Next readiness probe passes  Pod added back to Service endpoints
            # 10. Traffic resumes
          difficulty: 1
        - id: v3
          title: Startup Probe Timeout
          description: >-
            A container has:<br>
            <code>startupProbe: httpGet /healthz port 8080, periodSeconds: 10, failureThreshold: 12</code><br>
            <code>livenessProbe: httpGet /healthz port 8080, periodSeconds: 10, failureThreshold: 3</code><br><br>
            The app takes 150 seconds to start. What happens? Does the liveness probe interfere?
          functionSignature: "Describe the sequence of events"
          testCases:
            - input: "App starts in 150 seconds"
              output: "Startup probe fails for 120s then the container is killed (150 > failureThreshold * periodSeconds = 120)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The startup probe allows failureThreshold * periodSeconds = 12 * 10 = 120 seconds. The app
                needs 150 seconds. What happens?
            - title: "\U0001F4A1 Hint"
              content: >-
                The app will NOT start in time. The startup probe reaches its failure threshold at 120s and
                the container is killed. The liveness probe never even runs because startup never succeeded.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Startup probe checks every 10s, all fail
                2. After 12 failures (120s), container is killed
                3. Liveness probe never ran (blocked by startup probe)
                4. Container restarts, same thing happens  CrashLoopBackOff</pre>
          solution: |-
            # Sequence of events:
            # 1. Container starts. Startup probe begins checking.
            # 2. Liveness and readiness probes are DISABLED while startup probe runs.
            # 3. Every 10s, startup probe checks /healthz  fails (app not ready yet)
            # 4. At 120s (12 failures * 10s), startup probe exceeds failureThreshold
            # 5. Container is killed and restarted
            # 6. The app needed 150s but only got 120s  it will NEVER start
            # 7. The liveness probe never ran because startup probe never succeeded
            # 8. Pod enters CrashLoopBackOff
            #
            # Fix: Increase failureThreshold to at least 15 (15 * 10 = 150s)
            # or increase both to give a buffer: failureThreshold 20 = 200s
          difficulty: 2
        - id: v4
          title: Intermittent Liveness Failures
          description: >-
            A container has:<br>
            <code>livenessProbe: httpGet /healthz port 8080, periodSeconds: 10, failureThreshold: 3,
            successThreshold: 1</code><br><br>
            The app is flaky: <code>/healthz</code> returns 500 twice, then 200, then 500 twice, then 200,
            repeating. Does the container ever get restarted?
          functionSignature: "Describe what happens"
          testCases:
            - input: "Pattern: fail, fail, success, fail, fail, success, ..."
              output: "Container is never restarted because it never reaches 3 CONSECUTIVE failures"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The key word is <em>consecutive</em>. Does a success reset the failure counter?
            - title: "\U0001F4A1 Hint"
              content: >-
                A single success resets the consecutive failure counter to zero. Since
                <code>failureThreshold</code> is 3 but the app only fails twice in a row before succeeding,
                it never reaches 3 consecutive failures.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>fail  count=1
                fail  count=2
                success  count=0 (reset!)
                fail  count=1
                fail  count=2
                success  count=0 (reset!)
                ... never reaches 3</pre>
          solution: |-
            # The container is NEVER restarted.
            #
            # failureThreshold counts CONSECUTIVE failures.
            # A single success resets the counter to 0.
            #
            # Timeline:
            # Check 1: fail  consecutive failures = 1
            # Check 2: fail  consecutive failures = 2
            # Check 3: success  consecutive failures = 0 (reset!)
            # Check 4: fail  consecutive failures = 1
            # Check 5: fail  consecutive failures = 2
            # Check 6: success  consecutive failures = 0 (reset!)
            #
            # The failure count never reaches 3, so the container
            # is never killed. This is by design  intermittent
            # failures shouldn't trigger a restart.
          difficulty: 2
        - id: v5
          title: No Readiness Probe During Rolling Update
          description: >-
            A Deployment with 3 replicas has NO readiness probe defined. You update the image from
            <code>v1</code> to <code>v2</code>. The new version takes 30 seconds to initialize before
            it can handle requests. What happens during the rolling update?
          functionSignature: "Describe the user-visible impact"
          testCases:
            - input: "Rolling update from v1 to v2, new Pods take 30s to initialize"
              output: "Users see errors for ~30s per new Pod because traffic is sent before the app is ready"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Without a readiness probe, when does Kubernetes consider a Pod ready? What happens when the
                Service sends traffic to a Pod that isn't ready?
            - title: "\U0001F4A1 Hint"
              content: >-
                Without a readiness probe, the Pod is considered Ready as soon as the container starts. The
                Service immediately sends traffic to it, but the app won't respond correctly for 30 seconds.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. New Pod starts  immediately marked Ready
                2. Service sends traffic to new Pod
                3. App returns errors for 30s (still initializing)
                4. Users experience 5xx errors during rollout
                5. Fix: add a readiness probe</pre>
          solution: |-
            # Without a readiness probe:
            #
            # 1. K8s creates a new Pod with v2 image
            # 2. Container starts  Pod immediately marked Ready
            # 3. Service adds the Pod to endpoints
            # 4. Traffic is routed to the new Pod
            # 5. App is still initializing for 30s  returns errors (502/503)
            # 6. Users see intermittent errors during the rollout
            # 7. K8s terminates an old v1 Pod (it thinks v2 is serving)
            # 8. This repeats for each Pod in the rolling update
            #
            # With a readiness probe, the Pod wouldn't be added to
            # endpoints until it passes the readiness check, preventing
            # errors during the rollout.
          difficulty: 2
        - id: v6
          title: Liveness Probe Checking Database (Anti-Pattern)
          description: >-
            A Deployment with 10 replicas has:<br>
            <code>livenessProbe: httpGet /healthz port 8080, periodSeconds: 5, failureThreshold: 2</code><br><br>
            The <code>/healthz</code> endpoint checks the database connection. The database goes down for 30
            seconds. What happens?
          functionSignature: "Describe the cascading failure"
          testCases:
            - input: "Database goes down for 30s, all 10 Pods have liveness checking DB"
              output: "All 10 Pods restart within ~10s, thundering herd reconnects crash DB again"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                All 10 Pods check the database every 5 seconds via liveness probe. With a failureThreshold
                of only 2, how quickly are all Pods killed?
            - title: "\U0001F4A1 Hint"
              content: >-
                After 2 failures (10 seconds), all 10 Pods are restarted simultaneously. When the DB
                recovers, all 10 try to reconnect at once (thundering herd).
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. DB down  all 10 Pods' liveness fails
                2. After 10s, all 10 containers restart
                3. DB recovers  10 Pods reconnect simultaneously
                4. Thundering herd overwhelms DB  DB slows/crashes
                5. Liveness fails again  restart loop
                6. Fix: only check DB in readiness, not liveness</pre>
          solution: |-
            # Cascading failure sequence:
            #
            # 1. Database goes down
            # 2. t=5s: All 10 Pods  /healthz fails (DB check)  failure 1
            # 3. t=10s: All 10 Pods  /healthz fails  failure 2 = failureThreshold
            # 4. All 10 containers are killed and restarted SIMULTANEOUSLY
            # 5. Database recovers at t=30s
            # 6. All 10 restarting Pods try to connect to DB at the same time
            # 7. Thundering herd overwhelms the database
            # 8. DB slows down or crashes again
            # 9. Liveness probes fail again  all Pods restart again
            # 10. Cycle repeats  cascading failure
            #
            # Fix: Move the DB check to readinessProbe (removes from
            # endpoints, no restart). Use livenessProbe to check only
            # the process (/livez endpoint that doesn't touch DB).
          difficulty: 3
        - id: v7
          title: Readiness with High successThreshold
          description: >-
            A Pod has:<br>
            <code>readinessProbe: httpGet /ready port 8080, periodSeconds: 5, failureThreshold: 1,
            successThreshold: 3</code><br><br>
            The app finishes startup and <code>/ready</code> starts returning 200. How long until the Pod is
            added to Service endpoints?
          functionSignature: "Calculate the time to ready"
          testCases:
            - input: "/ready starts returning 200 at t=0"
              output: "Pod is added to endpoints at t=10s (3 successes * 5s period, minus the first check)"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                With <code>successThreshold: 3</code>, the Pod needs 3 consecutive successful checks before
                it's marked Ready.
            - title: "\U0001F4A1 Hint"
              content: >-
                3 consecutive successes at 5-second intervals. The first success is at the current check,
                second at +5s, third at +10s. The Pod becomes Ready after the third success.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>t=0s: /ready returns 200  success 1
                t=5s: /ready returns 200  success 2
                t=10s: /ready returns 200  success 3 = successThreshold
                Pod is added to Service endpoints at t=10s</pre>
          solution: |-
            # With successThreshold: 3 and periodSeconds: 5:
            #
            # t=0s:  /ready  200  consecutive successes = 1
            # t=5s:  /ready  200  consecutive successes = 2
            # t=10s: /ready  200  consecutive successes = 3 (= successThreshold)
            #  Pod is marked Ready and added to Service endpoints
            #
            # Time from first success to Ready: 10 seconds (2 additional checks)
            #
            # This is useful for flaky apps where you want to be sure
            # the app is stable before sending traffic to it.
          difficulty: 2
        - id: v8
          title: Startup Probe Protects Liveness
          description: >-
            A container has:<br>
            <code>startupProbe: httpGet /healthz port 8080, periodSeconds: 5, failureThreshold: 20</code><br>
            <code>livenessProbe: httpGet /healthz port 8080, periodSeconds: 10, failureThreshold: 3</code><br><br>
            The app starts in 45 seconds. Without the startup probe, what would happen? With it, what happens?
          functionSignature: "Compare both scenarios"
          testCases:
            - input: "App starts in 45 seconds, with startup probe"
              output: "Startup probe fails for 45s then succeeds, liveness begins at ~45s"
            - input: "App starts in 45 seconds, without startup probe"
              output: "Liveness would fail 3 times in 30s and kill the container before it's ready"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Without a startup probe, the liveness probe starts immediately. 3 failures at 10s = 30s.
                But the app needs 45s to start.
            - title: "\U0001F4A1 Hint"
              content: >-
                Without startup probe: liveness kills the container at 30s, app never starts.
                With startup probe: liveness is delayed until startup succeeds at ~45s, then runs normally.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>WITHOUT startup probe:
                  t=10s: liveness fail 1
                  t=20s: liveness fail 2
                  t=30s: liveness fail 3  KILLED (app needed 45s!)

                WITH startup probe:
                  t=0-45s: startup probe runs, liveness is DISABLED
                  t=45s: startup succeeds
                  t=55s: first liveness check  passes</pre>
          solution: |-
            # WITHOUT startup probe:
            # t=10s: Liveness check  /healthz fails (app not ready)  failure 1
            # t=20s: Liveness check  fails  failure 2
            # t=30s: Liveness check  fails  failure 3 = threshold
            # Container is KILLED at 30s. App needed 45s  NEVER starts.
            # CrashLoopBackOff cycle.
            #
            # WITH startup probe:
            # t=0-45s: Startup probe runs every 5s, all fail.
            #          Liveness and readiness probes are DISABLED.
            # t=45s:   Startup probe succeeds. It never runs again.
            # t=55s:   First liveness check  /healthz passes
            # t=60s:   Second liveness check  passes
            # ... app runs normally.
            #
            # Startup probe window: 5 * 20 = 100s (plenty of room for 45s startup)
          difficulty: 1
    - id: challenge_3
      block: 2
      difficulty: 2
      concept: Observability Setup
      variants:
        - id: v1
          title: Production Web App Observability
          description: >-
            Write a complete container spec for a production web app (<code>myapp:2.0</code>, port 8080)
            with all three probes, resource requests/limits (for metrics-server), and named ports. The app
            takes up to 30 seconds to start, exposes <code>/livez</code>, <code>/ready</code>, and
            <code>/metrics</code>.
          functionSignature: "Complete container spec YAML"
          testCases:
            - input: "kubectl top pods shows resource usage"
              output: "CPU and memory usage displayed because resource requests are set"
            - input: "App takes 25 seconds to start during rolling update"
              output: "No errors  startup probe allows 30s, readiness prevents premature traffic"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                A production container needs: probes for health checking, resource requests/limits for
                scheduling and metrics, and named ports for Service targeting.
            - title: "\U0001F4A1 Hint"
              content: >-
                Include: <code>ports</code> (named), <code>resources</code> (requests + limits),
                <code>startupProbe</code>, <code>livenessProbe</code>, <code>readinessProbe</code>.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>containers:
                - name, image, ports (named)
                  resources: requests + limits
                  startupProbe: /livez, 30s window
                  livenessProbe: /livez
                  readinessProbe: /ready</pre>
          solution: |-
            containers:
            - name: app
              image: myapp:2.0
              ports:
              - name: http
                containerPort: 8080
              - name: metrics
                containerPort: 9090
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 500m
                  memory: 256Mi
              startupProbe:
                httpGet:
                  path: /livez
                  port: http
                periodSeconds: 5
                failureThreshold: 6       # 5 * 6 = 30s
              livenessProbe:
                httpGet:
                  path: /livez
                  port: http
                periodSeconds: 10
                timeoutSeconds: 3
                failureThreshold: 3
              readinessProbe:
                httpGet:
                  path: /ready
                  port: http
                periodSeconds: 5
                timeoutSeconds: 3
                failureThreshold: 3
          difficulty: 2
        - id: v2
          title: API Deployment with Full Observability
          description: >-
            Write a complete Deployment YAML for an API service (<code>api-server:1.5</code>, 3 replicas,
            port 8080) with all three probes, resource requests/limits, and a matching Service. The API takes
            up to 45 seconds to start. Use <code>/healthz</code> for startup/liveness and <code>/ready</code>
            for readiness.
          functionSignature: "Complete Deployment + Service YAML"
          testCases:
            - input: "kubectl top pods -l app=api"
              output: "Shows CPU and memory for all 3 replicas"
            - input: "One replica's /ready fails"
              output: "That Pod is removed from Service endpoints, other 2 continue serving"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                A full observability setup includes: Deployment with probes and resources, plus a Service to
                expose it. The readiness probe ensures the Service only routes to healthy Pods.
            - title: "\U0001F4A1 Hint"
              content: >-
                Remember to set <code>selector.matchLabels</code> in the Deployment and <code>selector</code>
                in the Service to the same label. Resource requests enable <code>kubectl top</code>.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Deployment:
                  replicas: 3
                  template:
                    containers with probes + resources
                ---
                Service:
                  selector matching Deployment labels</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: api-server
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: api
              template:
                metadata:
                  labels:
                    app: api
                spec:
                  containers:
                  - name: api
                    image: api-server:1.5
                    ports:
                    - name: http
                      containerPort: 8080
                    resources:
                      requests:
                        cpu: 200m
                        memory: 256Mi
                      limits:
                        cpu: "1"
                        memory: 512Mi
                    startupProbe:
                      httpGet:
                        path: /healthz
                        port: http
                      periodSeconds: 5
                      failureThreshold: 9     # 5 * 9 = 45s
                    livenessProbe:
                      httpGet:
                        path: /healthz
                        port: http
                      periodSeconds: 10
                      timeoutSeconds: 3
                      failureThreshold: 3
                    readinessProbe:
                      httpGet:
                        path: /ready
                        port: http
                      periodSeconds: 5
                      timeoutSeconds: 3
                      failureThreshold: 3
            ---
            apiVersion: v1
            kind: Service
            metadata:
              name: api-server
            spec:
              selector:
                app: api
              ports:
              - port: 80
                targetPort: http
          difficulty: 3
        - id: v3
          title: Database StatefulSet Observability
          description: >-
            Write a complete container spec for a PostgreSQL database (<code>postgres:16</code>, port 5432)
            with appropriate probes (exec-based using <code>pg_isready</code>), resource requests/limits,
            and environment variables for credentials. The database takes up to 60 seconds to initialize.
          functionSignature: "Complete container spec YAML"
          testCases:
            - input: "Database starts and pg_isready returns 0"
              output: "All probes pass, Pod is Ready"
            - input: "Database hangs, pg_isready times out"
              output: "Liveness fails, container restarts"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Databases don't have HTTP endpoints, so use exec probes with <code>pg_isready</code>.
                Resource requests are important for databases to ensure they get scheduled on nodes with
                enough capacity.
            - title: "\U0001F4A1 Hint"
              content: >-
                <code>pg_isready -U postgres</code> returns exit code 0 when PostgreSQL is accepting
                connections. Use this for all three probes.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>containers:
                - name, image, ports
                  env: POSTGRES_PASSWORD
                  resources: requests + limits
                  startupProbe: exec pg_isready, 60s window
                  livenessProbe: exec pg_isready
                  readinessProbe: exec pg_isready</pre>
          solution: |-
            containers:
            - name: postgres
              image: postgres:16
              ports:
              - containerPort: 5432
              env:
              - name: POSTGRES_PASSWORD
                valueFrom:
                  secretKeyRef:
                    name: postgres-secret
                    key: password
              resources:
                requests:
                  cpu: 250m
                  memory: 512Mi
                limits:
                  cpu: "1"
                  memory: 1Gi
              startupProbe:
                exec:
                  command:
                  - pg_isready
                  - -U
                  - postgres
                periodSeconds: 5
                failureThreshold: 12      # 5 * 12 = 60s
              livenessProbe:
                exec:
                  command:
                  - pg_isready
                  - -U
                  - postgres
                periodSeconds: 10
                timeoutSeconds: 5
                failureThreshold: 3
              readinessProbe:
                exec:
                  command:
                  - pg_isready
                  - -U
                  - postgres
                periodSeconds: 5
                timeoutSeconds: 3
                failureThreshold: 3
          difficulty: 2
        - id: v4
          title: Microservice with Sidecar Observability
          description: >-
            Write a Pod spec with two containers: an app (<code>order-service:1.0</code>, port 8080) and an
            Envoy sidecar (<code>envoyproxy/envoy:v1.28</code>, port 9901 for admin). Both need probes and
            resource requests. The app takes 20 seconds to start; Envoy starts in 2 seconds. App endpoints:
            <code>/healthz</code>, <code>/ready</code>. Envoy admin: <code>/ready</code> on port 9901.
          functionSignature: "Complete Pod spec with two containers"
          testCases:
            - input: "kubectl top pods --containers shows both containers"
              output: "Both order-service and envoy show CPU and memory usage"
            - input: "App container becomes unhealthy"
              output: "App container restarts, envoy sidecar is unaffected"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Each container gets its own probes and resource definitions. The Pod is only Ready when ALL
                containers pass their readiness probes.
            - title: "\U0001F4A1 Hint"
              content: >-
                Define probes per container. The app needs startup/liveness/readiness on port 8080. Envoy
                needs liveness/readiness on port 9901. Both need resource requests.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>spec:
                  containers:
                  - name: app (port 8080, all 3 probes, resources)
                  - name: envoy (port 9901, liveness + readiness, resources)</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: order-service
              labels:
                app: order-service
            spec:
              containers:
              - name: app
                image: order-service:1.0
                ports:
                - name: http
                  containerPort: 8080
                resources:
                  requests:
                    cpu: 100m
                    memory: 128Mi
                  limits:
                    cpu: 500m
                    memory: 256Mi
                startupProbe:
                  httpGet:
                    path: /healthz
                    port: http
                  periodSeconds: 5
                  failureThreshold: 4     # 5 * 4 = 20s
                livenessProbe:
                  httpGet:
                    path: /healthz
                    port: http
                  periodSeconds: 10
                  failureThreshold: 3
                readinessProbe:
                  httpGet:
                    path: /ready
                    port: http
                  periodSeconds: 5
                  failureThreshold: 3
              - name: envoy
                image: envoyproxy/envoy:v1.28
                ports:
                - name: admin
                  containerPort: 9901
                resources:
                  requests:
                    cpu: 50m
                    memory: 64Mi
                  limits:
                    cpu: 200m
                    memory: 128Mi
                livenessProbe:
                  httpGet:
                    path: /ready
                    port: admin
                  periodSeconds: 10
                  failureThreshold: 3
                readinessProbe:
                  httpGet:
                    path: /ready
                    port: admin
                  periodSeconds: 5
                  failureThreshold: 3
          difficulty: 3
        - id: v5
          title: Worker Deployment with Exec Probes
          description: >-
            Write a Deployment for a background worker (<code>job-worker:3.0</code>, 5 replicas) that has no
            network ports. It writes a heartbeat to <code>/tmp/alive</code> and creates
            <code>/tmp/queue-connected</code> once connected. Include exec probes and resource requests.
            The worker takes up to 20 seconds to connect to the queue.
          functionSignature: "Complete Deployment YAML"
          testCases:
            - input: "Worker connects to queue in 10s"
              output: "Startup probe succeeds, readiness passes when /tmp/queue-connected exists"
            - input: "Worker deadlocks, stops writing heartbeat"
              output: "Liveness exec fails, container restarts"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Workers without network ports need exec probes. The heartbeat file pattern works well for
                liveness, and a marker file for readiness.
            - title: "\U0001F4A1 Hint"
              content: >-
                Liveness: check if <code>/tmp/alive</code> was modified recently. Readiness: check if
                <code>/tmp/queue-connected</code> exists. Startup: same as readiness.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Deployment:
                  replicas: 5
                  containers:
                  - name: worker (no ports)
                    resources
                    startupProbe: exec test -f /tmp/queue-connected
                    livenessProbe: exec check heartbeat freshness
                    readinessProbe: exec test -f /tmp/queue-connected</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: job-worker
            spec:
              replicas: 5
              selector:
                matchLabels:
                  app: job-worker
              template:
                metadata:
                  labels:
                    app: job-worker
                spec:
                  containers:
                  - name: worker
                    image: job-worker:3.0
                    resources:
                      requests:
                        cpu: 200m
                        memory: 256Mi
                      limits:
                        cpu: "1"
                        memory: 512Mi
                    startupProbe:
                      exec:
                        command:
                        - sh
                        - -c
                        - "test -f /tmp/queue-connected"
                      periodSeconds: 5
                      failureThreshold: 4     # 5 * 4 = 20s
                    livenessProbe:
                      exec:
                        command:
                        - sh
                        - -c
                        - "find /tmp/alive -mmin -1 | grep -q alive"
                      periodSeconds: 30
                      timeoutSeconds: 5
                      failureThreshold: 3
                    readinessProbe:
                      exec:
                        command:
                        - sh
                        - -c
                        - "test -f /tmp/queue-connected"
                      periodSeconds: 5
                      failureThreshold: 3
          difficulty: 3
        - id: v6
          title: Debug Observability Commands
          description: >-
            A Pod named <code>web-abc12</code> (Deployment <code>web</code>, label <code>app=web</code>) is
            experiencing issues. Write the commands to: (1) check current resource usage, (2) view recent
            events, (3) get the last 50 log lines, (4) stream logs from all web Pods, (5) check if the Pod
            is in Service endpoints.
          functionSignature: "kubectl commands for debugging"
          testCases:
            - input: "Pod is in CrashLoopBackOff"
              output: "Events show reason, --previous logs show crash cause, kubectl top shows resource usage"
            - input: "Pod is Running but not receiving traffic"
              output: "Events show readiness probe failures, endpoints command shows Pod is missing"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Debugging Kubernetes issues requires checking multiple data sources: metrics, events, logs,
                and endpoint status.
            - title: "\U0001F4A1 Hint"
              content: >-
                Use <code>kubectl top</code> for metrics, <code>kubectl get events</code> for events,
                <code>kubectl logs</code> for logs, and <code>kubectl get endpoints</code> for Service
                endpoints.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. kubectl top pod web-abc12
                2. kubectl get events --field-selector ...
                3. kubectl logs web-abc12 --tail=50
                4. kubectl logs -l app=web -f
                5. kubectl get endpoints web</pre>
          solution: |-
            # 1. Check current resource usage
            kubectl top pod web-abc12

            # 2. View recent events for the Pod
            kubectl get events --field-selector involvedObject.name=web-abc12 --sort-by=.lastTimestamp

            # 3. Get the last 50 log lines
            kubectl logs web-abc12 --tail=50

            # 4. Stream logs from all web Pods
            kubectl logs -l app=web -f --all-containers

            # 5. Check if Pod is in Service endpoints
            kubectl get endpoints web
          difficulty: 2
        - id: v7
          title: Full Stack Observability Manifest
          description: >-
            Write a complete YAML manifest for a production microservice: Deployment (<code>payment-service:2.1</code>,
            3 replicas, port 8080, metrics port 9090), Service (ClusterIP), and all three probes. The app
            takes up to 60 seconds to start, exposes <code>/healthz</code>, <code>/ready</code>, and
            <code>/metrics</code>. Include resource requests, limits, and labels for monitoring.
          functionSignature: "Complete Deployment + Service YAML"
          testCases:
            - input: "Prometheus ServiceMonitor targeting port 'metrics'"
              output: "Metrics are scraped from port 9090 using the named port"
            - input: "Rolling update, new Pod takes 50 seconds to start"
              output: "Startup probe allows 60s, readiness prevents premature traffic, zero downtime"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                A production-ready manifest includes: labels for selection, named ports for Services and
                monitoring, resource management, and comprehensive probes.
            - title: "\U0001F4A1 Hint"
              content: >-
                Name your ports (<code>http</code>, <code>metrics</code>) so Services and monitoring tools
                can reference them. Include both resource requests and limits for proper scheduling and OOM
                protection.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Deployment:
                  labels: app, version
                  containers:
                    ports: http (8080), metrics (9090)
                    resources: requests + limits
                    startupProbe, livenessProbe, readinessProbe
                ---
                Service:
                  ClusterIP, selector, port mapping</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: payment-service
              labels:
                app: payment-service
                version: "2.1"
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: payment-service
              template:
                metadata:
                  labels:
                    app: payment-service
                    version: "2.1"
                spec:
                  containers:
                  - name: payment
                    image: payment-service:2.1
                    ports:
                    - name: http
                      containerPort: 8080
                    - name: metrics
                      containerPort: 9090
                    resources:
                      requests:
                        cpu: 200m
                        memory: 256Mi
                      limits:
                        cpu: "1"
                        memory: 512Mi
                    startupProbe:
                      httpGet:
                        path: /healthz
                        port: http
                      periodSeconds: 5
                      failureThreshold: 12    # 5 * 12 = 60s
                    livenessProbe:
                      httpGet:
                        path: /healthz
                        port: http
                      periodSeconds: 10
                      timeoutSeconds: 3
                      failureThreshold: 3
                    readinessProbe:
                      httpGet:
                        path: /ready
                        port: http
                      periodSeconds: 5
                      timeoutSeconds: 3
                      failureThreshold: 3
            ---
            apiVersion: v1
            kind: Service
            metadata:
              name: payment-service
              labels:
                app: payment-service
            spec:
              type: ClusterIP
              selector:
                app: payment-service
              ports:
              - name: http
                port: 80
                targetPort: http
          difficulty: 3
