conceptLinks:
  Deployments: "#lesson-deployments"
  Rolling Updates: "#lesson-rolling-updates"
  Rollback: "#lesson-rollback"
  DaemonSets: "#lesson-daemonsets"
  StatefulSets: "#lesson-statefulsets"
  Jobs: "#lesson-jobs"
  CronJobs: "#lesson-cronjobs"
  Controller Comparison: "#lesson-controller-comparison"
sharedContent: {}
variants:
  warmups:
    - id: warmup_1
      concept: Deployments
      variants:
        - id: v1
          title: Basic Nginx Deployment
          description: >-
            Write a Deployment YAML named <code>web</code> with <strong>3 replicas</strong> running the
            <code>nginx:1.25</code> image. Use the label <code>app: web</code>.
          hints:
            - "Use <code>apiVersion: apps/v1</code> and <code>kind: Deployment</code>."
            - "The <code>selector.matchLabels</code> must match the Pod template labels."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: web
              labels:
                app: web
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: web
              template:
                metadata:
                  labels:
                    app: web
                spec:
                  containers:
                  - name: nginx
                    image: nginx:1.25
                    ports:
                    - containerPort: 80
        - id: v2
          title: Single-Replica API Deployment
          description: >-
            Write a Deployment YAML named <code>api-server</code> with <strong>1 replica</strong> running
            <code>myapp/api:v2.1</code> on port <strong>8080</strong>. Label it <code>app: api</code> and
            <code>tier: backend</code>.
          hints:
            - "Include both labels in <code>metadata.labels</code>, <code>selector.matchLabels</code>, and <code>template.metadata.labels</code>."
            - "Set <code>containerPort: 8080</code> in the container spec."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: api-server
              labels:
                app: api
                tier: backend
            spec:
              replicas: 1
              selector:
                matchLabels:
                  app: api
                  tier: backend
              template:
                metadata:
                  labels:
                    app: api
                    tier: backend
                spec:
                  containers:
                  - name: api
                    image: myapp/api:v2.1
                    ports:
                    - containerPort: 8080
        - id: v3
          title: Deployment with Resource Limits
          description: >-
            Write a Deployment named <code>worker</code> with <strong>5 replicas</strong> running
            <code>myapp/worker:latest</code>. Set resource requests to <code>100m</code> CPU and
            <code>128Mi</code> memory, and limits to <code>250m</code> CPU and <code>256Mi</code> memory.
          hints:
            - "Place <code>resources</code> inside the container spec with <code>requests</code> and <code>limits</code> sub-fields."
            - "CPU is specified in millicores (e.g., <code>100m</code>) and memory in mebibytes (e.g., <code>128Mi</code>)."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: worker
              labels:
                app: worker
            spec:
              replicas: 5
              selector:
                matchLabels:
                  app: worker
              template:
                metadata:
                  labels:
                    app: worker
                spec:
                  containers:
                  - name: worker
                    image: myapp/worker:latest
                    resources:
                      requests:
                        cpu: 100m
                        memory: 128Mi
                      limits:
                        cpu: 250m
                        memory: 256Mi
        - id: v4
          title: Deployment with Environment Variables
          description: >-
            Write a Deployment named <code>frontend</code> with <strong>2 replicas</strong> running
            <code>myapp/frontend:v1.0</code> on port <strong>3000</strong>. Add environment variables
            <code>NODE_ENV=production</code> and <code>API_URL=http://api:8080</code>.
          hints:
            - "Add an <code>env</code> list under the container spec with <code>name</code> and <code>value</code> pairs."
            - "Environment variables go inside the container definition, not at the Pod level."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: frontend
              labels:
                app: frontend
            spec:
              replicas: 2
              selector:
                matchLabels:
                  app: frontend
              template:
                metadata:
                  labels:
                    app: frontend
                spec:
                  containers:
                  - name: frontend
                    image: myapp/frontend:v1.0
                    ports:
                    - containerPort: 3000
                    env:
                    - name: NODE_ENV
                      value: "production"
                    - name: API_URL
                      value: "http://api:8080"
        - id: v5
          title: Deployment with Recreate Strategy
          description: >-
            Write a Deployment named <code>legacy-app</code> with <strong>2 replicas</strong> running
            <code>myapp/legacy:v3</code>. Use the <strong>Recreate</strong> strategy (not RollingUpdate).
          hints:
            - "Set <code>strategy.type: Recreate</code>. No <code>rollingUpdate</code> sub-fields are needed."
            - "Recreate kills all old Pods before starting new ones — there will be downtime during updates."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: legacy-app
              labels:
                app: legacy-app
            spec:
              replicas: 2
              selector:
                matchLabels:
                  app: legacy-app
              strategy:
                type: Recreate
              template:
                metadata:
                  labels:
                    app: legacy-app
                spec:
                  containers:
                  - name: legacy
                    image: myapp/legacy:v3
                    ports:
                    - containerPort: 8080
        - id: v6
          title: Deployment in a Namespace
          description: >-
            Write a Deployment named <code>cache</code> in the namespace <code>staging</code> with
            <strong>3 replicas</strong> running <code>redis:7.2</code> on port <strong>6379</strong>.
          hints:
            - "Add <code>namespace: staging</code> under <code>metadata</code>."
            - "The namespace must already exist for the Deployment to be created."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: cache
              namespace: staging
              labels:
                app: cache
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: cache
              template:
                metadata:
                  labels:
                    app: cache
                spec:
                  containers:
                  - name: redis
                    image: redis:7.2
                    ports:
                    - containerPort: 6379
        - id: v7
          title: Deployment with Multiple Containers
          description: >-
            Write a Deployment named <code>app</code> with <strong>2 replicas</strong> that runs two containers:
            <code>myapp/web:v1</code> on port 8080 and a sidecar <code>myapp/log-agent:v1</code> that mounts
            a shared <code>emptyDir</code> volume at <code>/var/log/app</code>.
          hints:
            - "Define two containers in the <code>containers</code> list."
            - "Use a <code>volumes</code> entry with <code>emptyDir: {}</code> and mount it in both containers via <code>volumeMounts</code>."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: app
              labels:
                app: app
            spec:
              replicas: 2
              selector:
                matchLabels:
                  app: app
              template:
                metadata:
                  labels:
                    app: app
                spec:
                  containers:
                  - name: web
                    image: myapp/web:v1
                    ports:
                    - containerPort: 8080
                    volumeMounts:
                    - name: logs
                      mountPath: /var/log/app
                  - name: log-agent
                    image: myapp/log-agent:v1
                    volumeMounts:
                    - name: logs
                      mountPath: /var/log/app
                  volumes:
                  - name: logs
                    emptyDir: {}
        - id: v8
          title: Deployment with Liveness Probe
          description: >-
            Write a Deployment named <code>healthcheck-app</code> with <strong>3 replicas</strong> running
            <code>myapp/api:v2</code> on port 8080. Add an HTTP liveness probe that checks
            <code>/healthz</code> every 10 seconds with an initial delay of 5 seconds.
          hints:
            - "Add a <code>livenessProbe</code> with <code>httpGet</code>, <code>path</code>, <code>port</code>, <code>periodSeconds</code>, and <code>initialDelaySeconds</code>."
            - "The probe goes inside the container spec, not at the Pod level."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: healthcheck-app
              labels:
                app: healthcheck-app
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: healthcheck-app
              template:
                metadata:
                  labels:
                    app: healthcheck-app
                spec:
                  containers:
                  - name: api
                    image: myapp/api:v2
                    ports:
                    - containerPort: 8080
                    livenessProbe:
                      httpGet:
                        path: /healthz
                        port: 8080
                      initialDelaySeconds: 5
                      periodSeconds: 10
        - id: v9
          title: Deployment with ConfigMap Volume
          description: >-
            Write a Deployment named <code>nginx-custom</code> with <strong>2 replicas</strong> running
            <code>nginx:1.25</code>. Mount a ConfigMap named <code>nginx-config</code> as a volume at
            <code>/etc/nginx/conf.d</code>.
          hints:
            - "Define a volume with <code>configMap.name: nginx-config</code> in the Pod spec."
            - "Mount the volume in the container using <code>volumeMounts</code>."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: nginx-custom
              labels:
                app: nginx-custom
            spec:
              replicas: 2
              selector:
                matchLabels:
                  app: nginx-custom
              template:
                metadata:
                  labels:
                    app: nginx-custom
                spec:
                  containers:
                  - name: nginx
                    image: nginx:1.25
                    ports:
                    - containerPort: 80
                    volumeMounts:
                    - name: config
                      mountPath: /etc/nginx/conf.d
                  volumes:
                  - name: config
                    configMap:
                      name: nginx-config
        - id: v10
          title: Deployment with Revision History Limit
          description: >-
            Write a Deployment named <code>webapp</code> with <strong>4 replicas</strong> running
            <code>myapp/web:v3</code>. Set <code>revisionHistoryLimit</code> to <strong>5</strong> and
            <code>minReadySeconds</code> to <strong>10</strong>.
          hints:
            - "<code>revisionHistoryLimit</code> controls how many old ReplicaSets are kept for rollback."
            - "<code>minReadySeconds</code> specifies how long a new Pod must be Ready before it is considered Available."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: webapp
              labels:
                app: webapp
            spec:
              replicas: 4
              revisionHistoryLimit: 5
              minReadySeconds: 10
              selector:
                matchLabels:
                  app: webapp
              template:
                metadata:
                  labels:
                    app: webapp
                spec:
                  containers:
                  - name: web
                    image: myapp/web:v3
                    ports:
                    - containerPort: 8080
    - id: warmup_2
      concept: Rolling Updates
      variants:
        - id: v1
          title: Zero-Downtime Rolling Update
          description: >-
            Write the <code>strategy</code> section of a Deployment that ensures <strong>zero downtime</strong>
            during rolling updates. Never allow fewer than the desired number of Pods to be running.
          hints:
            - "Set <code>maxUnavailable: 0</code> so no Pods go down during the update."
            - "Set <code>maxSurge: 1</code> to allow one extra Pod during the transition."
          solution: |-
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: 1
                maxUnavailable: 0
        - id: v2
          title: Fast Rolling Update
          description: >-
            Write the <code>strategy</code> section for a Deployment with <strong>10 replicas</strong> that
            should update as fast as possible. Allow up to 3 extra Pods and up to 2 Pods to be unavailable
            during the update.
          hints:
            - "Higher <code>maxSurge</code> and <code>maxUnavailable</code> values speed up the rollout."
            - "With <code>maxSurge: 3</code> you can have up to 13 Pods during update; with <code>maxUnavailable: 2</code> as few as 8."
          solution: |-
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: 3
                maxUnavailable: 2
        - id: v3
          title: Percentage-Based Rolling Update
          description: >-
            Write the <code>strategy</code> section using <strong>percentage values</strong>: allow a maximum
            surge of <strong>25%</strong> and maximum unavailability of <strong>25%</strong>.
          hints:
            - "Percentages are specified as strings like <code>\"25%\"</code>."
            - "For 4 replicas, 25% maxSurge = ceil(1.0) = 1 extra Pod; 25% maxUnavailable = floor(1.0) = 1 Pod can be down."
          solution: |-
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: "25%"
                maxUnavailable: "25%"
        - id: v4
          title: Conservative Rolling Update with minReadySeconds
          description: >-
            Write a Deployment <code>spec</code> section (without the full template) that uses a rolling update
            with <code>maxSurge: 1</code>, <code>maxUnavailable: 0</code>, and waits <strong>30 seconds</strong>
            after a Pod is Ready before considering it Available.
          hints:
            - "Use <code>minReadySeconds: 30</code> at the Deployment spec level."
            - "This gives the new Pod 30 seconds to warm up and prove it is stable before continuing the rollout."
          solution: |-
            spec:
              replicas: 3
              minReadySeconds: 30
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxSurge: 1
                  maxUnavailable: 0
        - id: v5
          title: One-at-a-Time Rolling Update
          description: >-
            Write the <code>strategy</code> section for a Deployment where only <strong>one Pod</strong> is
            replaced at a time and there is always at least the desired number of Pods running.
          hints:
            - "<code>maxSurge: 1</code> creates one new Pod at a time."
            - "<code>maxUnavailable: 0</code> means no old Pod is terminated until its replacement is Ready."
          solution: |-
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: 1
                maxUnavailable: 0
        - id: v6
          title: Blue-Green Style via Recreate
          description: >-
            Write the <code>strategy</code> section for a Deployment that should tear down all old Pods before
            starting new ones. This is needed when old and new versions cannot coexist.
          hints:
            - "Use <code>type: Recreate</code> instead of <code>RollingUpdate</code>."
            - "The Recreate strategy does not accept <code>maxSurge</code> or <code>maxUnavailable</code>."
          solution: |-
            strategy:
              type: Recreate
        - id: v7
          title: Aggressive Rolling Update for Large Deployment
          description: >-
            Write the <code>strategy</code> section for a Deployment with <strong>50 replicas</strong>. Allow
            a surge of <strong>50%</strong> and unavailability of <strong>10%</strong> for a fast update that
            keeps most Pods running.
          hints:
            - "50% maxSurge of 50 replicas means up to 75 Pods during the update."
            - "10% maxUnavailable means at least 45 Pods must remain available."
          solution: |-
            strategy:
              type: RollingUpdate
              rollingUpdate:
                maxSurge: "50%"
                maxUnavailable: "10%"
        - id: v8
          title: Rolling Update with Revision History
          description: >-
            Write the Deployment <code>spec</code> fields (without the full template) that configure a rolling
            update with <code>maxSurge: 2</code>, <code>maxUnavailable: 1</code>, and keeps only the last
            <strong>3 revisions</strong> for rollback.
          hints:
            - "Use <code>revisionHistoryLimit: 3</code> to limit how many old ReplicaSets are kept."
            - "Lower values save cluster resources but reduce rollback options."
          solution: |-
            spec:
              replicas: 5
              revisionHistoryLimit: 3
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxSurge: 2
                  maxUnavailable: 1
        - id: v9
          title: Complete Deployment with Rolling Update Strategy
          description: >-
            Write a complete Deployment named <code>payment-service</code> with <strong>4 replicas</strong>
            running <code>myapp/payment:v3.1</code> on port 8443. Configure a zero-downtime rolling update
            with <code>minReadySeconds: 15</code>.
          hints:
            - "Combine <code>maxSurge: 1</code>, <code>maxUnavailable: 0</code>, and <code>minReadySeconds: 15</code>."
            - "This is a production-safe configuration: always maintain full capacity, wait for Pods to stabilize."
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: payment-service
              labels:
                app: payment-service
            spec:
              replicas: 4
              minReadySeconds: 15
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxSurge: 1
                  maxUnavailable: 0
              selector:
                matchLabels:
                  app: payment-service
              template:
                metadata:
                  labels:
                    app: payment-service
                spec:
                  containers:
                  - name: payment
                    image: myapp/payment:v3.1
                    ports:
                    - containerPort: 8443
        - id: v10
          title: DaemonSet Update Strategy
          description: >-
            Write the <code>updateStrategy</code> section for a DaemonSet that updates <strong>one node at
            a time</strong> using a rolling update.
          hints:
            - "DaemonSets use <code>updateStrategy</code> instead of <code>strategy</code>."
            - "DaemonSets only support <code>maxUnavailable</code> — there is no <code>maxSurge</code> because you cannot run two DaemonSet Pods on the same node."
          solution: |-
            updateStrategy:
              type: RollingUpdate
              rollingUpdate:
                maxUnavailable: 1
        - id: v11
          title: DaemonSet OnDelete Strategy
          description: >-
            Write the <code>updateStrategy</code> section for a DaemonSet that only updates a Pod when you
            manually delete it.
          hints:
            - "Use <code>type: OnDelete</code> instead of <code>RollingUpdate</code>."
            - "OnDelete gives full control over when each node is updated."
          solution: |-
            updateStrategy:
              type: OnDelete
    - id: warmup_3
      concept: Rollback
      variants:
        - id: v1
          title: Check Rollout Status
          description: >-
            Write the <code>kubectl</code> command to check the rollout status of a Deployment named
            <code>web</code>.
          hints:
            - "Use <code>kubectl rollout status</code> followed by the resource type and name."
          solution: |-
            kubectl rollout status deployment web
        - id: v2
          title: View Rollout History
          description: >-
            Write the <code>kubectl</code> command to view the revision history of a Deployment named
            <code>api</code>.
          hints:
            - "Use <code>kubectl rollout history</code> followed by the resource type and name."
          solution: |-
            kubectl rollout history deployment api
        - id: v3
          title: Inspect a Specific Revision
          description: >-
            Write the <code>kubectl</code> command to see the details of <strong>revision 3</strong> for a
            Deployment named <code>web</code>.
          hints:
            - "Add the <code>--revision</code> flag to <code>kubectl rollout history</code>."
          solution: |-
            kubectl rollout history deployment web --revision=3
        - id: v4
          title: Rollback to Previous Revision
          description: >-
            Write the <code>kubectl</code> command to roll back a Deployment named <code>api</code> to its
            <strong>previous</strong> revision.
          hints:
            - "Use <code>kubectl rollout undo</code> without specifying a revision number."
          solution: |-
            kubectl rollout undo deployment api
        - id: v5
          title: Rollback to a Specific Revision
          description: >-
            Write the <code>kubectl</code> command to roll back a Deployment named <code>web</code> to
            <strong>revision 2</strong>.
          hints:
            - "Use <code>kubectl rollout undo</code> with the <code>--to-revision</code> flag."
          solution: |-
            kubectl rollout undo deployment web --to-revision=2
        - id: v6
          title: Pause a Rollout
          description: >-
            Write the <code>kubectl</code> command to pause an in-progress rollout of a Deployment named
            <code>webapp</code>.
          hints:
            - "Use <code>kubectl rollout pause</code> to freeze the rollout partway through."
          solution: |-
            kubectl rollout pause deployment webapp
        - id: v7
          title: Resume a Paused Rollout
          description: >-
            Write the <code>kubectl</code> command to resume a paused rollout of a Deployment named
            <code>webapp</code>.
          hints:
            - "Use <code>kubectl rollout resume</code> to continue a paused rollout."
          solution: |-
            kubectl rollout resume deployment webapp
        - id: v8
          title: Trigger a Rolling Update
          description: >-
            Write the <code>kubectl</code> command to update a Deployment named <code>web</code> to use
            the image <code>nginx:1.26</code> (container name is <code>nginx</code>).
          hints:
            - "Use <code>kubectl set image</code> with the format <code>deployment/name container=image</code>."
          solution: |-
            kubectl set image deployment/web nginx=nginx:1.26
        - id: v9
          title: Annotate Change Cause
          description: >-
            Write the <code>kubectl</code> command to annotate a Deployment named <code>web</code> with a
            change cause of <code>"Update nginx to 1.26"</code> so it appears in rollout history.
          hints:
            - "Use <code>kubectl annotate</code> with the key <code>kubernetes.io/change-cause</code>."
          solution: |-
            kubectl annotate deployment web kubernetes.io/change-cause="Update nginx to 1.26"
        - id: v10
          title: Scale a Deployment
          description: >-
            Write the <code>kubectl</code> command to scale a Deployment named <code>api</code> to
            <strong>5 replicas</strong>.
          hints:
            - "Use <code>kubectl scale</code> with the <code>--replicas</code> flag."
            - "Scaling does not create a new revision — it just adjusts the existing ReplicaSet."
          solution: |-
            kubectl scale deployment api --replicas=5
        - id: v11
          title: Verify Current Image After Rollback
          description: >-
            Write the <code>kubectl</code> command to check which image the first container of a Deployment
            named <code>web</code> is currently using.
          hints:
            - "Use <code>kubectl get deploy</code> with <code>-o jsonpath</code> to extract the image field."
          solution: |-
            kubectl get deploy web -o jsonpath='{.spec.template.spec.containers[0].image}'
        - id: v12
          title: Restart All Pods in a Deployment
          description: >-
            Write the <code>kubectl</code> command to trigger a rolling restart of all Pods in a Deployment
            named <code>web</code> without changing the image.
          hints:
            - "Use <code>kubectl rollout restart</code> to trigger a fresh rollout with the same spec."
          solution: |-
            kubectl rollout restart deployment web
    - id: warmup_4
      concept: Jobs
      variants:
        - id: v1
          title: Basic Job
          description: >-
            Write a Job YAML named <code>data-migration</code> that runs <code>myapp/migrate:v2</code> with
            the command <code>["python", "migrate.py"]</code>. Use <code>restartPolicy: Never</code>.
          hints:
            - "Use <code>apiVersion: batch/v1</code> and <code>kind: Job</code>."
            - "Jobs require <code>restartPolicy: Never</code> or <code>restartPolicy: OnFailure</code> in the Pod template."
          solution: |-
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: data-migration
            spec:
              template:
                spec:
                  restartPolicy: Never
                  containers:
                  - name: migrate
                    image: myapp/migrate:v2
                    command: ["python", "migrate.py"]
        - id: v2
          title: Job with Completions and Parallelism
          description: >-
            Write a Job named <code>batch-process</code> that must complete <strong>10 tasks</strong> running
            <strong>3 Pods in parallel</strong>. Use <code>myapp/processor:v1</code> with restart policy
            <code>OnFailure</code>.
          hints:
            - "Set <code>completions: 10</code> for the total number of successful completions needed."
            - "Set <code>parallelism: 3</code> to run 3 Pods at a time."
          solution: |-
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: batch-process
            spec:
              completions: 10
              parallelism: 3
              template:
                spec:
                  restartPolicy: OnFailure
                  containers:
                  - name: processor
                    image: myapp/processor:v1
        - id: v3
          title: Job with Backoff Limit and Timeout
          description: >-
            Write a Job named <code>report-gen</code> running <code>myapp/report:v1</code> that retries up
            to <strong>3 times</strong> on failure and has a hard timeout of <strong>600 seconds</strong>.
          hints:
            - "Use <code>backoffLimit: 3</code> for the retry count."
            - "Use <code>activeDeadlineSeconds: 600</code> for the timeout. Whichever triggers first marks the Job as Failed."
          solution: |-
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: report-gen
            spec:
              backoffLimit: 3
              activeDeadlineSeconds: 600
              template:
                spec:
                  restartPolicy: Never
                  containers:
                  - name: report
                    image: myapp/report:v1
        - id: v4
          title: Parallel Work Queue Job
          description: >-
            Write a Job named <code>queue-worker</code> that runs <strong>5 Pods in parallel</strong> to drain
            a work queue. Do not set <code>completions</code> — the Pods themselves decide when to stop.
            Use <code>myapp/queue-worker:v2</code>.
          hints:
            - "Omitting <code>completions</code> (or setting it to null) creates a work-queue-style Job."
            - "Set <code>parallelism: 5</code> to run 5 Pods concurrently. The Job completes when any Pod succeeds."
          solution: |-
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: queue-worker
            spec:
              parallelism: 5
              template:
                spec:
                  restartPolicy: Never
                  containers:
                  - name: worker
                    image: myapp/queue-worker:v2
        - id: v5
          title: CronJob — Daily at 2 AM
          description: >-
            Write a CronJob named <code>db-backup</code> that runs daily at <strong>2:00 AM</strong>. Use
            <code>myapp/db-backup:latest</code> with <code>restartPolicy: OnFailure</code>.
          hints:
            - "Use <code>apiVersion: batch/v1</code> and <code>kind: CronJob</code>."
            - "The cron expression for daily at 2 AM is <code>\"0 2 * * *\"</code>."
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: db-backup
            spec:
              schedule: "0 2 * * *"
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: backup
                        image: myapp/db-backup:latest
        - id: v6
          title: CronJob — Every 15 Minutes
          description: >-
            Write a CronJob named <code>health-check</code> that runs <strong>every 15 minutes</strong>.
            Use <code>myapp/health-checker:v1</code> and set <code>concurrencyPolicy: Forbid</code>.
          hints:
            - "The cron expression for every 15 minutes is <code>\"*/15 * * * *\"</code>."
            - "<code>concurrencyPolicy: Forbid</code> prevents a new Job from starting if the previous one is still running."
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: health-check
            spec:
              schedule: "*/15 * * * *"
              concurrencyPolicy: Forbid
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: checker
                        image: myapp/health-checker:v1
        - id: v7
          title: CronJob — Weekly Sunday Midnight
          description: >-
            Write a CronJob named <code>weekly-report</code> that runs every <strong>Sunday at midnight</strong>.
            Use <code>myapp/reporter:v2</code>. Keep only the last <strong>3</strong> successful Jobs and
            <strong>1</strong> failed Job.
          hints:
            - "The cron expression for Sunday at midnight is <code>\"0 0 * * 0\"</code>."
            - "Use <code>successfulJobsHistoryLimit: 3</code> and <code>failedJobsHistoryLimit: 1</code>."
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: weekly-report
            spec:
              schedule: "0 0 * * 0"
              successfulJobsHistoryLimit: 3
              failedJobsHistoryLimit: 1
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: reporter
                        image: myapp/reporter:v2
        - id: v8
          title: CronJob with Starting Deadline
          description: >-
            Write a CronJob named <code>cleanup</code> that runs daily at <strong>3:00 AM</strong>. If the
            job is missed by more than <strong>200 seconds</strong>, skip it. Use <code>busybox</code>
            with command <code>["/bin/sh", "-c", "find /tmp -mtime +7 -delete"]</code>.
          hints:
            - "Use <code>startingDeadlineSeconds: 200</code> to skip missed runs."
            - "Always set <code>startingDeadlineSeconds</code> to avoid the 100-missed-schedules edge case."
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: cleanup
            spec:
              schedule: "0 3 * * *"
              startingDeadlineSeconds: 200
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: cleanup
                        image: busybox
                        command: ["/bin/sh", "-c", "find /tmp -mtime +7 -delete"]
        - id: v9
          title: CronJob with Replace Policy
          description: >-
            Write a CronJob named <code>metrics-sync</code> that runs <strong>every 5 minutes</strong> using
            <code>myapp/metrics-sync:v1</code>. If a previous run is still active, <strong>replace</strong>
            it with the new run.
          hints:
            - "Use <code>concurrencyPolicy: Replace</code> to kill the running Job and start a new one."
            - "The cron expression for every 5 minutes is <code>\"*/5 * * * *\"</code>."
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: metrics-sync
            spec:
              schedule: "*/5 * * * *"
              concurrencyPolicy: Replace
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: sync
                        image: myapp/metrics-sync:v1
        - id: v10
          title: CronJob — Weekdays at 8:30 AM
          description: >-
            Write a CronJob named <code>morning-digest</code> that runs on <strong>weekdays at 8:30 AM</strong>.
            Use <code>myapp/digest:v1</code> with a <code>backoffLimit</code> of <strong>2</strong> on the
            inner Job spec.
          hints:
            - "The cron expression for weekdays at 8:30 AM is <code>\"30 8 * * 1-5\"</code>."
            - "<code>backoffLimit</code> goes inside <code>jobTemplate.spec</code>, not the CronJob spec."
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: morning-digest
            spec:
              schedule: "30 8 * * 1-5"
              jobTemplate:
                spec:
                  backoffLimit: 2
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: digest
                        image: myapp/digest:v1
        - id: v11
          title: Suspended CronJob
          description: >-
            Write a CronJob named <code>paused-task</code> that is scheduled for <strong>every hour</strong>
            but is currently <strong>suspended</strong>. Use <code>myapp/task:v1</code>.
          hints:
            - "Set <code>suspend: true</code> to temporarily disable the schedule without deleting the CronJob."
            - "You can resume later with <code>kubectl patch cronjob paused-task -p '{\"spec\":{\"suspend\":false}}'</code>."
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: paused-task
            spec:
              schedule: "0 * * * *"
              suspend: true
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: task
                        image: myapp/task:v1
  challenges:
    - id: challenge_1
      block: 1
      difficulty: 2
      concept: Controller Comparison
      variants:
        - id: v1
          title: "Scenario: Web Application"
          description: >-
            Your team needs to deploy a stateless web application that serves HTTP traffic. It needs 5
            replicas, rolling updates with zero downtime, and the ability to roll back to a previous version.
            <br><br>Which workload controller should you use? Write the minimal YAML (metadata + spec outline)
            for the correct controller.
          functionSignature: "Deployment"
          testCases:
            - input: "Stateless web app, 5 replicas, rolling updates, rollback capability"
              output: "Deployment — manages ReplicaSets for rolling updates and rollback"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Is this workload stateless or stateful? Does it need to run on every node or just N replicas? Does it run forever or to completion?"
            - title: "\U0001F4A1 Hint"
              content: "Stateless + N replicas + rolling updates = Deployment. Deployments manage ReplicaSets which provide self-healing, scaling, and revision history."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Stateless long-running → Deployment
                One per node → DaemonSet
                Stable identity/storage → StatefulSet
                Run-to-completion → Job
                Scheduled → CronJob</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: web-app
              labels:
                app: web-app
            spec:
              replicas: 5
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxSurge: 1
                  maxUnavailable: 0
              selector:
                matchLabels:
                  app: web-app
              template:
                metadata:
                  labels:
                    app: web-app
                spec:
                  containers:
                  - name: web
                    image: myapp/web:v1
                    ports:
                    - containerPort: 8080
          difficulty: 1
        - id: v2
          title: "Scenario: Log Collector Agent"
          description: >-
            You need to run a Fluentd log collection agent on <strong>every node</strong> in the cluster,
            including control-plane nodes. When a new node is added, it should automatically get a log
            collector.<br><br>Which controller should you use? Write the YAML.
          functionSignature: "DaemonSet"
          testCases:
            - input: "Log collector, one per node, auto-schedule on new nodes"
              output: "DaemonSet — ensures exactly one Pod runs on every (or selected) node"
          hints:
            - title: "\U0001F914 Think about it"
              content: "You need exactly one Pod per node, not N replicas. Which controller guarantees one Pod per node?"
            - title: "\U0001F4A1 Hint"
              content: "DaemonSets run one Pod per node. Add a toleration for the control-plane taint so it runs on those nodes too."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>One Pod per node → DaemonSet
                Add tolerations for control-plane nodes
                Mount hostPath volumes for node-level access</pre>
          solution: |-
            apiVersion: apps/v1
            kind: DaemonSet
            metadata:
              name: fluentd
              namespace: kube-system
              labels:
                app: fluentd
            spec:
              selector:
                matchLabels:
                  app: fluentd
              template:
                metadata:
                  labels:
                    app: fluentd
                spec:
                  tolerations:
                  - key: node-role.kubernetes.io/control-plane
                    effect: NoSchedule
                  containers:
                  - name: fluentd
                    image: fluentd:v1.16
                    volumeMounts:
                    - name: varlog
                      mountPath: /var/log
                  volumes:
                  - name: varlog
                    hostPath:
                      path: /var/log
          difficulty: 1
        - id: v3
          title: "Scenario: Database Cluster"
          description: >-
            You need to deploy a 3-node PostgreSQL cluster where each node has a stable hostname
            (<code>pg-0</code>, <code>pg-1</code>, <code>pg-2</code>) and its own persistent 50Gi volume
            that survives Pod rescheduling.<br><br>Which controller should you use? Write the YAML including
            the headless Service.
          functionSignature: "StatefulSet"
          testCases:
            - input: "Database, stable hostnames (pg-0, pg-1, pg-2), per-Pod persistent storage"
              output: "StatefulSet — provides stable identity, ordered operations, and per-Pod PVCs"
          hints:
            - title: "\U0001F914 Think about it"
              content: "The workload needs stable hostnames and per-Pod storage. Which controller provides both?"
            - title: "\U0001F4A1 Hint"
              content: "StatefulSets give Pods stable names (name-0, name-1, ...) and volumeClaimTemplates for per-Pod storage. They require a headless Service."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Create headless Service (clusterIP: None)
                2. Create StatefulSet with serviceName referencing it
                3. Use volumeClaimTemplates for per-Pod storage</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: pg
              labels:
                app: postgres
            spec:
              clusterIP: None
              selector:
                app: postgres
              ports:
              - port: 5432
                name: postgres
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: pg
            spec:
              serviceName: pg
              replicas: 3
              selector:
                matchLabels:
                  app: postgres
              template:
                metadata:
                  labels:
                    app: postgres
                spec:
                  containers:
                  - name: postgres
                    image: postgres:16
                    ports:
                    - containerPort: 5432
                      name: postgres
                    volumeMounts:
                    - name: data
                      mountPath: /var/lib/postgresql/data
              volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 50Gi
          difficulty: 2
        - id: v4
          title: "Scenario: Database Migration"
          description: >-
            You need to run a one-time database migration script that should complete successfully exactly
            once. If it fails, retry up to 3 times. Timeout after 10 minutes.<br><br>Which controller
            should you use? Write the YAML.
          functionSignature: "Job"
          testCases:
            - input: "One-time migration, retries on failure, 10-minute timeout"
              output: "Job — runs Pods to completion with retry and timeout support"
          hints:
            - title: "\U0001F914 Think about it"
              content: "This is a run-to-completion task, not a long-running service. It runs once, not on a schedule."
            - title: "\U0001F4A1 Hint"
              content: "Use a Job with <code>backoffLimit: 3</code> for retries and <code>activeDeadlineSeconds: 600</code> for the 10-minute timeout."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Run once → Job (not CronJob)
                Retries → backoffLimit
                Timeout → activeDeadlineSeconds
                restartPolicy must be Never or OnFailure</pre>
          solution: |-
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: db-migration
            spec:
              backoffLimit: 3
              activeDeadlineSeconds: 600
              template:
                spec:
                  restartPolicy: Never
                  containers:
                  - name: migrate
                    image: myapp/migrate:v2
                    command: ["python", "migrate.py"]
          difficulty: 1
        - id: v5
          title: "Scenario: Nightly Backup"
          description: >-
            You need to back up a database every night at 1:00 AM. The backup should not overlap with
            itself if it runs long. Keep the last 5 successful Jobs and last 2 failed Jobs.<br><br>Which
            controller should you use? Write the YAML.
          functionSignature: "CronJob"
          testCases:
            - input: "Scheduled nightly backup, no overlaps, retain history"
              output: "CronJob — creates Jobs on a cron schedule with concurrency control"
          hints:
            - title: "\U0001F914 Think about it"
              content: "This is a scheduled, recurring task. Which controller creates Jobs on a schedule?"
            - title: "\U0001F4A1 Hint"
              content: "Use a CronJob with <code>concurrencyPolicy: Forbid</code> to prevent overlapping runs and history limits for cleanup."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Scheduled recurring task → CronJob
                No overlaps → concurrencyPolicy: Forbid
                History → successfulJobsHistoryLimit / failedJobsHistoryLimit</pre>
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: db-backup
            spec:
              schedule: "0 1 * * *"
              concurrencyPolicy: Forbid
              successfulJobsHistoryLimit: 5
              failedJobsHistoryLimit: 2
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: backup
                        image: myapp/db-backup:latest
          difficulty: 2
        - id: v6
          title: "Scenario: Monitoring Agent per Node"
          description: >-
            You need to deploy a Prometheus node-exporter on every worker node (but NOT on control-plane
            nodes). Each agent needs access to the host's <code>/proc</code> and <code>/sys</code>
            filesystems.<br><br>Which controller should you use? Write the YAML.
          functionSignature: "DaemonSet"
          testCases:
            - input: "One agent per worker node, hostPath access, exclude control-plane"
              output: "DaemonSet with no control-plane toleration and hostPath volumes"
          hints:
            - title: "\U0001F914 Think about it"
              content: "One per node = DaemonSet. To exclude control-plane nodes, simply do not add a toleration for their taint."
            - title: "\U0001F4A1 Hint"
              content: "DaemonSets without a control-plane toleration will skip control-plane nodes. Use <code>hostPath</code> volumes for <code>/proc</code> and <code>/sys</code>."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. DaemonSet without control-plane toleration
                2. hostPath volumes for /proc and /sys
                3. Mount them read-only in the container</pre>
          solution: |-
            apiVersion: apps/v1
            kind: DaemonSet
            metadata:
              name: node-exporter
              labels:
                app: node-exporter
            spec:
              selector:
                matchLabels:
                  app: node-exporter
              template:
                metadata:
                  labels:
                    app: node-exporter
                spec:
                  containers:
                  - name: node-exporter
                    image: prom/node-exporter:v1.7.0
                    ports:
                    - containerPort: 9100
                    volumeMounts:
                    - name: proc
                      mountPath: /host/proc
                      readOnly: true
                    - name: sys
                      mountPath: /host/sys
                      readOnly: true
                  volumes:
                  - name: proc
                    hostPath:
                      path: /proc
                  - name: sys
                    hostPath:
                      path: /sys
          difficulty: 2
        - id: v7
          title: "Scenario: Batch Image Processing"
          description: >-
            You have 100 images to process. You want to run a batch processing Job that completes all 100
            items, running 10 Pods in parallel. Retry up to 5 times on failure.<br><br>Write the Job YAML.
          functionSignature: "Job"
          testCases:
            - input: "100 completions, 10 parallel Pods, 5 retries"
              output: "Job with completions: 100, parallelism: 10, backoffLimit: 5"
          hints:
            - title: "\U0001F914 Think about it"
              content: "You need a fixed number of completions with parallel execution. Which Job configuration handles this?"
            - title: "\U0001F4A1 Hint"
              content: "Set <code>completions: 100</code> for the total tasks, <code>parallelism: 10</code> for concurrent Pods, and <code>backoffLimit: 5</code> for retries."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Fixed completion count:
                  completions: N (total tasks)
                  parallelism: M (concurrent Pods)
                  backoffLimit: X (retries)</pre>
          solution: |-
            apiVersion: batch/v1
            kind: Job
            metadata:
              name: image-processing
            spec:
              completions: 100
              parallelism: 10
              backoffLimit: 5
              template:
                spec:
                  restartPolicy: OnFailure
                  containers:
                  - name: processor
                    image: myapp/image-processor:v1
          difficulty: 2
        - id: v8
          title: "Scenario: Kafka Broker Cluster"
          description: >-
            You need to deploy a 5-node Kafka broker cluster. Each broker needs a stable network identity
            (so brokers can find each other by name), 100Gi of persistent storage per broker, and
            <strong>Parallel</strong> Pod management for faster startup.<br><br>Write the YAML.
          functionSignature: "StatefulSet"
          testCases:
            - input: "5 Kafka brokers, stable identity, 100Gi per broker, parallel startup"
              output: "StatefulSet with Parallel podManagementPolicy and volumeClaimTemplates"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Kafka brokers need stable hostnames (broker-0, broker-1, ...) and persistent storage. Which controller provides this?"
            - title: "\U0001F4A1 Hint"
              content: "Use a StatefulSet with <code>podManagementPolicy: Parallel</code> for faster startup. Each broker gets its own PVC via <code>volumeClaimTemplates</code>."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Headless Service for DNS
                2. StatefulSet with podManagementPolicy: Parallel
                3. volumeClaimTemplates for per-broker storage</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: kafka
              labels:
                app: kafka
            spec:
              clusterIP: None
              selector:
                app: kafka
              ports:
              - port: 9092
                name: broker
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: kafka
            spec:
              serviceName: kafka
              replicas: 5
              podManagementPolicy: Parallel
              selector:
                matchLabels:
                  app: kafka
              template:
                metadata:
                  labels:
                    app: kafka
                spec:
                  containers:
                  - name: kafka
                    image: confluentinc/cp-kafka:7.5
                    ports:
                    - containerPort: 9092
                      name: broker
                    volumeMounts:
                    - name: data
                      mountPath: /var/lib/kafka/data
              volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 100Gi
          difficulty: 3
    - id: challenge_2
      block: 1
      difficulty: 2
      concept: Rolling Updates
      variants:
        - id: v1
          title: "Zero-Downtime Update for Payment Service"
          description: >-
            Your payment service has <strong>6 replicas</strong>. It must <strong>never</strong> have fewer than
            6 Pods available during an update (zero downtime). Updates should proceed one Pod at a time.
            Each new Pod must be stable for <strong>20 seconds</strong> before the rollout continues.
            <br><br>Write the complete Deployment YAML with the correct strategy and settings.
          functionSignature: "Deployment"
          testCases:
            - input: "6 replicas, never below 6 available, one at a time, 20s stability check"
              output: "maxSurge: 1, maxUnavailable: 0, minReadySeconds: 20"
          hints:
            - title: "\U0001F914 Think about it"
              content: "If you can never go below 6, that means maxUnavailable must be 0. How do you add new Pods one at a time?"
            - title: "\U0001F4A1 Hint"
              content: "<code>maxUnavailable: 0</code> ensures no Pod is killed until its replacement is Ready. <code>maxSurge: 1</code> adds one Pod at a time. <code>minReadySeconds: 20</code> makes the rollout wait."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Zero-downtime pattern:
                  maxSurge: 1 (one extra Pod at a time)
                  maxUnavailable: 0 (never go below desired)
                  minReadySeconds: 20 (stability period)</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: payment-service
              labels:
                app: payment
            spec:
              replicas: 6
              minReadySeconds: 20
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxSurge: 1
                  maxUnavailable: 0
              selector:
                matchLabels:
                  app: payment
              template:
                metadata:
                  labels:
                    app: payment
                spec:
                  containers:
                  - name: payment
                    image: myapp/payment:v3.0
                    ports:
                    - containerPort: 8443
          difficulty: 2
        - id: v2
          title: "Fast Update for Dev Environment"
          description: >-
            In your development environment, speed matters more than availability. You have a Deployment with
            <strong>10 replicas</strong>. Configure the strategy to allow up to <strong>5 extra Pods</strong>
            and up to <strong>3 Pods unavailable</strong> during updates for maximum speed.
            <br><br>Write the complete Deployment YAML.
          functionSignature: "Deployment"
          testCases:
            - input: "10 replicas, 5 extra Pods allowed, 3 can be unavailable"
              output: "maxSurge: 5, maxUnavailable: 3 — up to 15 Pods during update, down to 7"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Higher maxSurge and maxUnavailable values make updates faster but sacrifice availability. What values match the requirements?"
            - title: "\U0001F4A1 Hint"
              content: "<code>maxSurge: 5</code> allows up to 15 total Pods. <code>maxUnavailable: 3</code> allows as few as 7 running Pods. This enables aggressive parallel replacement."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Fast update pattern:
                  maxSurge: high (creates many new Pods quickly)
                  maxUnavailable: high (kills old Pods quickly)
                  Trade-off: less availability during update</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: dev-app
              labels:
                app: dev-app
            spec:
              replicas: 10
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxSurge: 5
                  maxUnavailable: 3
              selector:
                matchLabels:
                  app: dev-app
              template:
                metadata:
                  labels:
                    app: dev-app
                spec:
                  containers:
                  - name: app
                    image: myapp/dev:latest
                    ports:
                    - containerPort: 8080
          difficulty: 2
        - id: v3
          title: "Singleton Workload with Recreate"
          description: >-
            You have a controller application that acquires a distributed lock. Only <strong>one instance</strong>
            can run at a time. If you deploy a new version, the old one must be fully stopped before the new
            one starts — otherwise two instances would fight for the lock.
            <br><br>Write the Deployment YAML with the correct strategy.
          functionSignature: "Deployment"
          testCases:
            - input: "1 replica, only one can run at a time, old must stop before new starts"
              output: "Recreate strategy — kills all old Pods before creating new ones"
          hints:
            - title: "\U0001F914 Think about it"
              content: "With RollingUpdate and maxSurge: 1, there would briefly be two Pods running. How do you prevent that?"
            - title: "\U0001F4A1 Hint"
              content: "The Recreate strategy terminates all existing Pods before starting new ones, ensuring no overlap."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Singleton/mutual-exclusion pattern:
                  replicas: 1
                  strategy: Recreate
                  No maxSurge/maxUnavailable needed</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: lock-controller
              labels:
                app: lock-controller
            spec:
              replicas: 1
              strategy:
                type: Recreate
              selector:
                matchLabels:
                  app: lock-controller
              template:
                metadata:
                  labels:
                    app: lock-controller
                spec:
                  containers:
                  - name: controller
                    image: myapp/lock-controller:v2
                    ports:
                    - containerPort: 8080
          difficulty: 2
        - id: v4
          title: "Percentage-Based Update for Large Fleet"
          description: >-
            You manage a Deployment with <strong>100 replicas</strong> serving production traffic. The update
            strategy should allow a surge of <strong>10%</strong> (10 extra Pods) and unavailability of
            <strong>5%</strong> (at most 5 Pods down). New Pods must be stable for <strong>30 seconds</strong>.
            Only keep the last <strong>5</strong> revisions.
            <br><br>Write the Deployment YAML.
          functionSignature: "Deployment"
          testCases:
            - input: "100 replicas, 10% surge, 5% unavailable, 30s stability, 5 revisions"
              output: "maxSurge: 10%, maxUnavailable: 5%, minReadySeconds: 30, revisionHistoryLimit: 5"
          hints:
            - title: "\U0001F914 Think about it"
              content: "For large Deployments, percentage values scale better than absolute numbers. How do percentages get calculated?"
            - title: "\U0001F4A1 Hint"
              content: "maxSurge rounds up (ceil) and maxUnavailable rounds down (floor). For 100 replicas: 10% surge = 10 extra, 5% unavailable = 5 down."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Large fleet pattern:
                  maxSurge: "10%" → ceil(100 * 0.10) = 10 extra Pods
                  maxUnavailable: "5%" → floor(100 * 0.05) = 5 Pods can be down
                  minReadySeconds: 30 (warm-up time)
                  revisionHistoryLimit: 5 (save cluster resources)</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: production-api
              labels:
                app: production-api
            spec:
              replicas: 100
              minReadySeconds: 30
              revisionHistoryLimit: 5
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxSurge: "10%"
                  maxUnavailable: "5%"
              selector:
                matchLabels:
                  app: production-api
              template:
                metadata:
                  labels:
                    app: production-api
                spec:
                  containers:
                  - name: api
                    image: myapp/api:v5.0
                    ports:
                    - containerPort: 8080
                    resources:
                      requests:
                        cpu: 200m
                        memory: 256Mi
                      limits:
                        cpu: 500m
                        memory: 512Mi
          difficulty: 3
        - id: v5
          title: "Canary via Pause/Resume"
          description: >-
            You want to perform a canary deployment. Write the sequence of <code>kubectl</code> commands to:
            <br>1. Update Deployment <code>web</code> to <code>nginx:1.26</code>
            <br>2. Immediately pause the rollout (so only a fraction of Pods update)
            <br>3. Check the rollout status
            <br>4. If healthy, resume the rollout
            <br>5. If unhealthy, undo the rollout
          functionSignature: "kubectl commands"
          testCases:
            - input: "Canary update → pause → verify → resume or undo"
              output: "set image, pause, status, resume/undo sequence"
          hints:
            - title: "\U0001F914 Think about it"
              content: "How can you stop a rolling update partway so only some Pods get the new version? What commands let you control this?"
            - title: "\U0001F4A1 Hint"
              content: "Use <code>kubectl rollout pause</code> immediately after starting the update to freeze it partway. Then <code>resume</code> or <code>undo</code> depending on health."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. kubectl set image (trigger update)
                2. kubectl rollout pause (freeze partway)
                3. kubectl rollout status (check state)
                4a. kubectl rollout resume (if healthy)
                4b. kubectl rollout undo (if unhealthy)</pre>
          solution: |-
            # Step 1: Trigger the update
            kubectl set image deployment/web nginx=nginx:1.26

            # Step 2: Immediately pause (only some Pods will have updated)
            kubectl rollout pause deployment web

            # Step 3: Check the status
            kubectl rollout status deployment web
            kubectl get rs

            # Step 4a: If healthy, resume the rollout
            kubectl rollout resume deployment web

            # Step 4b: If unhealthy, roll back
            kubectl rollout undo deployment web
          difficulty: 3
        - id: v6
          title: "DaemonSet Rolling Update Strategy"
          description: >-
            You have a DaemonSet named <code>node-monitor</code> running on 50 nodes. You need to update it
            with a rolling update that processes <strong>5 nodes at a time</strong> (maxUnavailable: 5).
            <br><br>Write the complete DaemonSet YAML with the correct update strategy.
          functionSignature: "DaemonSet"
          testCases:
            - input: "DaemonSet on 50 nodes, update 5 at a time"
              output: "updateStrategy.rollingUpdate.maxUnavailable: 5"
          hints:
            - title: "\U0001F914 Think about it"
              content: "DaemonSets use <code>updateStrategy</code> (not <code>strategy</code>) and only support <code>maxUnavailable</code>. Why no maxSurge?"
            - title: "\U0001F4A1 Hint"
              content: "DaemonSets cannot have maxSurge because only one DaemonSet Pod can run per node. During update, the old Pod is killed before the new one starts on that node."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>DaemonSet update:
                  updateStrategy (not strategy)
                  type: RollingUpdate
                  maxUnavailable only (no maxSurge)
                  Each node: kill old → start new</pre>
          solution: |-
            apiVersion: apps/v1
            kind: DaemonSet
            metadata:
              name: node-monitor
              labels:
                app: node-monitor
            spec:
              selector:
                matchLabels:
                  app: node-monitor
              updateStrategy:
                type: RollingUpdate
                rollingUpdate:
                  maxUnavailable: 5
              template:
                metadata:
                  labels:
                    app: node-monitor
                spec:
                  containers:
                  - name: monitor
                    image: myapp/node-monitor:v2
                    resources:
                      requests:
                        cpu: 50m
                        memory: 64Mi
                      limits:
                        cpu: 100m
                        memory: 128Mi
          difficulty: 3
    - id: challenge_3
      block: 2
      difficulty: 3
      concept: StatefulSets
      variants:
        - id: v1
          title: "Redis Sentinel Cluster"
          description: >-
            Deploy a 3-node Redis Sentinel cluster as a StatefulSet. Requirements:
            <br>- Name: <code>redis</code>
            <br>- Headless Service named <code>redis</code> on port 6379
            <br>- 5Gi persistent storage per Pod at <code>/data</code>
            <br>- Image: <code>redis:7.2</code>
            <br><br>Write the complete YAML (Service + StatefulSet).
          functionSignature: "StatefulSet"
          testCases:
            - input: "3-node Redis, headless Service, 5Gi per Pod"
              output: "Headless Service + StatefulSet with volumeClaimTemplates"
          hints:
            - title: "\U0001F914 Think about it"
              content: "What makes a Service 'headless'? What does the StatefulSet need to reference it?"
            - title: "\U0001F4A1 Hint"
              content: "Set <code>clusterIP: None</code> on the Service. In the StatefulSet, set <code>serviceName: redis</code> to match the headless Service name."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Service: clusterIP: None (headless)
                2. StatefulSet: serviceName must match Service name
                3. volumeClaimTemplates for per-Pod storage
                4. DNS: redis-0.redis, redis-1.redis, redis-2.redis</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: redis
              labels:
                app: redis
            spec:
              clusterIP: None
              selector:
                app: redis
              ports:
              - port: 6379
                name: redis
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: redis
            spec:
              serviceName: redis
              replicas: 3
              selector:
                matchLabels:
                  app: redis
              template:
                metadata:
                  labels:
                    app: redis
                spec:
                  containers:
                  - name: redis
                    image: redis:7.2
                    ports:
                    - containerPort: 6379
                      name: redis
                    volumeMounts:
                    - name: data
                      mountPath: /data
              volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi
          difficulty: 3
        - id: v2
          title: "Elasticsearch Cluster with Storage Class"
          description: >-
            Deploy a 3-node Elasticsearch cluster as a StatefulSet. Requirements:
            <br>- Name: <code>elasticsearch</code>
            <br>- Headless Service named <code>es</code> on ports 9200 (http) and 9300 (transport)
            <br>- 50Gi persistent storage per Pod using <code>storageClassName: fast-ssd</code>
            <br>- Mount at <code>/usr/share/elasticsearch/data</code>
            <br>- Set environment variable <code>discovery.seed_hosts=es</code>
          functionSignature: "StatefulSet"
          testCases:
            - input: "3-node ES, headless Service, 50Gi fast-ssd storage, env for discovery"
              output: "Headless Service with two ports + StatefulSet with storageClassName"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Elasticsearch needs a headless Service for node discovery. How do you specify a storage class in volumeClaimTemplates?"
            - title: "\U0001F4A1 Hint"
              content: "Add <code>storageClassName: fast-ssd</code> in the volumeClaimTemplate spec. The headless Service needs two named ports."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Headless Service with multiple ports
                2. StatefulSet with storageClassName in volumeClaimTemplates
                3. Environment variable for cluster discovery
                4. serviceName must match Service name</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: es
              labels:
                app: elasticsearch
            spec:
              clusterIP: None
              selector:
                app: elasticsearch
              ports:
              - port: 9200
                name: http
              - port: 9300
                name: transport
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: elasticsearch
            spec:
              serviceName: es
              replicas: 3
              selector:
                matchLabels:
                  app: elasticsearch
              template:
                metadata:
                  labels:
                    app: elasticsearch
                spec:
                  containers:
                  - name: elasticsearch
                    image: elasticsearch:8.11.0
                    ports:
                    - containerPort: 9200
                      name: http
                    - containerPort: 9300
                      name: transport
                    env:
                    - name: discovery.seed_hosts
                      value: "es"
                    volumeMounts:
                    - name: data
                      mountPath: /usr/share/elasticsearch/data
              volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  accessModes: ["ReadWriteOnce"]
                  storageClassName: fast-ssd
                  resources:
                    requests:
                      storage: 50Gi
          difficulty: 3
        - id: v3
          title: "MySQL Primary-Replica with Canary Update"
          description: >-
            Deploy a 3-node MySQL StatefulSet where updates happen using a <strong>partition</strong>. Set the
            partition to <strong>2</strong> so that only <code>mysql-2</code> gets the new version first
            (canary). Requirements:
            <br>- Name: <code>mysql</code>, image: <code>mysql:8.0</code>
            <br>- Headless Service on port 3306
            <br>- 20Gi storage per Pod at <code>/var/lib/mysql</code>
            <br>- Ordered pod management (default)
          functionSignature: "StatefulSet"
          testCases:
            - input: "3-node MySQL, partition: 2 for canary, ordered management"
              output: "StatefulSet with updateStrategy.rollingUpdate.partition: 2"
          hints:
            - title: "\U0001F914 Think about it"
              content: "StatefulSet partitions control which Pods get updated. If partition is 2, only Pods with ordinal >= 2 are updated. How does this enable canary testing?"
            - title: "\U0001F4A1 Hint"
              content: "With <code>partition: 2</code> and 3 replicas, only <code>mysql-2</code> gets the update. If it is healthy, lower the partition to 1 to update <code>mysql-1</code>, then to 0 for all."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>StatefulSet canary pattern:
                  partition: N-1 → update only the last Pod
                  Verify health
                  partition: 0 → update all remaining Pods</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: mysql
              labels:
                app: mysql
            spec:
              clusterIP: None
              selector:
                app: mysql
              ports:
              - port: 3306
                name: mysql
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: mysql
            spec:
              serviceName: mysql
              replicas: 3
              podManagementPolicy: OrderedReady
              updateStrategy:
                type: RollingUpdate
                rollingUpdate:
                  partition: 2
              selector:
                matchLabels:
                  app: mysql
              template:
                metadata:
                  labels:
                    app: mysql
                spec:
                  containers:
                  - name: mysql
                    image: mysql:8.0
                    ports:
                    - containerPort: 3306
                      name: mysql
                    volumeMounts:
                    - name: data
                      mountPath: /var/lib/mysql
              volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 20Gi
          difficulty: 4
        - id: v4
          title: "ZooKeeper Ensemble"
          description: >-
            Deploy a 3-node ZooKeeper ensemble as a StatefulSet. Requirements:
            <br>- Name: <code>zk</code>, image: <code>zookeeper:3.9</code>
            <br>- Headless Service named <code>zk-headless</code> on ports 2181 (client), 2888 (follower), 3888 (election)
            <br>- 10Gi persistent storage per Pod at <code>/data</code>
            <br>- Parallel pod management for faster startup
            <br>- Set resource requests: 250m CPU, 512Mi memory
          functionSignature: "StatefulSet"
          testCases:
            - input: "3-node ZooKeeper, 3 ports, parallel management, resource requests"
              output: "StatefulSet with Parallel policy, 3 named ports, resources"
          hints:
            - title: "\U0001F914 Think about it"
              content: "ZooKeeper needs multiple ports for different protocols. Since node ordering does not matter for initial startup, which pod management policy is best?"
            - title: "\U0001F4A1 Hint"
              content: "Use <code>podManagementPolicy: Parallel</code> for faster startup. Define three named ports in both the Service and the container spec."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. Headless Service with 3 named ports
                2. StatefulSet with Parallel podManagementPolicy
                3. volumeClaimTemplates for per-node data
                4. Resource requests to guarantee scheduling</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: zk-headless
              labels:
                app: zookeeper
            spec:
              clusterIP: None
              selector:
                app: zookeeper
              ports:
              - port: 2181
                name: client
              - port: 2888
                name: follower
              - port: 3888
                name: election
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: zk
            spec:
              serviceName: zk-headless
              replicas: 3
              podManagementPolicy: Parallel
              selector:
                matchLabels:
                  app: zookeeper
              template:
                metadata:
                  labels:
                    app: zookeeper
                spec:
                  containers:
                  - name: zookeeper
                    image: zookeeper:3.9
                    ports:
                    - containerPort: 2181
                      name: client
                    - containerPort: 2888
                      name: follower
                    - containerPort: 3888
                      name: election
                    resources:
                      requests:
                        cpu: 250m
                        memory: 512Mi
                    volumeMounts:
                    - name: data
                      mountPath: /data
              volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 10Gi
          difficulty: 4
        - id: v5
          title: "MongoDB Replica Set with Init Container"
          description: >-
            Deploy a 3-node MongoDB replica set as a StatefulSet. Requirements:
            <br>- Name: <code>mongo</code>, image: <code>mongo:7.0</code>
            <br>- Headless Service named <code>mongo</code> on port 27017
            <br>- 30Gi storage per Pod at <code>/data/db</code>
            <br>- An init container using <code>busybox</code> that runs <code>chmod 700 /data/db</code>
              to set directory permissions before MongoDB starts
            <br>- Pass <code>--replSet rs0</code> as command args to MongoDB
          functionSignature: "StatefulSet"
          testCases:
            - input: "3-node MongoDB, init container for permissions, replSet args"
              output: "StatefulSet with initContainers, args, volumeClaimTemplates"
          hints:
            - title: "\U0001F914 Think about it"
              content: "How do you run setup tasks before the main container starts? Where do command arguments go in the container spec?"
            - title: "\U0001F4A1 Hint"
              content: "Use <code>initContainers</code> in the Pod spec to run the chmod command. The init container must mount the same volume. Use <code>args</code> to pass <code>--replSet rs0</code> to MongoDB."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1. initContainers run before main containers
                2. Both init and main containers mount the same volume
                3. args in the container spec pass command-line arguments
                4. Init container completes → main container starts</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: mongo
              labels:
                app: mongo
            spec:
              clusterIP: None
              selector:
                app: mongo
              ports:
              - port: 27017
                name: mongo
            ---
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
              name: mongo
            spec:
              serviceName: mongo
              replicas: 3
              selector:
                matchLabels:
                  app: mongo
              template:
                metadata:
                  labels:
                    app: mongo
                spec:
                  initContainers:
                  - name: init-permissions
                    image: busybox
                    command: ["sh", "-c", "chmod 700 /data/db"]
                    volumeMounts:
                    - name: data
                      mountPath: /data/db
                  containers:
                  - name: mongo
                    image: mongo:7.0
                    args: ["--replSet", "rs0"]
                    ports:
                    - containerPort: 27017
                      name: mongo
                    volumeMounts:
                    - name: data
                      mountPath: /data/db
              volumeClaimTemplates:
              - metadata:
                  name: data
                spec:
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 30Gi
          difficulty: 4
    - id: challenge_4
      block: 1
      difficulty: 2
      concept: CronJobs
      variants:
        - id: v1
          title: "Log Rotation Every 6 Hours"
          description: >-
            Write a CronJob named <code>log-rotate</code> that runs <strong>every 6 hours</strong> (at
            midnight, 6 AM, noon, 6 PM). Use <code>busybox</code> with command
            <code>["/bin/sh", "-c", "find /var/log -name '*.log' -mtime +7 -delete"]</code>.
            Set <code>concurrencyPolicy: Forbid</code>.
          functionSignature: "CronJob"
          testCases:
            - input: "Every 6 hours, Forbid concurrency, busybox cleanup command"
              output: "schedule: \"0 */6 * * *\", concurrencyPolicy: Forbid"
          hints:
            - title: "\U0001F914 Think about it"
              content: "What cron expression means 'every 6 hours on the hour'? It runs at 0, 6, 12, and 18 o'clock."
            - title: "\U0001F4A1 Hint"
              content: "Use <code>\"0 */6 * * *\"</code> which means 'at minute 0 of every 6th hour'."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>*/N in the hour field = every N hours
                0 */6 * * * = at :00 of hours 0, 6, 12, 18
                Forbid = skip if previous still running</pre>
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: log-rotate
            spec:
              schedule: "0 */6 * * *"
              concurrencyPolicy: Forbid
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: cleanup
                        image: busybox
                        command: ["/bin/sh", "-c", "find /var/log -name '*.log' -mtime +7 -delete"]
          difficulty: 1
        - id: v2
          title: "Business Hours Health Check"
          description: >-
            Write a CronJob named <code>biz-health</code> that runs every <strong>30 minutes on weekdays
            (Monday-Friday)</strong> between 9 AM and 5 PM. Use <code>myapp/healthcheck:v1</code>.
            Set a <code>startingDeadlineSeconds</code> of <strong>120</strong>.
            <br><br><em>Note: cron cannot express time ranges directly, so use the closest approximation:
            every 30 minutes on weekdays.</em>
          functionSignature: "CronJob"
          testCases:
            - input: "Every 30 min, weekdays only, starting deadline 120s"
              output: "schedule: \"*/30 * * * 1-5\", startingDeadlineSeconds: 120"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Cron cannot directly express 'only between 9-5'. The day-of-week field limits to weekdays. How do you express every 30 minutes?"
            - title: "\U0001F4A1 Hint"
              content: "Use <code>\"*/30 * * * 1-5\"</code> for every 30 minutes on weekdays. Standard cron cannot limit to specific hours, so the application itself should handle the hour check if needed."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>*/30 = every 30 minutes
                1-5 in day-of-week = Mon through Fri
                startingDeadlineSeconds prevents stale runs</pre>
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: biz-health
            spec:
              schedule: "*/30 * * * 1-5"
              startingDeadlineSeconds: 120
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: healthcheck
                        image: myapp/healthcheck:v1
          difficulty: 2
        - id: v3
          title: "Monthly Report with Timeout"
          description: >-
            Write a CronJob named <code>monthly-report</code> that runs on the <strong>1st of every month
            at 6:00 AM</strong>. The Job should timeout after <strong>1 hour</strong> (3600 seconds) and
            retry up to <strong>2 times</strong>. Use <code>myapp/report-gen:v3</code>. Keep the last
            <strong>6</strong> successful Jobs.
          functionSignature: "CronJob"
          testCases:
            - input: "1st of month at 6 AM, 1h timeout, 2 retries, keep 6 successes"
              output: "schedule: \"0 6 1 * *\", activeDeadlineSeconds: 3600, backoffLimit: 2"
          hints:
            - title: "\U0001F914 Think about it"
              content: "Which cron fields represent 'day of month' and 'hour'? Where do timeout and retry settings go?"
            - title: "\U0001F4A1 Hint"
              content: "The cron expression is <code>\"0 6 1 * *\"</code>. Timeout (<code>activeDeadlineSeconds</code>) and retries (<code>backoffLimit</code>) go inside <code>jobTemplate.spec</code>."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>0 6 1 * * = minute 0, hour 6, day 1, any month, any weekday
                activeDeadlineSeconds: in jobTemplate.spec (Job-level timeout)
                backoffLimit: in jobTemplate.spec (Job-level retries)
                successfulJobsHistoryLimit: in CronJob spec</pre>
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: monthly-report
            spec:
              schedule: "0 6 1 * *"
              successfulJobsHistoryLimit: 6
              jobTemplate:
                spec:
                  activeDeadlineSeconds: 3600
                  backoffLimit: 2
                  template:
                    spec:
                      restartPolicy: Never
                      containers:
                      - name: report
                        image: myapp/report-gen:v3
          difficulty: 2
        - id: v4
          title: "Cache Warm-Up Every 10 Minutes"
          description: >-
            Write a CronJob named <code>cache-warmer</code> that runs <strong>every 10 minutes</strong>.
            Use <code>myapp/cache-warmer:v1</code>. If the previous run is still active, <strong>replace</strong>
            it with the new one. Set <code>startingDeadlineSeconds: 60</code> and
            <code>failedJobsHistoryLimit: 3</code>.
          functionSignature: "CronJob"
          testCases:
            - input: "Every 10 min, Replace concurrency, 60s deadline, keep 3 failures"
              output: "schedule: \"*/10 * * * *\", concurrencyPolicy: Replace"
          hints:
            - title: "\U0001F914 Think about it"
              content: "If the cache warmer is slow and still running when the next run fires, you only care about the latest run. Which concurrency policy handles this?"
            - title: "\U0001F4A1 Hint"
              content: "<code>concurrencyPolicy: Replace</code> kills the running Job and starts a new one. This ensures you always get the most recent cache state."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>Replace policy:
                  Old Job still running? Kill it → start new one
                  Use when only the latest run matters
                  Good for: cache warming, data sync, metric collection</pre>
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: cache-warmer
            spec:
              schedule: "*/10 * * * *"
              concurrencyPolicy: Replace
              startingDeadlineSeconds: 60
              failedJobsHistoryLimit: 3
              jobTemplate:
                spec:
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: warmer
                        image: myapp/cache-warmer:v1
          difficulty: 2
        - id: v5
          title: "Twice-Daily Database Vacuum"
          description: >-
            Write a CronJob named <code>db-vacuum</code> that runs <strong>twice a day</strong> at
            <strong>midnight and noon</strong>. Use <code>postgres:16</code> with command
            <code>["psql", "-h", "db", "-U", "admin", "-c", "VACUUM ANALYZE;"]</code>.
            <br><br>Set <code>concurrencyPolicy: Forbid</code>, <code>backoffLimit: 1</code>, and
            <code>activeDeadlineSeconds: 1800</code>.
          functionSignature: "CronJob"
          testCases:
            - input: "Midnight and noon, Forbid overlap, 30-min timeout, 1 retry"
              output: "schedule: \"0 0,12 * * *\", concurrencyPolicy: Forbid"
          hints:
            - title: "\U0001F914 Think about it"
              content: "How do you express 'at midnight AND noon' in a single cron expression? The hour field supports comma-separated values."
            - title: "\U0001F4A1 Hint"
              content: "Use <code>\"0 0,12 * * *\"</code> which means 'at minute 0 of hours 0 and 12'. The comma separates multiple values in one field."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>0,12 in hour field = midnight and noon
                Comma = list of specific values
                Forbid = skip if previous still running
                activeDeadlineSeconds + backoffLimit in jobTemplate.spec</pre>
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: db-vacuum
            spec:
              schedule: "0 0,12 * * *"
              concurrencyPolicy: Forbid
              jobTemplate:
                spec:
                  backoffLimit: 1
                  activeDeadlineSeconds: 1800
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: vacuum
                        image: postgres:16
                        command: ["psql", "-h", "db", "-U", "admin", "-c", "VACUUM ANALYZE;"]
          difficulty: 2
        - id: v6
          title: "Quarterly Compliance Scan"
          description: >-
            Write a CronJob named <code>compliance-scan</code> that runs on the <strong>1st of January,
            April, July, and October at 3:00 AM</strong>. Use <code>myapp/compliance-scanner:v2</code>.
            The Job should timeout after <strong>2 hours</strong>, retry up to <strong>3 times</strong>,
            and use <code>restartPolicy: Never</code> to preserve failed Pod logs.
          functionSignature: "CronJob"
          testCases:
            - input: "Quarterly (Jan, Apr, Jul, Oct), 2h timeout, 3 retries, preserve logs"
              output: "schedule: \"0 3 1 1,4,7,10 *\", activeDeadlineSeconds: 7200"
          hints:
            - title: "\U0001F914 Think about it"
              content: "How do you express specific months in cron? The month field accepts comma-separated values."
            - title: "\U0001F4A1 Hint"
              content: "Use <code>\"0 3 1 1,4,7,10 *\"</code> for the 1st day of months 1, 4, 7, 10 at 3 AM. Use <code>restartPolicy: Never</code> so failed Pods are kept for log inspection."
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>1,4,7,10 in month field = quarterly
                restartPolicy: Never = keep failed Pods (inspect logs)
                restartPolicy: OnFailure = restart in same Pod (cleaner)
                2 hours = 7200 seconds</pre>
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: compliance-scan
            spec:
              schedule: "0 3 1 1,4,7,10 *"
              jobTemplate:
                spec:
                  activeDeadlineSeconds: 7200
                  backoffLimit: 3
                  template:
                    spec:
                      restartPolicy: Never
                      containers:
                      - name: scanner
                        image: myapp/compliance-scanner:v2
          difficulty: 3
        - id: v7
          title: "Complete CronJob with All Options"
          description: >-
            Write a CronJob named <code>etl-pipeline</code> that runs <strong>daily at 4:30 AM</strong>.
            Configure every available CronJob option:
            <br>- <code>concurrencyPolicy: Forbid</code>
            <br>- <code>startingDeadlineSeconds: 300</code>
            <br>- <code>successfulJobsHistoryLimit: 5</code>
            <br>- <code>failedJobsHistoryLimit: 3</code>
            <br>- <code>suspend: false</code>
            <br>- Job-level: <code>backoffLimit: 2</code>, <code>activeDeadlineSeconds: 7200</code>
            <br>- Pod-level: <code>restartPolicy: OnFailure</code>, resource requests 500m CPU / 1Gi memory
            <br>- Image: <code>myapp/etl:v4</code>
          functionSignature: "CronJob"
          testCases:
            - input: "Daily 4:30 AM, all CronJob and Job options configured"
              output: "Fully-configured CronJob with all spec fields"
          hints:
            - title: "\U0001F914 Think about it"
              content: "A CronJob has three levels of configuration: CronJob spec, Job spec (jobTemplate.spec), and Pod spec (jobTemplate.spec.template.spec). Where does each setting go?"
            - title: "\U0001F4A1 Hint"
              content: |-
                CronJob level: schedule, concurrencyPolicy, startingDeadlineSeconds, history limits, suspend.
                Job level: backoffLimit, activeDeadlineSeconds.
                Pod level: restartPolicy, containers, resources.
            - title: "\U0001F527 Pattern"
              content: |-
                <pre>CronJob spec:
                  schedule, concurrencyPolicy, suspend, deadlines, history
                  └─ jobTemplate.spec:
                       backoffLimit, activeDeadlineSeconds
                       └─ template.spec:
                            restartPolicy, containers, resources</pre>
          solution: |-
            apiVersion: batch/v1
            kind: CronJob
            metadata:
              name: etl-pipeline
            spec:
              schedule: "30 4 * * *"
              concurrencyPolicy: Forbid
              startingDeadlineSeconds: 300
              successfulJobsHistoryLimit: 5
              failedJobsHistoryLimit: 3
              suspend: false
              jobTemplate:
                spec:
                  backoffLimit: 2
                  activeDeadlineSeconds: 7200
                  template:
                    spec:
                      restartPolicy: OnFailure
                      containers:
                      - name: etl
                        image: myapp/etl:v4
                        resources:
                          requests:
                            cpu: 500m
                            memory: 1Gi
          difficulty: 3
