conceptLinks:
  Namespace Operations: "#working-with-namespaces"
  ResourceQuotas: "#resource-quotas"
  LimitRanges: "#limit-ranges"
  Labels & Selectors: "#labels-selectors-and-annotations"
  Namespace Isolation: "#namespace-strategies"
  Label Design: "#labels-selectors-and-annotations"
  Quota Enforcement: "#resource-quotas"
sharedContent: {}
variants:
  warmups:
    - id: warmup_1
      concept: Namespace Operations
      variants:
        - id: v1
          title: Create a Namespace Imperatively
          description: >-
            Write the <code>kubectl</code> command to create a namespace called <code>dev</code>.
          hints:
            - "Use <code>kubectl create namespace &lt;name&gt;</code>."
          solution: |-
            kubectl create namespace dev
          annotations:
            - type: tip
              label: Shorthand
              text: >-
                You can also use <code>kubectl create ns dev</code> — <code>ns</code> is the short name for namespace.
        - id: v2
          title: Create a Namespace Declaratively
          description: >-
            Write a YAML manifest to create a namespace called <code>staging</code> with the label
            <code>environment: staging</code>.
          hints:
            - "Use <code>apiVersion: v1</code> and <code>kind: Namespace</code>."
            - "Labels go under <code>metadata.labels</code>."
          solution: |-
            apiVersion: v1
            kind: Namespace
            metadata:
              name: staging
              labels:
                environment: staging
          annotations:
            - type: tip
              label: Declarative vs Imperative
              text: >-
                Declarative YAML manifests let you add labels and annotations at creation time. The imperative
                <code>kubectl create ns</code> command creates a bare namespace — you'd need separate commands to add labels.
        - id: v3
          title: Set Default Namespace
          description: >-
            Write the <code>kubectl</code> command to set <code>dev</code> as the default namespace for your current
            context, so you no longer need to type <code>-n dev</code> on every command.
          hints:
            - "Use <code>kubectl config set-context</code> with <code>--current</code> and <code>--namespace</code>."
          solution: |-
            kubectl config set-context --current --namespace=dev
          annotations:
            - type: tip
              label: kubens
              text: >-
                The <code>kubens</code> tool (from the kubectx package) makes this faster: <code>kubens dev</code>
                switches your default namespace instantly.
        - id: v4
          title: List Pods Across All Namespaces
          description: >-
            Write the <code>kubectl</code> command to list all pods across every namespace in the cluster.
          hints:
            - "Use the <code>-A</code> flag or <code>--all-namespaces</code>."
          solution: |-
            kubectl get pods -A
          annotations:
            - type: tip
              label: Wide Output
              text: >-
                Add <code>-o wide</code> to see which node each Pod is running on:
                <code>kubectl get pods -A -o wide</code>.
        - id: v5
          title: List Pods in a Specific Namespace
          description: >-
            Write the <code>kubectl</code> command to list all pods in the <code>kube-system</code> namespace.
          hints:
            - "Use the <code>-n</code> flag to specify a namespace."
          solution: |-
            kubectl get pods -n kube-system
          annotations:
            - type: tip
              label: System Components
              text: >-
                The <code>kube-system</code> namespace holds control plane components: CoreDNS, kube-proxy,
                the API server, scheduler, and controller manager.
        - id: v6
          title: Delete a Namespace
          description: >-
            Write the <code>kubectl</code> command to delete the namespace called <code>test-env</code>.
          hints:
            - "Use <code>kubectl delete namespace &lt;name&gt;</code>."
          solution: |-
            kubectl delete namespace test-env
          annotations:
            - type: gotcha
              label: Cascading Delete
              text: >-
                Deleting a namespace deletes everything inside it — all Pods, Services, Deployments, ConfigMaps,
                Secrets. There is no confirmation prompt and no undo.
        - id: v7
          title: Verify Your Current Namespace
          description: >-
            Write the <code>kubectl</code> command to check which namespace is currently set as default for your
            context.
          hints:
            - "Use <code>kubectl config view --minify</code> and look for the namespace field."
          solution: |-
            kubectl config view --minify | grep namespace
          annotations:
            - type: tip
              label: Minify Flag
              text: >-
                The <code>--minify</code> flag shows only the current context's configuration instead of the
                entire kubeconfig file.
        - id: v8
          title: List All Namespaces
          description: >-
            Write the <code>kubectl</code> command to list all namespaces in the cluster with their labels shown.
          hints:
            - "Use <code>kubectl get namespaces</code> with <code>--show-labels</code>."
          solution: |-
            kubectl get namespaces --show-labels
          annotations:
            - type: tip
              label: Default Four
              text: >-
                Every cluster starts with four namespaces: <code>default</code>, <code>kube-system</code>,
                <code>kube-public</code>, and <code>kube-node-lease</code>.
        - id: v9
          title: Check Namespace-Scoped Resources
          description: >-
            Write the <code>kubectl</code> command to list all resource types that are namespace-scoped (not
            cluster-scoped).
          hints:
            - "Use <code>kubectl api-resources</code> with the <code>--namespaced</code> flag."
          solution: |-
            kubectl api-resources --namespaced=true
          annotations:
            - type: tip
              label: Cluster-Scoped
              text: >-
                Use <code>--namespaced=false</code> to see cluster-scoped resources like Nodes, PersistentVolumes,
                ClusterRoles, and Namespaces themselves.
        - id: v10
          title: Cross-Namespace DNS
          description: >-
            A Pod in the <code>frontend</code> namespace needs to call a Service named <code>api</code> in the
            <code>backend</code> namespace. Write the fully qualified DNS name it should use.
          hints:
            - "The pattern is <code>&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code>."
          solution: |-
            api.backend.svc.cluster.local
          annotations:
            - type: tip
              label: Short Names
              text: >-
                Within the same namespace, you can use just the service name (<code>api</code>). Across namespaces,
                you need at least <code>api.backend</code> or the full FQDN.
        - id: v11
          title: Create Namespace with Annotation
          description: >-
            Write a YAML manifest to create a namespace called <code>prod</code> with the label
            <code>environment: production</code> and the annotation <code>owner: "platform-team@company.com"</code>.
          hints:
            - "Annotations go under <code>metadata.annotations</code>, just like labels go under <code>metadata.labels</code>."
          solution: |-
            apiVersion: v1
            kind: Namespace
            metadata:
              name: prod
              labels:
                environment: production
              annotations:
                owner: "platform-team@company.com"
          annotations:
            - type: tip
              label: Annotations on Namespaces
              text: >-
                Annotations on namespaces are useful for tracking ownership, cost centers, and contact info.
                Tools and scripts can read these for automation and reporting.
        - id: v12
          title: Deploy to a Specific Namespace
          description: >-
            You have a file <code>deployment.yaml</code>. Write the <code>kubectl</code> command to apply it to
            the <code>staging</code> namespace.
          hints:
            - "Use <code>kubectl apply -f</code> with the <code>-n</code> flag."
          solution: |-
            kubectl apply -f deployment.yaml -n staging
          annotations:
            - type: gotcha
              label: Namespace in YAML
              text: >-
                If the YAML manifest already has <code>metadata.namespace</code> set, the <code>-n</code> flag is
                ignored. The namespace in the manifest takes precedence.
    - id: warmup_2
      concept: ResourceQuotas
      variants:
        - id: v1
          title: Basic Compute Quota
          description: >-
            Write a ResourceQuota YAML manifest named <code>compute-quota</code> for the <code>dev</code> namespace
            that limits total CPU requests to 4 cores and total memory requests to 8Gi.
          hints:
            - "Use <code>kind: ResourceQuota</code> with <code>apiVersion: v1</code>."
            - "Put limits under <code>spec.hard</code>. CPU requests use the key <code>requests.cpu</code>."
          solution: |-
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: dev
            spec:
              hard:
                requests.cpu: "4"
                requests.memory: 8Gi
          annotations:
            - type: gotcha
              label: Mandatory Requests
              text: >-
                Once a ResourceQuota is set for CPU or memory, every Pod in that namespace must specify resource
                requests and limits. Pods without them are rejected by the API server.
        - id: v2
          title: Full Compute Quota with Limits
          description: >-
            Write a ResourceQuota named <code>team-quota</code> for the <code>team-backend</code> namespace that
            limits: CPU requests to 8, memory requests to 16Gi, CPU limits to 16, and memory limits to 32Gi.
          hints:
            - "Use both <code>requests.cpu</code>/<code>requests.memory</code> and <code>limits.cpu</code>/<code>limits.memory</code> under <code>spec.hard</code>."
          solution: |-
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: team-quota
              namespace: team-backend
            spec:
              hard:
                requests.cpu: "8"
                requests.memory: 16Gi
                limits.cpu: "16"
                limits.memory: 32Gi
          annotations:
            - type: tip
              label: Requests vs Limits
              text: >-
                Quota <code>requests.*</code> caps the sum of all container requests. Quota <code>limits.*</code>
                caps the sum of all container limits. Both are tracked independently.
        - id: v3
          title: Object Count Quota
          description: >-
            Write a ResourceQuota named <code>object-quota</code> for the <code>dev</code> namespace that limits
            the namespace to a maximum of 20 pods, 10 services, and 5 persistent volume claims.
          hints:
            - "Object count keys are just the resource name: <code>pods</code>, <code>services</code>, <code>persistentvolumeclaims</code>."
          solution: |-
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: object-quota
              namespace: dev
            spec:
              hard:
                pods: "20"
                services: "10"
                persistentvolumeclaims: "5"
          annotations:
            - type: tip
              label: Other Countable Resources
              text: >-
                You can also limit <code>configmaps</code>, <code>secrets</code>, <code>replicationcontrollers</code>,
                and more. Use <code>count/&lt;resource&gt;.&lt;group&gt;</code> for custom resources.
        - id: v4
          title: Storage Quota
          description: >-
            Write a ResourceQuota named <code>storage-quota</code> for the <code>data-team</code> namespace that
            limits total storage requests to 100Gi and maximum PVCs to 10.
          hints:
            - "Use <code>requests.storage</code> for total storage and <code>persistentvolumeclaims</code> for PVC count."
          solution: |-
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: storage-quota
              namespace: data-team
            spec:
              hard:
                requests.storage: 100Gi
                persistentvolumeclaims: "10"
          annotations:
            - type: tip
              label: Per-StorageClass Quota
              text: >-
                You can scope quotas to a specific StorageClass:
                <code>&lt;storageclass&gt;.storageclass.storage.k8s.io/requests.storage: 50Gi</code>.
        - id: v5
          title: Combined Compute and Object Quota
          description: >-
            Write a single ResourceQuota named <code>dev-quota</code> for the <code>dev</code> namespace that limits:
            CPU requests to 4, memory requests to 8Gi, CPU limits to 8, memory limits to 16Gi, pods to 20,
            services to 10, configmaps to 20, and secrets to 20.
          hints:
            - "All limits go under one <code>spec.hard</code> block. Mix compute and object count keys."
          solution: |-
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: dev-quota
              namespace: dev
            spec:
              hard:
                requests.cpu: "4"
                requests.memory: 8Gi
                limits.cpu: "8"
                limits.memory: 16Gi
                pods: "20"
                services: "10"
                configmaps: "20"
                secrets: "20"
          annotations:
            - type: tip
              label: Multiple Quotas
              text: >-
                You can have multiple ResourceQuota objects in the same namespace. Their limits are additive — a Pod
                must satisfy all quotas. This lets you separate compute quotas from object count quotas.
        - id: v6
          title: Describe a Quota
          description: >-
            Write the <code>kubectl</code> command to view the current usage and limits of a ResourceQuota named
            <code>compute-quota</code> in the <code>dev</code> namespace.
          hints:
            - "Use <code>kubectl describe quota</code> or <code>kubectl describe resourcequota</code>."
          solution: |-
            kubectl describe quota compute-quota -n dev
          annotations:
            - type: tip
              label: Usage Tracking
              text: >-
                The describe output shows Used vs Hard columns for every limited resource. This tells you how close
                the namespace is to its budget.
        - id: v7
          title: Quota with ConfigMap and Secret Limits
          description: >-
            Write a ResourceQuota named <code>config-quota</code> for the <code>staging</code> namespace that
            limits configmaps to 30 and secrets to 15.
          hints:
            - "Use the <code>configmaps</code> and <code>secrets</code> keys under <code>spec.hard</code>."
          solution: |-
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: config-quota
              namespace: staging
            spec:
              hard:
                configmaps: "30"
                secrets: "15"
          annotations:
            - type: gotcha
              label: Default Secrets
              text: >-
                Every namespace already has a default ServiceAccount token Secret. Your quota's <code>Used</code>
                count starts at 1, not 0.
        - id: v8
          title: Tight Dev Quota
          description: >-
            Write a ResourceQuota named <code>tight-quota</code> for the <code>sandbox</code> namespace that limits:
            CPU requests to 1, memory requests to 2Gi, CPU limits to 2, memory limits to 4Gi, and pods to 5.
            This is for a sandbox environment with minimal resources.
          hints:
            - "Use low values under <code>spec.hard</code>. CPU values can be integers or millicores (e.g., <code>\"1\"</code> or <code>1000m</code>)."
          solution: |-
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: tight-quota
              namespace: sandbox
            spec:
              hard:
                requests.cpu: "1"
                requests.memory: 2Gi
                limits.cpu: "2"
                limits.memory: 4Gi
                pods: "5"
          annotations:
            - type: tip
              label: Sandbox Environments
              text: >-
                Tight quotas on sandbox namespaces prevent developers from accidentally consuming too many cluster
                resources during experimentation.
        - id: v9
          title: Production-Grade Quota
          description: >-
            Write a ResourceQuota named <code>prod-quota</code> for the <code>production</code> namespace that limits:
            CPU requests to 32, memory requests to 64Gi, CPU limits to 64, memory limits to 128Gi, pods to 100,
            services to 50, and storage requests to 500Gi.
          hints:
            - "Production quotas are typically larger. Use the same <code>spec.hard</code> structure with higher values."
          solution: |-
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: prod-quota
              namespace: production
            spec:
              hard:
                requests.cpu: "32"
                requests.memory: 64Gi
                limits.cpu: "64"
                limits.memory: 128Gi
                pods: "100"
                services: "50"
                requests.storage: 500Gi
          annotations:
            - type: tip
              label: Quota Sizing
              text: >-
                Size production quotas based on actual usage plus headroom. Use <code>kubectl describe quota</code>
                regularly to see how close you are to limits and adjust accordingly.
        - id: v10
          title: Apply a Quota File
          description: >-
            You have a file <code>quota.yaml</code>. Write the <code>kubectl</code> command to apply it to the cluster.
          hints:
            - "The namespace is already specified inside the YAML manifest. Just use <code>kubectl apply -f</code>."
          solution: |-
            kubectl apply -f quota.yaml
          annotations:
            - type: tip
              label: Namespace in Manifest
              text: >-
                When the <code>metadata.namespace</code> field is set in the YAML, <code>kubectl apply -f</code> targets
                that namespace automatically. You don't need the <code>-n</code> flag.
    - id: warmup_3
      concept: LimitRanges
      variants:
        - id: v1
          title: Basic LimitRange with Defaults
          description: >-
            Write a LimitRange YAML manifest named <code>default-limits</code> for the <code>dev</code> namespace
            that sets default container CPU limit to 200m and default memory limit to 256Mi.
          hints:
            - "Use <code>kind: LimitRange</code> with <code>apiVersion: v1</code>."
            - "Defaults go under <code>spec.limits[].default</code> with <code>type: Container</code>."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: default-limits
              namespace: dev
            spec:
              limits:
              - type: Container
                default:
                  cpu: 200m
                  memory: 256Mi
          annotations:
            - type: tip
              label: Auto-Injection
              text: >-
                When a LimitRange sets defaults, any Pod created without resource limits in that namespace
                automatically gets these values injected by the admission controller.
        - id: v2
          title: LimitRange with Default Requests
          description: >-
            Write a LimitRange named <code>dev-limits</code> for the <code>dev</code> namespace that sets default
            container limits to CPU 200m / memory 256Mi and default requests to CPU 100m / memory 128Mi.
          hints:
            - "Default limits use the <code>default</code> key. Default requests use <code>defaultRequest</code>."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: dev-limits
              namespace: dev
            spec:
              limits:
              - type: Container
                default:
                  cpu: 200m
                  memory: 256Mi
                defaultRequest:
                  cpu: 100m
                  memory: 128Mi
          annotations:
            - type: tip
              label: Quota Compatibility
              text: >-
                Setting both <code>default</code> and <code>defaultRequest</code> ensures Pods always have resource
                specs, which is required when a ResourceQuota is active in the namespace.
        - id: v3
          title: LimitRange with Min and Max
          description: >-
            Write a LimitRange named <code>container-limits</code> for the <code>staging</code> namespace that sets
            minimum CPU to 50m / memory to 64Mi and maximum CPU to 2 / memory to 2Gi for containers.
          hints:
            - "Use the <code>min</code> and <code>max</code> keys under <code>spec.limits[]</code>."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: container-limits
              namespace: staging
            spec:
              limits:
              - type: Container
                min:
                  cpu: 50m
                  memory: 64Mi
                max:
                  cpu: "2"
                  memory: 2Gi
          annotations:
            - type: gotcha
              label: Rejection on Violation
              text: >-
                If a container's requests are below <code>min</code> or its limits are above <code>max</code>,
                the API server rejects the Pod. The error message tells you which boundary was violated.
        - id: v4
          title: Full LimitRange
          description: >-
            Write a LimitRange named <code>full-limits</code> for the <code>dev</code> namespace with all four
            settings: default limits (CPU 200m, memory 256Mi), default requests (CPU 100m, memory 128Mi),
            min (CPU 50m, memory 64Mi), and max (CPU 1, memory 1Gi).
          hints:
            - "Combine <code>default</code>, <code>defaultRequest</code>, <code>min</code>, and <code>max</code> in one <code>limits[]</code> entry."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: full-limits
              namespace: dev
            spec:
              limits:
              - type: Container
                default:
                  cpu: 200m
                  memory: 256Mi
                defaultRequest:
                  cpu: 100m
                  memory: 128Mi
                min:
                  cpu: 50m
                  memory: 64Mi
                max:
                  cpu: "1"
                  memory: 1Gi
          annotations:
            - type: tip
              label: Consistency
              text: >-
                Default values must fall between min and max. If <code>default</code> is outside the
                <code>min</code>/<code>max</code> range, the LimitRange creation itself fails.
        - id: v5
          title: Pod-Level Max LimitRange
          description: >-
            Write a LimitRange named <code>pod-limits</code> for the <code>dev</code> namespace that sets the
            maximum total CPU for all containers in a Pod to 4 and maximum total memory to 4Gi.
          hints:
            - "Use <code>type: Pod</code> instead of <code>type: Container</code>."
            - "Pod-level limits only support <code>max</code> — the total across all containers in the Pod."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: pod-limits
              namespace: dev
            spec:
              limits:
              - type: Pod
                max:
                  cpu: "4"
                  memory: 4Gi
          annotations:
            - type: tip
              label: Pod vs Container
              text: >-
                Container-level limits apply per container. Pod-level limits apply to the sum of all containers
                in a Pod. Use both to prevent a multi-container Pod from consuming too many resources.
        - id: v6
          title: Combined Container and Pod LimitRange
          description: >-
            Write a LimitRange named <code>combined-limits</code> for the <code>dev</code> namespace that has
            both a Container entry (default CPU 200m, memory 256Mi, max CPU 1, memory 1Gi) and a Pod entry
            (max CPU 2, memory 2Gi).
          hints:
            - "The <code>spec.limits</code> array can have multiple entries with different <code>type</code> values."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: combined-limits
              namespace: dev
            spec:
              limits:
              - type: Container
                default:
                  cpu: 200m
                  memory: 256Mi
                max:
                  cpu: "1"
                  memory: 1Gi
              - type: Pod
                max:
                  cpu: "2"
                  memory: 2Gi
          annotations:
            - type: tip
              label: Multiple Types
              text: >-
                A single LimitRange can contain entries for Container, Pod, and PersistentVolumeClaim types.
                Each type is evaluated independently.
        - id: v7
          title: Describe a LimitRange
          description: >-
            Write the <code>kubectl</code> command to view the details of a LimitRange named
            <code>dev-limits</code> in the <code>dev</code> namespace.
          hints:
            - "Use <code>kubectl describe limitrange</code>."
          solution: |-
            kubectl describe limitrange dev-limits -n dev
          annotations:
            - type: tip
              label: Output Format
              text: >-
                The describe output shows a table with Type, Resource, Min, Max, Default Request, and Default
                Limit columns — making it easy to see all boundaries at a glance.
        - id: v8
          title: LimitRange for PVCs
          description: >-
            Write a LimitRange named <code>storage-limits</code> for the <code>dev</code> namespace that sets
            the minimum PVC storage to 1Gi and maximum to 10Gi.
          hints:
            - "Use <code>type: PersistentVolumeClaim</code> with <code>min</code> and <code>max</code> for <code>storage</code>."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: storage-limits
              namespace: dev
            spec:
              limits:
              - type: PersistentVolumeClaim
                min:
                  storage: 1Gi
                max:
                  storage: 10Gi
          annotations:
            - type: tip
              label: PVC Limits
              text: >-
                PVC LimitRanges prevent developers from requesting excessively large or tiny volumes. This
                complements the storage quota, which caps total storage across all PVCs.
        - id: v9
          title: Production LimitRange
          description: >-
            Write a LimitRange named <code>prod-limits</code> for the <code>production</code> namespace with:
            default limits (CPU 500m, memory 512Mi), default requests (CPU 250m, memory 256Mi), min (CPU 100m,
            memory 128Mi), and max (CPU 2, memory 2Gi).
          hints:
            - "Production limits are typically higher than dev limits. Use the same structure with larger values."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: prod-limits
              namespace: production
            spec:
              limits:
              - type: Container
                default:
                  cpu: 500m
                  memory: 512Mi
                defaultRequest:
                  cpu: 250m
                  memory: 256Mi
                min:
                  cpu: 100m
                  memory: 128Mi
                max:
                  cpu: "2"
                  memory: 2Gi
          annotations:
            - type: tip
              label: Environment Parity
              text: >-
                Production LimitRanges should allow higher resource usage than dev/staging to handle real traffic.
                Keep the same structure across environments — only the values change.
        - id: v10
          title: LimitRange with Max Ratio
          description: >-
            Write a LimitRange named <code>ratio-limits</code> for the <code>dev</code> namespace that sets
            a <code>maxLimitRequestRatio</code> of 2 for both CPU and memory, meaning a container's limit
            can be at most 2x its request.
          hints:
            - "Use the <code>maxLimitRequestRatio</code> key under <code>spec.limits[]</code> with <code>type: Container</code>."
          solution: |-
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: ratio-limits
              namespace: dev
            spec:
              limits:
              - type: Container
                maxLimitRequestRatio:
                  cpu: "2"
                  memory: "2"
          annotations:
            - type: tip
              label: Why Ratios Matter
              text: >-
                A large gap between requests and limits means Pods are overcommitted — they request little but
                can burst high. The ratio constraint prevents extreme overcommitment that can cause node instability.
    - id: warmup_4
      concept: Labels & Selectors
      variants:
        - id: v1
          title: Add a Label to a Pod
          description: >-
            Write the <code>kubectl</code> command to add the label <code>tier=frontend</code> to an existing pod
            named <code>web</code>.
          hints:
            - "Use <code>kubectl label pod &lt;name&gt; key=value</code>."
          solution: |-
            kubectl label pod web tier=frontend
          annotations:
            - type: tip
              label: Multiple Labels
              text: >-
                You can add multiple labels at once:
                <code>kubectl label pod web tier=frontend version=2.0</code>.
        - id: v2
          title: Update an Existing Label
          description: >-
            Write the <code>kubectl</code> command to change the label <code>tier</code> from <code>frontend</code>
            to <code>backend</code> on the pod named <code>web</code>.
          hints:
            - "You need the <code>--overwrite</code> flag to change an existing label."
          solution: |-
            kubectl label pod web tier=backend --overwrite
          annotations:
            - type: gotcha
              label: Overwrite Required
              text: >-
                Without <code>--overwrite</code>, kubectl refuses to change an existing label to prevent accidental
                changes. This is a safety feature.
        - id: v3
          title: Remove a Label
          description: >-
            Write the <code>kubectl</code> command to remove the label <code>tier</code> from the pod named
            <code>web</code>.
          hints:
            - "Append a minus sign to the key name: <code>key-</code>."
          solution: |-
            kubectl label pod web tier-
          annotations:
            - type: gotcha
              label: Service Impact
              text: >-
                Removing a label that a Service selector matches on will immediately remove the Pod from that
                Service's endpoints. Traffic will stop flowing to it.
        - id: v4
          title: Equality-Based Selector
          description: >-
            Write the <code>kubectl</code> command to list all pods that have the label <code>app=api</code>.
          hints:
            - "Use the <code>-l</code> flag (or <code>--selector</code>) with <code>key=value</code>."
          solution: |-
            kubectl get pods -l app=api
          annotations:
            - type: tip
              label: Equality Operators
              text: >-
                Equality-based selectors support <code>=</code>, <code>==</code> (same as =), and <code>!=</code>.
        - id: v5
          title: Multiple Label Selector (AND)
          description: >-
            Write the <code>kubectl</code> command to list all pods that have both <code>app=api</code> AND
            <code>tier=backend</code>.
          hints:
            - "Separate multiple selectors with commas. Commas mean AND."
          solution: |-
            kubectl get pods -l app=api,tier=backend
          annotations:
            - type: tip
              label: AND Logic
              text: >-
                Comma-separated selectors are always AND. There is no OR operator in label selectors. To achieve
                OR, use set-based selectors: <code>app in (api, web)</code>.
        - id: v6
          title: Set-Based Selector — In
          description: >-
            Write the <code>kubectl</code> command to list all pods where the <code>app</code> label is either
            <code>api</code> or <code>web</code>.
          hints:
            - "Use set-based syntax: <code>'key in (value1, value2)'</code>."
            - "Wrap the expression in single quotes to prevent shell interpretation."
          solution: |-
            kubectl get pods -l 'app in (api, web)'
          annotations:
            - type: tip
              label: Set-Based Operators
              text: >-
                Set-based operators: <code>in</code>, <code>notin</code>, <code>exists</code> (just the key name),
                and <code>!exists</code> (prefix the key with <code>!</code>).
        - id: v7
          title: Set-Based Selector — NotIn
          description: >-
            Write the <code>kubectl</code> command to list all pods where the <code>environment</code> label
            is NOT <code>test</code> or <code>canary</code>.
          hints:
            - "Use <code>notin</code> with a set of values."
          solution: |-
            kubectl get pods -l 'environment notin (test, canary)'
          annotations:
            - type: tip
              label: Exclusion Pattern
              text: >-
                The <code>notin</code> selector also matches Pods that don't have the label at all. If you want
                only Pods that have the label but with different values, combine with an existence check.
        - id: v8
          title: Existence Selector
          description: >-
            Write the <code>kubectl</code> command to list all pods that have a label key called
            <code>environment</code> (regardless of its value).
          hints:
            - "Use just the key name as the selector — no operator needed."
          solution: |-
            kubectl get pods -l 'environment'
          annotations:
            - type: tip
              label: Key Existence
              text: >-
                The existence selector <code>-l 'environment'</code> matches any Pod with that label key, regardless
                of value. Use <code>-l '!environment'</code> to find Pods missing the label.
        - id: v9
          title: Show Labels in Output
          description: >-
            Write the <code>kubectl</code> command to list all pods in the <code>prod</code> namespace with
            their labels displayed.
          hints:
            - "Use <code>--show-labels</code> to add a LABELS column to the output."
          solution: |-
            kubectl get pods -n prod --show-labels
          annotations:
            - type: tip
              label: Custom Columns
              text: >-
                For specific labels, use <code>-L app,tier</code> to show only those labels as separate columns
                in the output.
        - id: v10
          title: Add Annotation
          description: >-
            Write the <code>kubectl</code> command to add the annotation
            <code>build.company.com/git-sha="a1b2c3d"</code> to a deployment named <code>api</code>.
          hints:
            - "Use <code>kubectl annotate</code> with <code>key=value</code> syntax."
          solution: |-
            kubectl annotate deployment api build.company.com/git-sha="a1b2c3d"
          annotations:
            - type: tip
              label: Labels vs Annotations
              text: >-
                If you need to select or filter by it, use a label. If it's informational metadata (build info,
                tool config, contact info), use an annotation. Annotations can hold longer values (up to 256KB).
        - id: v11
          title: Remove an Annotation
          description: >-
            Write the <code>kubectl</code> command to remove the annotation <code>build.company.com/git-sha</code>
            from the deployment named <code>api</code>.
          hints:
            - "Same as removing labels — append a minus sign to the key."
          solution: |-
            kubectl annotate deployment api build.company.com/git-sha-
          annotations:
            - type: tip
              label: Same Syntax
              text: >-
                Label and annotation removal use the same trailing minus syntax. This is consistent across
                kubectl for both <code>kubectl label</code> and <code>kubectl annotate</code>.
        - id: v12
          title: Recommended Kubernetes Labels
          description: >-
            Write the <code>metadata.labels</code> section for a Deployment using the Kubernetes recommended label
            conventions. The app is named <code>payment-api</code>, version <code>3.2.1</code>, it is a
            <code>backend</code> component, part of the <code>online-store</code> application, managed by
            <code>helm</code>.
          hints:
            - "Use the <code>app.kubernetes.io/</code> prefix for recommended labels."
            - "Keys include: <code>name</code>, <code>version</code>, <code>component</code>, <code>part-of</code>, <code>managed-by</code>."
          solution: |-
            metadata:
              labels:
                app.kubernetes.io/name: payment-api
                app.kubernetes.io/version: "3.2.1"
                app.kubernetes.io/component: backend
                app.kubernetes.io/part-of: online-store
                app.kubernetes.io/managed-by: helm
          annotations:
            - type: tip
              label: Ecosystem Integration
              text: >-
                These standardized labels are recognized by Helm, ArgoCD, the Kubernetes dashboard, and other
                ecosystem tools. Using them makes your resources work well with the broader tooling.
        - id: v13
          title: matchExpressions Selector in YAML
          description: >-
            Write a Deployment <code>spec.selector</code> section that matches Pods with label
            <code>app: api</code> AND where <code>environment</code> is in the set
            <code>[production, staging]</code>.
          hints:
            - "Use <code>matchLabels</code> for the equality match and <code>matchExpressions</code> for the set-based match."
          solution: |-
            spec:
              selector:
                matchLabels:
                  app: api
                matchExpressions:
                - key: environment
                  operator: In
                  values:
                  - production
                  - staging
          annotations:
            - type: tip
              label: matchExpressions
              text: >-
                Supported operators in <code>matchExpressions</code> are: <code>In</code>, <code>NotIn</code>,
                <code>Exists</code>, and <code>DoesNotExist</code>. The <code>Exists</code> and <code>DoesNotExist</code>
                operators do not take a <code>values</code> field.
  challenges:
    - id: challenge_1
      block: 1
      difficulty: 2
      concept: Namespace Isolation
      variants:
        - id: v1
          title: Multi-Team Namespace Strategy
          description: >-
            Your company has three teams: <code>frontend</code>, <code>backend</code>, and <code>data</code>. Each
            team needs a <code>dev</code> and <code>prod</code> namespace. Write the YAML to create all six
            namespaces with labels for <code>team</code> and <code>environment</code>.
          functionSignature: "Namespace YAML (multi-document)"
          testCases:
            - input: "3 teams x 2 environments"
              output: "6 Namespace manifests with team and environment labels"
            - input: "kubectl get ns --show-labels"
              output: "Each namespace shows both team=<team> and environment=<env> labels"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                How do you name namespaces so they are clearly associated with both a team and an environment?
                What naming convention avoids collisions?
            - title: "\U0001F4A1 Hint"
              content: >-
                Use the pattern <code>team-environment</code> for names (e.g., <code>frontend-dev</code>). Separate
                documents with <code>---</code>. Add both <code>team</code> and <code>environment</code> labels.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>For each team x environment combination:
                apiVersion: v1, kind: Namespace
                metadata.name: {team}-{env}
                metadata.labels: team={team}, environment={env}
                Separate with ---</pre>
          solution: |-
            apiVersion: v1
            kind: Namespace
            metadata:
              name: frontend-dev
              labels:
                team: frontend
                environment: dev
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: frontend-prod
              labels:
                team: frontend
                environment: prod
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: backend-dev
              labels:
                team: backend
                environment: dev
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: backend-prod
              labels:
                team: backend
                environment: prod
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: data-dev
              labels:
                team: data
                environment: dev
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: data-prod
              labels:
                team: data
                environment: prod
          difficulty: 2
          annotations:
            - type: pattern
              label: Naming Convention
              text: >-
                The <code>{team}-{environment}</code> naming convention makes it easy to identify namespace purpose
                at a glance. Labels allow policy-based management across namespaces.
            - type: tip
              label: Label-Based Policy
              text: >-
                With labels on namespaces, you can apply NetworkPolicies, quotas, or admission rules to all
                namespaces matching a label (e.g., all <code>environment=prod</code> namespaces).
        - id: v2
          title: Team Namespace with Quota and LimitRange
          description: >-
            Create a complete namespace setup for the <code>backend</code> team's dev environment. Write a
            multi-document YAML that creates: (1) the namespace <code>backend-dev</code> with team and environment
            labels, (2) a ResourceQuota limiting CPU requests to 4, memory requests to 8Gi, and pods to 15,
            (3) a LimitRange with default limits (CPU 200m, memory 256Mi) and default requests (CPU 100m,
            memory 128Mi).
          functionSignature: "Multi-document YAML: Namespace + ResourceQuota + LimitRange"
          testCases:
            - input: "Create namespace, quota, and limitrange"
              output: "3 resources created in backend-dev namespace"
            - input: "Deploy a pod without resource specs"
              output: "Pod gets default limits/requests injected by the LimitRange"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Why do you need both a ResourceQuota and a LimitRange? What happens if you have a quota but no
                LimitRange?
            - title: "\U0001F4A1 Hint"
              content: >-
                The LimitRange injects defaults so Pods satisfy the quota requirement. Without it, every Pod
                must manually specify resource requests/limits or the API server rejects it.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Document 1: Namespace with labels
                Document 2: ResourceQuota in that namespace
                Document 3: LimitRange in that namespace
                Separate with ---</pre>
          solution: |-
            apiVersion: v1
            kind: Namespace
            metadata:
              name: backend-dev
              labels:
                team: backend
                environment: dev
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: backend-dev
            spec:
              hard:
                requests.cpu: "4"
                requests.memory: 8Gi
                pods: "15"
            ---
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: default-limits
              namespace: backend-dev
            spec:
              limits:
              - type: Container
                default:
                  cpu: 200m
                  memory: 256Mi
                defaultRequest:
                  cpu: 100m
                  memory: 128Mi
          difficulty: 2
          annotations:
            - type: pattern
              label: Quota + LimitRange Pair
              text: >-
                Always pair ResourceQuotas with LimitRanges. The LimitRange ensures Pods have resource specs
                (satisfying the quota requirement) even when developers forget to specify them.
            - type: gotcha
              label: Apply Order
              text: >-
                Apply the Namespace first. The ResourceQuota and LimitRange reference the namespace in their
                metadata, so the namespace must exist before they can be created.
        - id: v3
          title: Tiered Environment Setup
          description: >-
            Design a namespace strategy with tiered quotas. Write the YAML for three namespaces — <code>sandbox</code>
            (loose: 2 CPU, 4Gi memory, 10 pods), <code>staging</code> (medium: 8 CPU, 16Gi memory, 30 pods),
            and <code>production</code> (generous: 32 CPU, 64Gi memory, 100 pods). Each namespace should have a
            ResourceQuota and an <code>environment</code> label.
          functionSignature: "Multi-document YAML: 3 Namespaces + 3 ResourceQuotas"
          testCases:
            - input: "kubectl describe quota -n sandbox"
              output: "requests.cpu: 2, requests.memory: 4Gi, pods: 10"
            - input: "kubectl describe quota -n production"
              output: "requests.cpu: 32, requests.memory: 64Gi, pods: 100"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Why would you give different resource budgets to different environments? What's the risk of giving
                dev the same budget as production?
            - title: "\U0001F4A1 Hint"
              content: >-
                Sandbox gets minimal resources (experimentation). Staging mirrors production config but smaller.
                Production gets the lion's share. Labels track which tier each namespace belongs to.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>For each environment:
                1. Namespace with environment label
                2. ResourceQuota with tiered hard limits
                Use --- to separate all 6 documents</pre>
          solution: |-
            apiVersion: v1
            kind: Namespace
            metadata:
              name: sandbox
              labels:
                environment: sandbox
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: sandbox
            spec:
              hard:
                requests.cpu: "2"
                requests.memory: 4Gi
                pods: "10"
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: staging
              labels:
                environment: staging
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: staging
            spec:
              hard:
                requests.cpu: "8"
                requests.memory: 16Gi
                pods: "30"
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: production
              labels:
                environment: production
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: production
            spec:
              hard:
                requests.cpu: "32"
                requests.memory: 64Gi
                pods: "100"
          difficulty: 2
          annotations:
            - type: pattern
              label: Tiered Quotas
              text: >-
                Tiered quotas match resource budgets to environment importance. Sandbox is cheap to experiment in.
                Production gets the resources it needs for real traffic.
        - id: v4
          title: Complete Namespace with Annotations and RBAC Hints
          description: >-
            Write the YAML for a production namespace named <code>payment-prod</code> with: labels for
            <code>team: payments</code>, <code>environment: production</code>, and
            <code>app.kubernetes.io/part-of: online-store</code>. Add annotations for
            <code>owner: "payments-team@company.com"</code>, <code>cost-center: "CC-PAY-001"</code>, and
            <code>oncall-slack: "#payments-oncall"</code>. Include a ResourceQuota (CPU requests 16, memory
            requests 32Gi, pods 50) and a LimitRange (default CPU 500m, memory 512Mi, max CPU 2, memory 2Gi).
          functionSignature: "Multi-document YAML: Namespace + Quota + LimitRange"
          testCases:
            - input: "kubectl describe namespace payment-prod"
              output: "Shows labels, annotations, quota, and limitrange"
            - input: "Deploy pod without resource specs"
              output: "Gets 500m CPU limit and 512Mi memory limit injected"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                What information belongs in labels (for filtering) vs annotations (for metadata)? How do the
                three resources work together?
            - title: "\U0001F4A1 Hint"
              content: >-
                Labels: team, environment, part-of (used for selection). Annotations: owner email, cost center,
                Slack channel (informational, not for selection). Quota caps the namespace. LimitRange sets defaults.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Document 1: Namespace with labels + annotations
                Document 2: ResourceQuota in that namespace
                Document 3: LimitRange with default + max</pre>
          solution: |-
            apiVersion: v1
            kind: Namespace
            metadata:
              name: payment-prod
              labels:
                team: payments
                environment: production
                app.kubernetes.io/part-of: online-store
              annotations:
                owner: "payments-team@company.com"
                cost-center: "CC-PAY-001"
                oncall-slack: "#payments-oncall"
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: payment-prod
            spec:
              hard:
                requests.cpu: "16"
                requests.memory: 32Gi
                pods: "50"
            ---
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: default-limits
              namespace: payment-prod
            spec:
              limits:
              - type: Container
                default:
                  cpu: 500m
                  memory: 512Mi
                defaultRequest:
                  cpu: 250m
                  memory: 256Mi
                max:
                  cpu: "2"
                  memory: 2Gi
          difficulty: 3
          annotations:
            - type: pattern
              label: Namespace as Unit of Management
              text: >-
                A well-configured namespace bundles identity (labels/annotations), budget (quota), and safety
                rails (limit range) into a single manageable unit. This is the foundation for multi-tenant clusters.
            - type: tip
              label: GitOps Friendly
              text: >-
                Storing namespace + quota + limitrange as a multi-document YAML file in Git gives you version
                control, audit trail, and reproducible namespace provisioning.
        - id: v5
          title: Namespace with Full Resource Controls
          description: >-
            Create a comprehensive setup for a <code>ml-training</code> namespace. Write YAML for: (1) the namespace
            with labels <code>team: data-science</code> and <code>workload-type: batch</code>, (2) a ResourceQuota
            limiting CPU requests to 64, memory to 256Gi, pods to 200, PVCs to 20, and storage to 1Ti, (3) a
            LimitRange with Container defaults (CPU 1, memory 2Gi), min (CPU 100m, memory 256Mi), max (CPU 8,
            memory 32Gi), and Pod max (CPU 16, memory 64Gi).
          functionSignature: "Multi-document YAML: Namespace + Quota + LimitRange"
          testCases:
            - input: "kubectl describe quota -n ml-training"
              output: "Shows CPU, memory, pod, PVC, and storage limits"
            - input: "Deploy a pod requesting 10 CPU"
              output: "Rejected — exceeds container max of 8 CPU from LimitRange"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                ML training jobs are resource-intensive. How do you allow large individual jobs while still
                capping total namespace consumption? What role does the Pod-level max play?
            - title: "\U0001F4A1 Hint"
              content: >-
                The Container max limits any single container. The Pod max limits the sum across all containers
                in a Pod. The ResourceQuota limits the sum across all Pods in the namespace. Three levels of control.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>1. Namespace with team + workload-type labels
                2. ResourceQuota: compute + storage + object counts
                3. LimitRange: Container (default, min, max) + Pod (max)</pre>
          solution: |-
            apiVersion: v1
            kind: Namespace
            metadata:
              name: ml-training
              labels:
                team: data-science
                workload-type: batch
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: ml-training
            spec:
              hard:
                requests.cpu: "64"
                requests.memory: 256Gi
                pods: "200"
                persistentvolumeclaims: "20"
                requests.storage: 1Ti
            ---
            apiVersion: v1
            kind: LimitRange
            metadata:
              name: resource-limits
              namespace: ml-training
            spec:
              limits:
              - type: Container
                default:
                  cpu: "1"
                  memory: 2Gi
                defaultRequest:
                  cpu: 500m
                  memory: 1Gi
                min:
                  cpu: 100m
                  memory: 256Mi
                max:
                  cpu: "8"
                  memory: 32Gi
              - type: Pod
                max:
                  cpu: "16"
                  memory: 64Gi
          difficulty: 3
          annotations:
            - type: pattern
              label: Three-Level Resource Control
              text: >-
                Container max caps individual containers. Pod max caps multi-container Pods. ResourceQuota caps the
                whole namespace. Together they prevent any single job from starving others.
            - type: tip
              label: GPU Quotas
              text: >-
                For GPU workloads, you can also quota <code>requests.nvidia.com/gpu</code> (or similar extended
                resources) to limit GPU consumption per namespace.
        - id: v6
          title: Microservices Namespace Isolation
          description: >-
            A microservices platform has three services: <code>user-service</code>, <code>order-service</code>,
            and <code>notification-service</code>. Each runs in its own namespace. Write the YAML to create all
            three namespaces, each with a ResourceQuota (4 CPU requests, 8Gi memory, 10 pods) and labels for
            <code>app.kubernetes.io/part-of: ecommerce</code> and the specific <code>app.kubernetes.io/name</code>.
          functionSignature: "Multi-document YAML: 3 Namespaces + 3 ResourceQuotas"
          testCases:
            - input: "kubectl get ns -l app.kubernetes.io/part-of=ecommerce"
              output: "Lists all three service namespaces"
            - input: "kubectl describe quota -n order-service"
              output: "Shows 4 CPU, 8Gi memory, 10 pod limits"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Why give each microservice its own namespace instead of putting them all together? What does
                namespace-per-service give you that a shared namespace doesn't?
            - title: "\U0001F4A1 Hint"
              content: >-
                Separate namespaces give independent resource budgets, RBAC boundaries, and failure isolation.
                A runaway order-service can't consume resources meant for notification-service.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>For each service:
                1. Namespace: name={service}, labels: part-of + name
                2. ResourceQuota: same budget for each (or vary by importance)
                Separate with ---</pre>
          solution: |-
            apiVersion: v1
            kind: Namespace
            metadata:
              name: user-service
              labels:
                app.kubernetes.io/name: user-service
                app.kubernetes.io/part-of: ecommerce
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: user-service
            spec:
              hard:
                requests.cpu: "4"
                requests.memory: 8Gi
                pods: "10"
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: order-service
              labels:
                app.kubernetes.io/name: order-service
                app.kubernetes.io/part-of: ecommerce
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: order-service
            spec:
              hard:
                requests.cpu: "4"
                requests.memory: 8Gi
                pods: "10"
            ---
            apiVersion: v1
            kind: Namespace
            metadata:
              name: notification-service
              labels:
                app.kubernetes.io/name: notification-service
                app.kubernetes.io/part-of: ecommerce
            ---
            apiVersion: v1
            kind: ResourceQuota
            metadata:
              name: compute-quota
              namespace: notification-service
            spec:
              hard:
                requests.cpu: "4"
                requests.memory: 8Gi
                pods: "10"
          difficulty: 2
          annotations:
            - type: pattern
              label: Namespace per Service
              text: >-
                Namespace-per-service isolates resource budgets and RBAC. Each team owns their namespace and can
                independently manage their service without affecting others.
    - id: challenge_2
      block: 1
      difficulty: 1
      concept: Label Design
      variants:
        - id: v1
          title: Label Schema for a Web App
          description: >-
            Design a label schema for a three-tier web application with a frontend, backend API, and database.
            Write the <code>metadata.labels</code> section for each tier's Deployment using Kubernetes recommended
            labels. The app is called <code>bookstore</code>, version <code>1.0.0</code>, managed by
            <code>kubectl</code>.
          functionSignature: "YAML metadata.labels for 3 Deployments"
          testCases:
            - input: "kubectl get deployments -l app.kubernetes.io/part-of=bookstore"
              output: "Lists all three Deployments"
            - input: "kubectl get deployments -l app.kubernetes.io/component=backend"
              output: "Lists only the API Deployment"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                What labels do all three tiers share? What label differentiates them? How does the Kubernetes
                recommended label schema handle this?
            - title: "\U0001F4A1 Hint"
              content: >-
                Shared: <code>part-of</code> (bookstore), <code>version</code>, <code>managed-by</code>.
                Different: <code>name</code> (specific to each tier), <code>component</code> (frontend/backend/database).
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Each Deployment gets:
                app.kubernetes.io/name: {tier-name}
                app.kubernetes.io/component: {frontend|backend|database}
                app.kubernetes.io/part-of: bookstore
                app.kubernetes.io/version: "1.0.0"
                app.kubernetes.io/managed-by: kubectl</pre>
          solution: |-
            # Frontend Deployment labels
            metadata:
              labels:
                app.kubernetes.io/name: bookstore-frontend
                app.kubernetes.io/component: frontend
                app.kubernetes.io/part-of: bookstore
                app.kubernetes.io/version: "1.0.0"
                app.kubernetes.io/managed-by: kubectl

            # Backend API Deployment labels
            metadata:
              labels:
                app.kubernetes.io/name: bookstore-api
                app.kubernetes.io/component: backend
                app.kubernetes.io/part-of: bookstore
                app.kubernetes.io/version: "1.0.0"
                app.kubernetes.io/managed-by: kubectl

            # Database Deployment labels
            metadata:
              labels:
                app.kubernetes.io/name: bookstore-db
                app.kubernetes.io/component: database
                app.kubernetes.io/part-of: bookstore
                app.kubernetes.io/version: "1.0.0"
                app.kubernetes.io/managed-by: kubectl
          difficulty: 1
          annotations:
            - type: pattern
              label: Standard Labels
              text: >-
                The <code>app.kubernetes.io</code> label convention is recognized by the Kubernetes ecosystem.
                <code>part-of</code> groups related components, <code>component</code> distinguishes tiers.
        - id: v2
          title: Selector for Backend Pods Only
          description: >-
            Given pods labeled with <code>app=api</code>, <code>tier=backend</code>, and
            <code>environment=production</code>, write the <code>kubectl</code> command to list only the
            production backend API pods.
          functionSignature: "kubectl get pods with -l selector"
          testCases:
            - input: "Pods: app=api/tier=backend/env=production, app=web/tier=frontend/env=production, app=api/tier=backend/env=staging"
              output: "Only the first pod matches"
          hints:
            - title: "\U0001F914 Think about it"
              content: How do you combine multiple label requirements? What operator joins them?
            - title: "\U0001F4A1 Hint"
              content: >-
                Use comma-separated selectors. Each comma is an AND condition. All three labels must match.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>kubectl get pods -l key1=val1,key2=val2,key3=val3</pre>
          solution: |-
            kubectl get pods -l app=api,tier=backend,environment=production
          difficulty: 1
          annotations:
            - type: tip
              label: AND Logic
              text: >-
                Comma-separated selectors are always AND. Every condition must be true for a Pod to match.
        - id: v3
          title: Find All Non-Production Pods
          description: >-
            Write the <code>kubectl</code> command to list all pods where the <code>environment</code> label
            is NOT <code>production</code>. This should match pods in dev, staging, test, and any other
            environment.
          functionSignature: "kubectl get pods with set-based selector"
          testCases:
            - input: "Pods with environment=dev, environment=staging, environment=production"
              output: "Lists dev and staging pods only"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Can you do this with an equality-based selector? What set-based operator excludes specific values?
            - title: "\U0001F4A1 Hint"
              content: >-
                Use <code>notin</code> to exclude specific values from matching. Remember to quote the expression
                for the shell.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>kubectl get pods -l 'key notin (value)'</pre>
          solution: |-
            kubectl get pods -l 'environment notin (production)'
          difficulty: 1
          annotations:
            - type: gotcha
              label: Missing Labels
              text: >-
                <code>notin</code> also matches Pods that don't have the label at all. If a Pod has no
                <code>environment</code> label, it matches the <code>notin</code> selector.
        - id: v4
          title: Canary Deployment Labels
          description: >-
            You're running a canary deployment. The stable version has labels <code>app: api</code>,
            <code>version: "2.0"</code>, <code>track: stable</code>. The canary has <code>app: api</code>,
            <code>version: "2.1"</code>, <code>track: canary</code>. Write a Service selector that sends traffic
            to BOTH stable and canary pods (they share the same <code>app</code> label).
          functionSignature: "Service spec.selector YAML"
          testCases:
            - input: "Stable pods: app=api,track=stable. Canary pods: app=api,track=canary"
              output: "Service routes to both stable and canary pods"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                A Service selector matches all Pods with ALL the specified labels. What label do both stable and
                canary pods share? What label should the Service selector NOT include?
            - title: "\U0001F4A1 Hint"
              content: >-
                The Service selector should only include labels common to both versions. Use <code>app: api</code>
                only — not <code>track</code> or <code>version</code>.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>spec:
                  selector:
                    app: api    # matches both stable and canary</pre>
          solution: |-
            apiVersion: v1
            kind: Service
            metadata:
              name: api
            spec:
              selector:
                app: api
              ports:
              - port: 80
                targetPort: 8080
          difficulty: 2
          annotations:
            - type: pattern
              label: Canary via Labels
              text: >-
                Canary deployments use label-based routing. The Service matches a broad label (app). The
                Deployments use additional labels (track, version) to differentiate. Adjusting replica counts
                controls traffic split.
        - id: v5
          title: Find Unlabeled Pods
          description: >-
            Write the <code>kubectl</code> command to find all pods across all namespaces that do NOT have an
            <code>app</code> label. These are potentially unmanaged or misconfigured resources.
          functionSignature: "kubectl command"
          testCases:
            - input: "Mix of labeled and unlabeled pods across namespaces"
              output: "Only pods without an 'app' label are listed"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                How do you test for the absence of a label key? What selector syntax means "this key does not exist"?
            - title: "\U0001F4A1 Hint"
              content: >-
                Use the <code>!</code> prefix to match resources that do NOT have a specific label key.
                Combine with <code>-A</code> for all namespaces.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>kubectl get pods -A -l '!labelkey'</pre>
          solution: |-
            kubectl get pods -A -l '!app'
          difficulty: 2
          annotations:
            - type: tip
              label: Auditing Labels
              text: >-
                Regularly auditing for unlabeled resources helps maintain cluster hygiene. Pods without standard
                labels are harder to manage, monitor, and troubleshoot.
        - id: v6
          title: Multi-App Selector with In
          description: >-
            You have pods for three microservices: <code>auth</code>, <code>users</code>, and <code>orders</code>.
            Write the <code>kubectl</code> command to list all pods that belong to any of these three services,
            but only in the <code>production</code> environment.
          functionSignature: "kubectl command with mixed selector types"
          testCases:
            - input: "Pods: app=auth/env=production, app=orders/env=production, app=payments/env=production, app=auth/env=dev"
              output: "Lists auth/production and orders/production only"
          hints:
            - title: "\U0001F914 Think about it"
              content: How do you combine a set-based selector (multiple app values) with an equality-based selector (one environment)?
            - title: "\U0001F4A1 Hint"
              content: >-
                Mix set-based and equality-based in one <code>-l</code> flag. Use <code>in</code> for the apps
                and <code>=</code> for the environment, separated by a comma.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>kubectl get pods -l 'app in (val1, val2, val3),environment=production'</pre>
          solution: |-
            kubectl get pods -l 'app in (auth, users, orders),environment=production'
          difficulty: 2
          annotations:
            - type: tip
              label: Mixed Selectors
              text: >-
                You can freely mix equality-based and set-based selectors in a single <code>-l</code> flag.
                They are all ANDed together.
        - id: v7
          title: Label-Based Annotation Strategy
          description: >-
            Write a Deployment manifest for a service named <code>checkout-api</code> in the <code>prod</code>
            namespace. Use Kubernetes recommended labels for: name, version (<code>3.1.0</code>), component
            (<code>backend</code>), part-of (<code>online-store</code>), managed-by (<code>argocd</code>).
            Add annotations for: git SHA, CI pipeline URL, on-call team Slack channel, and Prometheus scrape config.
          functionSignature: "Deployment YAML with labels and annotations"
          testCases:
            - input: "kubectl get deploy -l app.kubernetes.io/part-of=online-store -n prod"
              output: "Lists checkout-api"
            - input: "kubectl describe deploy checkout-api -n prod | grep prometheus"
              output: "Shows prometheus.io/scrape: true"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Which metadata is for selection (labels) and which is informational (annotations)? Can you
                select by git SHA? Should you?
            - title: "\U0001F4A1 Hint"
              content: >-
                Labels: standard k8s labels for grouping and selection. Annotations: build info (git-sha, pipeline),
                operational info (Slack channel), tool config (Prometheus). Annotations are not for selection.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>metadata:
                  labels: app.kubernetes.io/* labels for selection
                  annotations: build/*, oncall/*, prometheus.io/* for metadata
                spec: standard Deployment spec</pre>
          solution: |-
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: checkout-api
              namespace: prod
              labels:
                app.kubernetes.io/name: checkout-api
                app.kubernetes.io/version: "3.1.0"
                app.kubernetes.io/component: backend
                app.kubernetes.io/part-of: online-store
                app.kubernetes.io/managed-by: argocd
              annotations:
                build.company.com/git-sha: "abc123def456"
                build.company.com/pipeline: "https://ci.company.com/pipelines/789"
                oncall.company.com/slack: "#checkout-oncall"
                prometheus.io/scrape: "true"
                prometheus.io/port: "9090"
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app.kubernetes.io/name: checkout-api
              template:
                metadata:
                  labels:
                    app.kubernetes.io/name: checkout-api
                    app.kubernetes.io/version: "3.1.0"
                    app.kubernetes.io/component: backend
                    app.kubernetes.io/part-of: online-store
                spec:
                  containers:
                  - name: checkout-api
                    image: mycompany/checkout-api:3.1.0
                    ports:
                    - containerPort: 8080
          difficulty: 3
          annotations:
            - type: pattern
              label: Labels for Selection, Annotations for Everything Else
              text: >-
                Labels are indexed and queryable — use them for grouping and filtering. Annotations are arbitrary
                metadata — use them for build info, tool configuration, and operational context.
            - type: gotcha
              label: Label Value Limits
              text: >-
                Label values are limited to 63 characters and must be alphanumeric (with dashes, underscores, dots).
                Annotations can hold up to 256KB and have no character restrictions.
        - id: v8
          title: Blue-Green Deployment Labels
          description: >-
            Design the label and Service configuration for a blue-green deployment. Write: (1) a Service named
            <code>web</code> whose selector points to the blue version, (2) the <code>metadata.labels</code> for
            the blue Deployment (version <code>1.0</code>, slot <code>blue</code>), (3) the
            <code>metadata.labels</code> for the green Deployment (version <code>2.0</code>, slot <code>green</code>).
            Show how to switch traffic from blue to green by changing the Service selector.
          functionSignature: "Service + Deployment labels YAML"
          testCases:
            - input: "Service selector: slot=blue"
              output: "Traffic goes to blue deployment"
            - input: "Change Service selector to slot=green"
              output: "Traffic switches to green deployment"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                In blue-green deployment, both versions run simultaneously but only one receives traffic. What label
                controls which version the Service routes to?
            - title: "\U0001F4A1 Hint"
              content: >-
                Both Deployments share <code>app: web</code>. They differ by <code>slot: blue</code> vs
                <code>slot: green</code>. The Service selector includes <code>slot</code> to pick one.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Service selector: app=web, slot=blue (to route to blue)
                Switch: kubectl patch svc web -p '{"spec":{"selector":{"slot":"green"}}}'</pre>
          solution: |-
            # Service — currently routing to blue
            apiVersion: v1
            kind: Service
            metadata:
              name: web
            spec:
              selector:
                app: web
                slot: blue
              ports:
              - port: 80
                targetPort: 8080
            ---
            # Blue Deployment
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: web-blue
              labels:
                app: web
                slot: blue
                version: "1.0"
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: web
                  slot: blue
              template:
                metadata:
                  labels:
                    app: web
                    slot: blue
                    version: "1.0"
                spec:
                  containers:
                  - name: web
                    image: myapp/web:1.0
            ---
            # Green Deployment
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: web-green
              labels:
                app: web
                slot: green
                version: "2.0"
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: web
                  slot: green
              template:
                metadata:
                  labels:
                    app: web
                    slot: green
                    version: "2.0"
                spec:
                  containers:
                  - name: web
                    image: myapp/web:2.0

            # To switch traffic to green:
            # kubectl patch svc web -p '{"spec":{"selector":{"app":"web","slot":"green"}}}'
          difficulty: 3
          annotations:
            - type: pattern
              label: Blue-Green via Labels
              text: >-
                Blue-green deployments use label selectors for instant traffic switching. Both versions run in
                parallel. Changing the Service selector switches all traffic atomically — no gradual rollout.
            - type: tip
              label: Rollback
              text: >-
                To rollback, just switch the Service selector back to the old slot. Since the old version is
                still running, rollback is instant.
    - id: challenge_3
      block: 2
      difficulty: 2
      concept: Quota Enforcement
      variants:
        - id: v1
          title: Quota Exceeded — Pod Count
          description: >-
            A namespace <code>dev</code> has a ResourceQuota with <code>pods: "5"</code>. There are currently 4
            running pods. A developer runs <code>kubectl scale deployment web --replicas=3 -n dev</code> (the
            deployment currently has 2 replicas). What happens? How many pods will be running, and what error
            will appear?
          functionSignature: "Explanation of quota enforcement behavior"
          testCases:
            - input: "Current: 4 pods, quota: 5. Scale web from 2 to 3 replicas."
              output: "One new pod is created (total 5). The third replica fails — exceeds pod quota."
            - input: "kubectl describe rs -n dev"
              output: "Warning event: exceeded quota: compute-quota, pods quota exceeded"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                The scaling adds 1 pod (2 to 3 replicas = 1 new pod). Current total is 4. After adding 1, total
                is 5 — exactly at the quota. Does the scale request ask for all replicas at once or one at a time?
            - title: "\U0001F4A1 Hint"
              content: >-
                The Deployment controller creates pods one at a time through the ReplicaSet. It will successfully
                create 1 new pod (hitting the quota at 5). The deployment shows 3/3 ready because only 1 new pod
                was needed.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Before: 4 pods total, web has 2 replicas
                Scale to 3: needs 1 new pod
                4 + 1 = 5 = quota limit
                Result: scale succeeds, all 5 pods run
                If scale was to 4 (needing 2 new pods): first succeeds, second fails</pre>
          solution: |-
            The scale from 2 to 3 replicas needs 1 new pod.
            Current total: 4 pods. After scaling: 5 pods (exactly at quota).
            Result: The scale succeeds. All 3 web replicas run.

            If instead the developer scaled to 4 replicas (needing 2 new pods):
            - First new pod created: total = 5 (at quota)
            - Second new pod rejected: "exceeded quota: dev-quota"
            - The ReplicaSet shows a Warning event:
              "Error creating: pods is forbidden: exceeded quota"
            - Deployment shows 3/4 replicas ready
            - kubectl describe rs shows the quota error in Events
          difficulty: 2
          annotations:
            - type: gotcha
              label: Partial Scaling
              text: >-
                Kubernetes creates pods one at a time. If a scale hits the quota mid-way, some pods are created
                and some are rejected. The Deployment won't reach its desired replica count.
        - id: v2
          title: Quota Exceeded — CPU
          description: >-
            A namespace has a ResourceQuota with <code>requests.cpu: "4"</code>. Current usage is 3500m.
            A developer tries to create a Deployment with 3 replicas, each requesting 250m CPU. What happens?
          functionSignature: "Explanation of CPU quota enforcement"
          testCases:
            - input: "Current: 3500m used, quota: 4000m. New: 3 pods x 250m = 750m needed."
              output: "First 2 pods created (3500+500=4000m). Third pod rejected — would exceed 4000m quota."
            - input: "kubectl get deployment -n dev"
              output: "Deployment shows 2/3 replicas available"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Total needed: 3 x 250m = 750m. Available: 4000m - 3500m = 500m. How many of the 3 pods can fit
                in 500m of CPU budget?
            - title: "\U0001F4A1 Hint"
              content: >-
                500m available / 250m per pod = 2 pods can be created. The third pod would need 750m total but
                only 500m is available after creating 2.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Available CPU: 4000m - 3500m = 500m
                Pod 1: 250m → total 3750m ✓
                Pod 2: 250m → total 4000m ✓ (at limit)
                Pod 3: 250m → total 4250m ✗ (exceeds quota)
                Result: 2 of 3 pods created</pre>
          solution: |-
            Available CPU budget: 4000m - 3500m = 500m
            Each pod requests 250m CPU.

            Pod 1: created (total usage: 3750m) — within quota
            Pod 2: created (total usage: 4000m) — exactly at quota
            Pod 3: REJECTED — would push usage to 4250m, exceeding the 4000m quota

            The Deployment shows 2/3 replicas available.
            The ReplicaSet logs a warning event:
            "Error creating: pods is forbidden: exceeded quota:
            must specify requests.cpu <= 0 (remaining)"

            To fix: either increase the quota, reduce existing pod requests,
            or reduce the new deployment's CPU request per pod.
          difficulty: 2
          annotations:
            - type: gotcha
              label: Quota Tracks Requests, Not Usage
              text: >-
                ResourceQuota counts what Pods request, not what they actually use. A Pod requesting 500m but using
                only 100m still counts 500m against the quota.
        - id: v3
          title: Missing Resource Specs
          description: >-
            A namespace <code>dev</code> has a ResourceQuota with CPU and memory limits. There is NO LimitRange.
            A developer runs <code>kubectl run nginx --image=nginx -n dev</code> without specifying any resource
            requests or limits. What happens and why?
          functionSignature: "Explanation of quota + missing specs behavior"
          testCases:
            - input: "Quota set for CPU/memory, no LimitRange, pod has no resource specs"
              output: "API server rejects the pod with a Forbidden error"
            - input: "Error message"
              output: "must specify limits.cpu, limits.memory, requests.cpu, requests.memory"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                How can Kubernetes enforce a CPU quota if a Pod doesn't declare its CPU usage? Can it track what it
                doesn't know about?
            - title: "\U0001F4A1 Hint"
              content: >-
                The quota admission controller requires every Pod to declare resources when a quota exists. Without
                declared resources, the Pod is an unknown cost and is rejected. A LimitRange would inject defaults
                to avoid this.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Quota exists for CPU/memory → all pods MUST specify requests and limits
                No LimitRange → no auto-injection of defaults
                Pod without specs → REJECTED by admission controller
                Fix: add a LimitRange to inject defaults, OR specify resources in the Pod spec</pre>
          solution: |-
            The pod is REJECTED by the API server with:
            "Error from server (Forbidden): pods "nginx" is forbidden:
            failed quota: dev-quota: must specify limits.cpu, limits.memory,
            requests.cpu, requests.memory"

            Why: When a ResourceQuota for CPU/memory exists in a namespace,
            the quota admission controller requires ALL pods to declare their
            resource requests and limits. Without them, Kubernetes can't track
            resource consumption against the quota.

            Fix options:
            1. Add a LimitRange to auto-inject defaults (recommended)
            2. Always specify resources explicitly:
               kubectl run nginx --image=nginx -n dev \
                 --requests='cpu=100m,memory=128Mi' \
                 --limits='cpu=200m,memory=256Mi'
          difficulty: 2
          annotations:
            - type: gotcha
              label: Quota Forces Resource Specs
              text: >-
                This is the most common quota surprise. Teams enable quotas but forget the LimitRange, and suddenly
                every simple <code>kubectl run</code> fails. Always pair quotas with limit ranges.
        - id: v4
          title: LimitRange Rejection
          description: >-
            A namespace has a LimitRange with <code>max cpu: 2</code> and <code>max memory: 2Gi</code> per container.
            A developer creates a Pod with one container requesting <code>cpu: 500m, memory: 512Mi</code> and
            limits <code>cpu: 4, memory: 4Gi</code>. What happens?
          functionSignature: "Explanation of LimitRange enforcement"
          testCases:
            - input: "LimitRange max: cpu=2, memory=2Gi. Pod limits: cpu=4, memory=4Gi"
              output: "Pod rejected — both CPU and memory limits exceed the LimitRange max"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                LimitRange max applies to limits, not requests. Are the Pod's limits within the LimitRange max?
            - title: "\U0001F4A1 Hint"
              content: >-
                The Pod's CPU limit (4) exceeds max (2) and memory limit (4Gi) exceeds max (2Gi). Both violations
                are reported in the error message.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>LimitRange max: cpu=2, memory=2Gi
                Pod limits: cpu=4 > 2 ✗, memory=4Gi > 2Gi ✗
                Error: maximum cpu usage per Container is 2, but limit is 4
                       maximum memory usage per Container is 2Gi, but limit is 4Gi</pre>
          solution: |-
            The Pod is REJECTED with:
            "Error from server (Forbidden): pods is forbidden:
            [maximum cpu usage per Container is 2, but limit is 4,
             maximum memory usage per Container is 2Gi, but limit is 4Gi]"

            The LimitRange checks container limits against its max values:
            - CPU limit 4 > max 2 → violation
            - Memory limit 4Gi > max 2Gi → violation

            Both violations are reported. The requests (500m, 512Mi) are fine
            since they're below the max, but the limits exceed the boundary.

            Fix: reduce limits to at most cpu=2, memory=2Gi:
              limits:
                cpu: "2"
                memory: 2Gi
          difficulty: 2
          annotations:
            - type: gotcha
              label: Max Applies to Limits
              text: >-
                LimitRange <code>max</code> constrains container limits (the ceiling). LimitRange <code>min</code>
                constrains container requests (the floor). This is a common source of confusion.
        - id: v5
          title: Quota with Multiple Deployments
          description: >-
            A namespace has a ResourceQuota: <code>requests.cpu: "8"</code>, <code>requests.memory: 16Gi</code>,
            <code>pods: "20"</code>. Deployment A has 5 replicas at 500m CPU, 1Gi memory each. Deployment B has 3
            replicas at 1 CPU, 2Gi memory each. A developer wants to add Deployment C with 4 replicas at 500m CPU,
            1Gi memory each. Will it fit?
          functionSignature: "Quota capacity calculation"
          testCases:
            - input: "A: 5x500m=2500m CPU, 5x1Gi=5Gi mem. B: 3x1=3 CPU, 3x2Gi=6Gi mem. C: 4x500m=2 CPU, 4x1Gi=4Gi mem."
              output: "CPU: 2500m+3000m+2000m=7500m < 8000m ✓. Memory: 5+6+4=15Gi < 16Gi ✓. Pods: 5+3+4=12 < 20 ✓. Fits!"
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Add up the CPU requests, memory requests, and pod counts for all three deployments. Compare each
                total against the quota limit.
            - title: "\U0001F4A1 Hint"
              content: >-
                CPU: A(2500m) + B(3000m) + C(2000m) = 7500m vs 8000m quota.
                Memory: A(5Gi) + B(6Gi) + C(4Gi) = 15Gi vs 16Gi quota.
                Pods: 5 + 3 + 4 = 12 vs 20 quota.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>CPU:    2500m + 3000m + 2000m = 7500m / 8000m (93.75% — tight!)
                Memory: 5Gi + 6Gi + 4Gi = 15Gi / 16Gi (93.75% — tight!)
                Pods:   5 + 3 + 4 = 12 / 20 (60%)
                All within limits → Deployment C will be created successfully</pre>
          solution: |-
            Calculate total resource usage with all three deployments:

            CPU Requests:
              Deployment A: 5 x 500m  = 2500m
              Deployment B: 3 x 1000m = 3000m
              Deployment C: 4 x 500m  = 2000m
              Total: 7500m / 8000m quota = 93.75% ✓

            Memory Requests:
              Deployment A: 5 x 1Gi = 5Gi
              Deployment B: 3 x 2Gi = 6Gi
              Deployment C: 4 x 1Gi = 4Gi
              Total: 15Gi / 16Gi quota = 93.75% ✓

            Pod Count:
              5 + 3 + 4 = 12 / 20 quota = 60% ✓

            Result: Deployment C FITS. All 4 replicas will be created.

            Warning: The namespace is at 93.75% capacity for both CPU and
            memory. Any further scaling or new deployments will likely hit
            the quota. Consider requesting a quota increase proactively.
          difficulty: 2
          annotations:
            - type: tip
              label: Capacity Planning
              text: >-
                Always calculate remaining quota headroom before deploying. Running at 90%+ quota means autoscaling
                or incident response might fail due to insufficient budget.
        - id: v6
          title: Quota Blocking Autoscaler
          description: >-
            A namespace has a ResourceQuota with <code>pods: "10"</code> and <code>requests.cpu: "4"</code>.
            A Deployment with 8 replicas (each requesting 400m CPU) is configured with an HPA (Horizontal Pod
            Autoscaler) that scales from 8 to 15 replicas. Under load, the HPA tries to scale to 12 replicas.
            What happens?
          functionSignature: "Explanation of quota vs autoscaler interaction"
          testCases:
            - input: "Current: 8 pods x 400m = 3200m CPU. HPA wants 12 replicas. Quota: 10 pods, 4000m CPU."
              output: "Only 2 new pods created (pod quota hit at 10). CPU: 4000m exactly. HPA reports unable to scale."
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Two quota limits are in play: pod count and CPU. Which one is hit first? After 10 pods, what is
                the CPU usage?
            - title: "\U0001F4A1 Hint"
              content: >-
                Pod quota: 10 limit, currently 8, can add 2 more = 10 total.
                CPU quota: 4000m limit, currently 3200m, adding 2 pods = 3200m + 800m = 4000m.
                Both limits are hit simultaneously at 10 pods. The HPA can't reach 12.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Current: 8 pods x 400m = 3200m CPU
                Add 2: 10 pods x 400m = 4000m CPU (both quotas exactly at limit)
                Add 3: 11 pods → pod quota exceeded (10 max)
                       11 x 400m = 4400m → CPU quota exceeded (4000m max)
                HPA logs: unable to scale beyond 10 replicas</pre>
          solution: |-
            Current state: 8 pods x 400m = 3200m CPU

            HPA tries to scale to 12 replicas (add 4 pods):

            Pod 9:  created (9 pods, 3600m CPU) ✓
            Pod 10: created (10 pods, 4000m CPU) ✓ — both quotas at limit
            Pod 11: REJECTED — pod quota (10/10) and CPU quota (4000m/4000m) both exceeded
            Pod 12: REJECTED — same reason

            Result: Only 10 out of 12 desired replicas run.

            The HPA reports:
            "unable to scale: exceeded quota: pods: 10 / 10"

            Impact: Under load, the service can't scale to meet demand.
            The application may experience degraded performance or timeouts.

            Fix: Increase the namespace quota to accommodate autoscaling headroom.
            Rule of thumb: set quota >= HPA maxReplicas x per-pod resources.
          difficulty: 3
          annotations:
            - type: gotcha
              label: Quota vs Autoscaling
              text: >-
                Quotas and autoscalers can conflict. If the quota doesn't accommodate the HPA's max replicas,
                the autoscaler can't do its job during traffic spikes. Always size quotas to include autoscaling
                headroom.
            - type: tip
              label: Monitoring
              text: >-
                Monitor quota usage alongside HPA activity. Alert when quota usage exceeds 80% so you can
                increase limits before autoscaling is blocked.
        - id: v7
          title: Competing Deployments
          description: >-
            A namespace has a ResourceQuota with <code>requests.memory: 8Gi</code>. Current usage is 6Gi. Two
            developers simultaneously try to create deployments: Developer A creates a deployment with 2 replicas
            requesting 1Gi memory each. Developer B creates a deployment with 3 replicas requesting 1Gi memory each.
            What is the likely outcome?
          functionSignature: "Explanation of concurrent quota enforcement"
          testCases:
            - input: "Available: 2Gi. Dev A needs 2Gi. Dev B needs 3Gi. Both submitted simultaneously."
              output: "Whichever pods are admitted first consume the budget. At most 2Gi of new pods are created."
          hints:
            - title: "\U0001F914 Think about it"
              content: >-
                Quota enforcement is per-pod, not per-deployment. Pods from both deployments are created one at a time.
                Who wins the race?
            - title: "\U0001F4A1 Hint"
              content: >-
                The API server processes pod creation requests sequentially. Available budget is 2Gi.
                Pods are admitted first-come-first-served. After 2Gi of pods are admitted, all further pods are rejected.
            - title: "\U0001F527 Pattern"
              content: >-
                <pre>Available: 8Gi - 6Gi = 2Gi
                Possible outcomes (depends on pod creation order):
                - A gets 2 pods (2Gi), B gets 0 pods → A fully deployed, B fully blocked
                - A gets 1 pod (1Gi), B gets 1 pod (1Gi) → both partially deployed
                - B gets 2 pods (2Gi), A gets 0 → B partially deployed, A fully blocked
                Total new pods will consume exactly 2Gi regardless of mix</pre>
          solution: |-
            Available memory budget: 8Gi - 6Gi = 2Gi
            Developer A needs: 2 x 1Gi = 2Gi
            Developer B needs: 3 x 1Gi = 3Gi
            Combined need: 5Gi > 2Gi available

            Quota enforcement is first-come-first-served at the pod level.
            The outcome depends on which pods reach the API server first:

            Scenario 1: A's pods are created first
              A pod 1: 1Gi (7Gi total) ✓
              A pod 2: 1Gi (8Gi total) ✓ — quota full
              B pod 1-3: all REJECTED
              Result: A fully deployed, B completely blocked

            Scenario 2: Interleaved
              A pod 1: 1Gi (7Gi total) ✓
              B pod 1: 1Gi (8Gi total) ✓ — quota full
              A pod 2: REJECTED
              B pod 2-3: REJECTED
              Result: Both partially deployed

            Both deployments will show "exceeded quota" events on their
            ReplicaSets for the pods that couldn't be created.
          difficulty: 3
          annotations:
            - type: gotcha
              label: Race Condition
              text: >-
                When multiple deployments compete for limited quota, the outcome is non-deterministic. This is why
                capacity planning and quota sizing matter — leave enough headroom for concurrent deployments.
